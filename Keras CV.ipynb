{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fdba04-37d2-421d-a46a-86b6d7ec75da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ronlo\\anaconda3\\envs\\capstone-env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Bidirectional, GRU, TextVectorization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import time\n",
    "import nltk\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import product\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "from tensorflow.data import Dataset as tf_Dataset, AUTOTUNE as tf_AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de521ece-0342-43eb-91a9-59bb8bf38176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords  \n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "def get_wordnet_pos_optimized(treebank_tag):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts.\"\"\"\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    # Default to NOUN if not found\n",
    "    return tag_dict.get(treebank_tag[0], wordnet.NOUN)\n",
    "\n",
    "def clean_text(review, tokenizer, stop_words=None, lemmatize=False, tokenize=False):\n",
    "    \"\"\"Clean and preprocess a single review text.\"\"\"\n",
    "    tokens = tokenizer.tokenize(review.lower())\n",
    "    \n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        tokens = [lemmatizer.lemmatize(word, get_wordnet_pos_optimized(tag)) for word, tag in pos_tags]\n",
    "    \n",
    "    if stop_words:\n",
    "        stop_words_set = set(stop_words)\n",
    "        tokens = [word for word in tokens if word not in stop_words_set]\n",
    "    \n",
    "    if tokenize:\n",
    "        return tokens\n",
    "    else:\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "def preprocess_texts(reviews, tokenizer, stop_words=None, lemmatize=False, tokenize=False):\n",
    "    \"\"\"Apply optimized text cleaning and preprocessing to a list of texts.\"\"\"\n",
    "    return [clean_text(review, tokenizer, stop_words=stop_words, lemmatize=lemmatize, tokenize=tokenize) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ad5384-fdec-47a1-8455-6b4daa580836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tf_dataset(X, y, batch_size, is_training=False):\n",
    "    \"\"\"\n",
    "    Prepares a TensorFlow dataset for efficient training or evaluation.\n",
    "    \"\"\"\n",
    "    dataset = tf_Dataset.from_tensor_slices((X, y))\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(10000)  # Shuffle only if dataset is for training\n",
    "    return dataset.batch(batch_size).cache().prefetch(tf_AUTOTUNE)\n",
    "\n",
    "def extract_performance_metrics(history, callbacks):\n",
    "    early_stopping = next(\n",
    "        (cb for cb in callbacks if isinstance(cb, EarlyStopping)), \n",
    "        None\n",
    "    )\n",
    "    if early_stopping and early_stopping.stopped_epoch > 0:\n",
    "        adjusted_epoch = early_stopping.stopped_epoch - early_stopping.patience\n",
    "        max_epoch_index = len(history.history['loss']) - 1\n",
    "        best_epoch = max(0, min(adjusted_epoch, max_epoch_index))\n",
    "    else:\n",
    "        best_epoch = len(history.history['loss']) - 1\n",
    "\n",
    "    metrics = {\n",
    "        'loss': history.history['loss'][best_epoch],\n",
    "        'val_loss': history.history['val_loss'][best_epoch],\n",
    "        'val_accuracy': history.history.get('val_accuracy', [None])[best_epoch],\n",
    "        'val_auc': history.history.get('val_auc', [None])[best_epoch]\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c22612-da75-47e5-9652-65ea7cb46b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with validation split\n",
    "df = pd.read_csv('data/Airline_review.csv')[['Review_Title', 'Review', 'Recommended']]\n",
    "reviews = df['Review_Title'] + ' ' + df['Review']\n",
    "labels = df['Recommended'].map({'yes': 1, 'no': 0})\n",
    "X, X_test, y, y_test = train_test_split(np.array(reviews), np.array(labels), test_size=0.1, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8362922-66ef-4cf2-9e24-71f171c64618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ronlo\\anaconda3\\envs\\capstone-env\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiating important stuff\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "tokenizer = RegexpTokenizer(r\"([a-zA-Z]+(?:'[a-z]+)?)\")\n",
    "text_vectorization = TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=20000,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=200)\n",
    "CALLBACKS = [EarlyStopping(monitor='val_loss',\n",
    "                           min_delta=0.001,\n",
    "                           patience=5,\n",
    "                           restore_best_weights=True,\n",
    "                           verbose=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e256764-4309-4ada-8a87-68bb3038c30d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_aggregate = {'loss': 0, 'val_loss': 0, 'val_accuracy': 0, 'val_auc': 0}\n",
    "for train, validation in skf.split(X, y):\n",
    "    X_train = X[train]\n",
    "    y_train = y[train]\n",
    "    X_val = X[validation]\n",
    "    y_val = y[validation]\n",
    "\n",
    "    X_train_clean = preprocess_texts(X_train, tokenizer)\n",
    "    X_val_clean = preprocess_texts(X_val, tokenizer)\n",
    "\n",
    "    text_vectorization.adapt(X_train)\n",
    "    X_train = text_vectorization(X_train)\n",
    "    X_val = text_vectorization(X_val)\n",
    "\n",
    "    train_ds = prepare_tf_dataset(X_train, y_train, 256, is_training=True)\n",
    "    val_ds = prepare_tf_dataset(X_val, y_val, 256)\n",
    "    \n",
    "    rnn_model = Sequential([\n",
    "        Embedding(input_dim=20000, output_dim=32, input_length=200),\n",
    "        Bidirectional(GRU(16)),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    rnn_model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', 'AUC'])\n",
    "    \n",
    "    results = rnn_model.fit(train_ds,\n",
    "                            validation_data= val_ds,\n",
    "                             epochs=100,\n",
    "                             verbose=0,\n",
    "                            callbacks=CALLBACKS)\n",
    "    \n",
    "    metrics = extract_performance_metrics(results, CALLBACKS)\n",
    "    for key in metrics_aggregate:\n",
    "            metrics_aggregate[key] += metrics[key]\n",
    "results_dic = {key: val / 2 for key, val in metrics_aggregate.items()}\n",
    "pd.DataFrame([results_dic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d09eb0d-ea92-4829-97f2-fd27cafbdc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_cv(X,y,skf,tokenizer,text_vectorization,CALLBACKS, glove=False):\n",
    "    metrics_aggregate = {'loss': 0, 'val_loss': 0, 'val_accuracy': 0, 'val_auc': 0}\n",
    "    X_array = np.array(X)\n",
    "    y_array = np.array(y)\n",
    "    for train, validation in skf.split(X_array, y_array):\n",
    "        X_train = X_array[train]\n",
    "        y_train = y_array[train]\n",
    "        X_val = X_array[validation]\n",
    "        y_val = y_array[validation]\n",
    "    \n",
    "        X_train_clean = preprocess_texts(X_train, tokenizer)\n",
    "        X_val_clean = preprocess_texts(X_val, tokenizer)\n",
    "    \n",
    "        text_vectorization.adapt(X_train)\n",
    "        X_train = text_vectorization(X_train)\n",
    "        X_val = text_vectorization(X_val)\n",
    "    \n",
    "        train_ds = prepare_tf_dataset(X_train, y_train, 256, is_training=True)\n",
    "        val_ds = prepare_tf_dataset(X_val, y_val, 256)\n",
    "\n",
    "        if glove:\n",
    "            vocabulary = text_vectorization.get_vocabulary()\n",
    "            vocab_size = len(vocabulary)\n",
    "            \n",
    "            # Load GloVe embeddings from file.\n",
    "            glove_embeddings = {}\n",
    "            with open('data/glove.6B.300d.txt', 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    values = line.split()\n",
    "                    word = values[0]\n",
    "                    vector = np.asarray(values[1:], dtype='float32')\n",
    "                    glove_embeddings[word] = vector\n",
    "\n",
    "            # Initialize the embedding matrix with zeros.\n",
    "            embedding_matrix = np.zeros((vocab_size, 300))\n",
    "            \n",
    "            # Populate the embedding matrix with GloVe vectors.\n",
    "            for i, word in enumerate(vocabulary):\n",
    "                embedding_vector = glove_embeddings.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    \n",
    "            model = Sequential([Embedding(input_dim=20000, output_dim=300, input_length=200, weights=[embedding_matrix], trainable=False),\n",
    "                                Bidirectional(GRU(32)),\n",
    "                                Dropout(0.4),\n",
    "                                Dense(16, activation='relu'),\n",
    "                                Dropout(0.4),\n",
    "                                Dense(1, activation='sigmoid')\n",
    "                                ])\n",
    "        \n",
    "        model = Sequential([Embedding(input_dim=20000, output_dim=32, input_length=200),\n",
    "                            Bidirectional(GRU(16)),\n",
    "                            Dense(8, activation='relu'),\n",
    "                            Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy', 'AUC'])\n",
    "        \n",
    "        results = model.fit(train_ds,\n",
    "                             validation_data= val_ds,\n",
    "                             epochs=100,\n",
    "                            verbose=0,\n",
    "                            callbacks=CALLBACKS)\n",
    "        \n",
    "        metrics = extract_performance_metrics(results, CALLBACKS)\n",
    "        for key in metrics_aggregate:\n",
    "                metrics_aggregate[key] += metrics[key]\n",
    "    results_dic = {key: val / 2 for key, val in metrics_aggregate.items()}\n",
    "    return pd.DataFrame([results_dic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6123d7c-ebb3-4e59-a17e-ab3256cda684",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = Sequential([\n",
    "            Embedding(input_dim=20000, output_dim=32, input_length=200),\n",
    "            Bidirectional(GRU(16)),\n",
    "            Dense(8, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "keras_cv(rnn_model,X,y,skf,tokenizer,text_vectorization,CALLBACKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3235ff77-2ffd-4c8b-9ecf-f9ee3df7acb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ronlo\\anaconda3\\envs\\capstone-env\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ronlo\\anaconda3\\envs\\capstone-env\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.225006</td>\n",
       "      <td>0.287012</td>\n",
       "      <td>0.883853</td>\n",
       "      <td>0.944178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  val_loss  val_accuracy   val_auc\n",
       "0  0.225006  0.287012      0.883853  0.944178"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glv_model = Sequential([\n",
    "#     Embedding(input_dim=20000, output_dim=300, input_length=200, weights=[embedding_matrix], trainable=False),\n",
    "#     Bidirectional(GRU(32)),\n",
    "#     Dropout(0.4),\n",
    "#     Dense(16, activation='relu'),\n",
    "#     Dropout(0.4),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "keras_cv(X,y,skf,tokenizer,text_vectorization,CALLBACKS,glove=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8901e7c-78fb-47b0-9c57-c8c10ebf2053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
