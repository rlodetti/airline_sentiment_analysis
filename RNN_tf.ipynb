{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee659b3-a44b-4d8d-9c1f-8de2162c1bb3",
   "metadata": {},
   "source": [
    "1. Load Data (stratified sample)\n",
    "2. train-val-test split\n",
    "4. clean/tokenize, not stop, not lemma\n",
    "5. convert to tf.dataset\n",
    "6. run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc367dae-a0da-481d-a487-c5f27ac00637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading sample Data\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945900ad-0e74-4360-8dc7-0f51cd020da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-01 12:11:34.989486: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "def prepare_tf_dataset(X, y, batch_size, is_training=False):\n",
    "    \"\"\"\n",
    "    Prepares a TensorFlow dataset for efficient training or evaluation.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(10000)  # Shuffle only if dataset is for training\n",
    "    return dataset.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "def load_data(path, include_validation=False, sample_size=0):\n",
    "    \"\"\"Load review data from a CSV file, with optional sampling and validation split.\"\"\"\n",
    "    df = pd.read_csv(path)[['Review_Title', 'Review', 'Recommended']]\n",
    "    if sample_size > 0:\n",
    "        df = df.sample(sample_size)\n",
    "    \n",
    "    X = df['Review_Title'] + ' ' + df['Review']\n",
    "    y = df['Recommended'].map({'yes': 1, 'no': 0})\n",
    "    \n",
    "    if include_validation:\n",
    "        return split_data_with_validation(X, y)\n",
    "    else:\n",
    "        return train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "def split_data_with_validation(X, y):\n",
    "    \"\"\"Split data into training, validation, and test sets.\"\"\"\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_text(review, stop_words=None, lemmatize=True):\n",
    "    \"\"\"Clean and preprocess a single review text.\"\"\"\n",
    "    tokenizer = RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\")\n",
    "    lemmatizer = WordNetLemmatizer() if lemmatize else None\n",
    "    \n",
    "    tokens = tokenizer.tokenize(review.lower())\n",
    "    if lemmatize:\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    if stop_words:\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_texts(reviews, stop_words=None, lemmatize=False):\n",
    "    \"\"\"Apply text cleaning and preprocessing to a list of texts.\"\"\"\n",
    "    return [clean_text(review, stop_words=stop_words, lemmatize=lemmatize) for review in reviews]\n",
    "\n",
    "def load_and_prepare_data(path, include_validation=True, sample_size=5000, \n",
    "                          stop_words=None, lemmatize=False, \n",
    "                          max_tokens=10000, percentile_len=0.9, batch_size=64):\n",
    "    \"\"\"\n",
    "    Load, clean, and prepare data for training and validation.\n",
    "    \"\"\"\n",
    "    # Load the data with validation split and optional sampling\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_data(\n",
    "        path, include_validation=include_validation, sample_size=sample_size)\n",
    "    \n",
    "    # Clean the text data\n",
    "    X_train = preprocess_texts(X_train, stop_words=stop_words, lemmatize=lemmatize)\n",
    "    X_val = preprocess_texts(X_val, stop_words=stop_words, lemmatize=lemmatize)\n",
    "    \n",
    "    # Calculating percentile sequence length and vocabulary size for training set\n",
    "    lengths = pd.Series([len(review.split()) for review in X_train])\n",
    "    sequence_length = int(lengths.quantile(percentile_len))\n",
    "    vocab_size = min(max_tokens, len(set(word for review in X_train for word in review.split())))\n",
    "    \n",
    "    # Initialize and adapt the TextVectorization layer\n",
    "    text_vectorization = tf.keras.layers.TextVectorization(\n",
    "        standardize=None,\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=sequence_length)\n",
    "    \n",
    "    text_vectorization.adapt(X_train)\n",
    "    X_train = text_vectorization(X_train)\n",
    "    X_val = text_vectorization(X_val)\n",
    "    \n",
    "    # Prepare the datasets\n",
    "    train_ds = prepare_tf_dataset(X_train, y_train, batch_size, is_training=True)\n",
    "    val_ds = prepare_tf_dataset(X_val, y_val, batch_size)\n",
    "\n",
    "    return train_ds, val_ds, vocab_size, sequence_length, batch_size\n",
    "\n",
    "def add_rnn_layer(model, units, rnn_type='gru', bidirectional=False, return_sequences=False):\n",
    "    LayerClass = layers.GRU if rnn_type == 'gru' else layers.LSTM\n",
    "    layer = LayerClass(units, return_sequences=return_sequences)\n",
    "    if bidirectional:\n",
    "        layer = layers.Bidirectional(layer)\n",
    "    model.add(layer)\n",
    "\n",
    "def build_rnn_model(rnn_layers, dense_layers, recurrent_type, bi_directional, dropout_rate, units, sequence_length, vocab_size):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=units, input_length=sequence_length))\n",
    "\n",
    "    for i in range(rnn_layers):\n",
    "        add_rnn_layer(model, units, rnn_type=recurrent_type, bidirectional=bi_directional, \n",
    "                      return_sequences=(i < rnn_layers - 1))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "        units = max(2, units // 2)\n",
    "\n",
    "    for _ in range(dense_layers):\n",
    "        model.add(layers.Dense(units, activation=\"relu\"))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "        units = max(2, units // 2)\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.AUC(name=\"auc\")])\n",
    "    return model\n",
    "\n",
    "def extract_performance_metrics(history, callbacks, duration):\n",
    "    early_stopping = next(\n",
    "        (cb for cb in callbacks if isinstance(cb, tf.keras.callbacks.EarlyStopping)), \n",
    "        None\n",
    "    )\n",
    "    if early_stopping and early_stopping.stopped_epoch > 0:\n",
    "        adjusted_epoch = early_stopping.stopped_epoch - early_stopping.patience\n",
    "        max_epoch_index = len(history.history['loss']) - 1\n",
    "        best_epoch = max(0, min(adjusted_epoch, max_epoch_index))\n",
    "    else:\n",
    "        best_epoch = len(history.history['loss']) - 1\n",
    "\n",
    "    metrics = {\n",
    "        'loss': history.history['loss'][best_epoch],\n",
    "        'val_loss': history.history['val_loss'][best_epoch],\n",
    "        'val_accuracy': history.history.get('val_accuracy', [None])[best_epoch],\n",
    "        'val_auc': history.history.get('val_auc', [None])[best_epoch],\n",
    "        'duration': duration\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def train_and_evaluate(model_function, train_ds, val_ds, epochs):\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0.01,\n",
    "            patience=5, \n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    model = model_function()    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        epochs=epochs, \n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    duration = time.time() - start_time\n",
    "    return extract_performance_metrics(history, callbacks, duration)\n",
    "\n",
    "def calculate_average_metrics(runs, model_function, train_ds, val_ds, epochs):\n",
    "    metrics_aggregate = {'loss': 0, 'val_loss': 0, 'val_accuracy': 0, 'val_auc': 0, 'duration': 0}\n",
    "\n",
    "    for _ in range(runs):\n",
    "        metrics = train_and_evaluate(model_function, train_ds, val_ds, epochs)\n",
    "        for key in metrics_aggregate:\n",
    "            metrics_aggregate[key] += metrics[key]\n",
    "\n",
    "    return {key: val / runs for key, val in metrics_aggregate.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f8e1ed4-be78-4642-a3e4-c29e2e4c4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Prepare the Data\n",
    "train_ds, val_ds, vocab_size, sequence_length, batch_size = load_and_prepare_data(\n",
    "    path='data/Airline_review.csv', \n",
    "    include_validation=True, \n",
    "    sample_size=5000, \n",
    "    stop_words=None, \n",
    "    lemmatize=False, \n",
    "    max_tokens=10000, \n",
    "    percentile_len=0.9, \n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d5ee93-bbb8-40b9-9a83-7e474c425a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6415860652923584, 'val_loss': 0.6530152360598246, 'val_accuracy': 0.6539999842643738, 'val_auc': 0.5341134667396545, 'duration': 12.352736314137777}\n"
     ]
    }
   ],
   "source": [
    "# Model and Score the data\n",
    "runs = 3\n",
    "epochs = 2\n",
    "\n",
    "model_function = lambda: build_rnn_model(\n",
    "    rnn_layers=1,\n",
    "    dense_layers=1,\n",
    "    recurrent_type='gru',\n",
    "    bi_directional=False,\n",
    "    dropout_rate=0.2,\n",
    "    units=64,\n",
    "    sequence_length=sequence_length,\n",
    "    vocab_size=vocab_size\n",
    ")  \n",
    "\n",
    "average_metrics_result = calculate_average_metrics(runs, model_function, train_ds, val_ds, epochs)\n",
    "print(average_metrics_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85180bd1-c1e6-4338-a48a-6280a4530ed8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43maverage_metrics_result\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/pandas/core/frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    761\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    762\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    763\u001b[0m     )\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/pandas/core/internals/construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(average_metrics_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f192b6a-8381-4146-9aa5-a2531cbf0ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0ea1080e-f1eb-4fa3-a2a7-a9e76c734034",
   "metadata": {},
   "source": [
    "loss, val_loss, val_accuracy, val_auc, duration, rnn_layers, dense_layers, recurrent_type, bi_directional, dropout_rate, units, sequence_length, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7efe84-8900-45bc-a008-b3d3c0be2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 3\n",
    "epochs = 2\n",
    "for i in [True, False:\n",
    "    for j in ['gru','lstm']:\n",
    "        model_function = lambda: build_rnn_model(\n",
    "            rnn_layers=1,\n",
    "            dense_layers=1,\n",
    "            recurrent_type=j,\n",
    "            bi_directional=i,\n",
    "            dropout_rate=0.2,\n",
    "            units=64,\n",
    "            sequence_length=sequence_length,\n",
    "            vocab_size=vocab_size\n",
    "        )  \n",
    "\n",
    "        average_metrics_result = calculate_average_metrics(runs, model_function, train_ds, val_ds, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc956e3-a382-4808-a44c-8ee62e2afd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     loss  val_loss  val_accuracy  val_auc  duration recurrent_type  \\\n",
      "0  0.3116    0.3601        0.8493   0.9267        59            gru   \n",
      "1  0.6576    0.6476        0.6540   0.4842        35            gru   \n",
      "2  0.2927    0.3230        0.8647   0.9367       147           lstm   \n",
      "3  0.6587    0.6483        0.6540   0.5155        56           lstm   \n",
      "\n",
      "   bi_directional  \n",
      "0            True  \n",
      "1           False  \n",
      "2            True  \n",
      "3           False  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def run_experiment(recurrent_type, bi_directional, runs, train_ds, val_ds, epochs, sequence_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Runs the experiment for a specific configuration and returns the average metrics.\n",
    "    \"\"\"\n",
    "    model_function = lambda: build_rnn_model(\n",
    "        rnn_layers=1,\n",
    "        dense_layers=1,\n",
    "        recurrent_type=recurrent_type,\n",
    "        bi_directional=bi_directional,\n",
    "        dropout_rate=0.2,\n",
    "        units=64,\n",
    "        sequence_length=sequence_length,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "    return calculate_average_metrics(runs, model_function, train_ds, val_ds, epochs)\n",
    "\n",
    "# Configuration options\n",
    "configurations = [\n",
    "    {'recurrent_type': 'gru', 'bi_directional': True},\n",
    "    {'recurrent_type': 'gru', 'bi_directional': False},\n",
    "    {'recurrent_type': 'lstm', 'bi_directional': True},\n",
    "    {'recurrent_type': 'lstm', 'bi_directional': False},\n",
    "]\n",
    "\n",
    "results = []\n",
    "runs = 3\n",
    "epochs = 20\n",
    "\n",
    "# Run experiments\n",
    "for config in configurations:\n",
    "    metrics = run_experiment(\n",
    "        config['recurrent_type'],\n",
    "        config['bi_directional'],\n",
    "        runs,\n",
    "        train_ds,\n",
    "        val_ds,\n",
    "        epochs,\n",
    "        sequence_length=sequence_length,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "    results.append({\n",
    "        **metrics,\n",
    "        **config  # Unpack configuration into the results\n",
    "    })\n",
    "\n",
    "# Create DataFrame and format\n",
    "df = pd.DataFrame(results)\n",
    "df = df.round({'loss': 4, 'val_loss': 4, 'val_accuracy': 4, 'val_auc': 4})\n",
    "df['duration'] = df['duration'].round(0).astype(int)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab559ae-d649-4d83-9593-2ee131e7f79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_auc</th>\n",
       "      <th>duration</th>\n",
       "      <th>recurrent_type</th>\n",
       "      <th>bi_directional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.3411</td>\n",
       "      <td>0.8473</td>\n",
       "      <td>0.9232</td>\n",
       "      <td>17</td>\n",
       "      <td>gru</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6418</td>\n",
       "      <td>0.6508</td>\n",
       "      <td>0.6540</td>\n",
       "      <td>0.5349</td>\n",
       "      <td>12</td>\n",
       "      <td>gru</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2748</td>\n",
       "      <td>0.3133</td>\n",
       "      <td>0.8673</td>\n",
       "      <td>0.9384</td>\n",
       "      <td>31</td>\n",
       "      <td>lstm</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6397</td>\n",
       "      <td>0.6493</td>\n",
       "      <td>0.6540</td>\n",
       "      <td>0.5447</td>\n",
       "      <td>19</td>\n",
       "      <td>lstm</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  val_loss  val_accuracy  val_auc  duration recurrent_type  \\\n",
       "0  0.3028    0.3411        0.8473   0.9232        17            gru   \n",
       "1  0.6418    0.6508        0.6540   0.5349        12            gru   \n",
       "2  0.2748    0.3133        0.8673   0.9384        31           lstm   \n",
       "3  0.6397    0.6493        0.6540   0.5447        19           lstm   \n",
       "\n",
       "   bi_directional  \n",
       "0            True  \n",
       "1           False  \n",
       "2            True  \n",
       "3           False  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951880f-dd05-40c8-a30a-1b618dd8efdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
