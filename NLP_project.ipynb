{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f7cc28-6628-4dc2-bab5-e5118e1effe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 13:26:16.695135: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f04ed-8be3-42cc-a340-db1431a6eb56",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f539b-6351-4514-84f1-51c51c478ea9",
   "metadata": {},
   "source": [
    "- Combine title & review into one column\n",
    "- Binary Encode target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9be554d-7eef-48ea-bda5-889a023b8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Airline_review.csv')[['Review_Title','Review','Recommended']]\n",
    "reviews = df['Review_Title'] + ' ' + df['Review']\n",
    "labels = df['Recommended'].map({'yes':1,'no':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7c6b1-9d5e-4f27-942a-e09b2cf59e07",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a307c6a-4be1-4d07-95ff-7809517f3bf6",
   "metadata": {},
   "source": [
    "- Standard cleaning and tokenization for understanding\n",
    "- Number of Reviews\n",
    "- Number and distribution of target\n",
    "- Number of words per review\n",
    "- Frequency distribution of words\n",
    "- Distribution of review length\n",
    "\n",
    "**Next Steps**: \n",
    "- Maybe show difference between yes and no reviews\n",
    "- also show after stopwords have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06d87b22-4ff5-4758-b4af-3f9495fe19a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard cleaning and tokenization for understanding\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_text(review, stop_words=None, lemmatize=True):\n",
    "    \"\"\"Clean and preprocess a single review text.\"\"\"\n",
    "    tokenizer = RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\")\n",
    "    lemmatizer = WordNetLemmatizer() if lemmatize else None\n",
    "    \n",
    "    tokens = tokenizer.tokenize(review.lower())\n",
    "    if lemmatize:\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    if stop_words:\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def preprocess_texts(reviews, stop_words=None, lemmatize=False):\n",
    "    \"\"\"Apply text cleaning and preprocessing to a list of texts.\"\"\"\n",
    "    return [clean_text(review, stop_words=stop_words, lemmatize=lemmatize) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54a26930-1f1f-4e14-8094-b568c803c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_tokened = preprocess_texts(reviews, stop_words=None, lemmatize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61071a92-d43b-4769-80d0-742dbbce91be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23171"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Reviews\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "369a145f-8f62-4aed-b5b4-1cc7ec726441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recommended\n",
       "no     15364\n",
       "yes     7807\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Recommended\n",
       "no     0.66307\n",
       "yes    0.33693\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number and distribution of target\n",
    "display(df['Recommended'].value_counts())\n",
    "print('')\n",
    "display(df['Recommended'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e30f627-a5f6-49f9-8d3b-f13bfb3eabec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of words per review\n",
    "review_lengths = [len(review) for review in reviews_tokened]\n",
    "np.median(review_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93fb5223-7369-49f9-9ffe-b3dc35f437fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Frequency distribution of words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fdist = FreqDist(word for review in reviews_tokened for word in review)\n",
    "top_words = fdist.most_common(25)\n",
    "words = []\n",
    "counts = []\n",
    "for word, count in top_words:\n",
    "    words.append(word)\n",
    "    counts.append(count)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(x=words, y=counts, ax= ax)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7280bfed-44c4-4d15-aad9-0d6dea8c81d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGdCAYAAADzOWwgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAx/UlEQVR4nO3dfXBUZZ73/09LmgAhaQiQNK2JBGUQTEQeFEF3wAFBFFmL2mEURSzRweExAoMgOkZqTBxGgZ0g+LAssEbEe0tw2VlEAiIOBQoGIw32gt4GBE2MM4Y8aEia5Pr9wS/npvMAJIR0d877VdVVnHOuDt+viH7qOtd1jsMYYwQAAGBjVwS7AAAAgGAjEAEAANsjEAEAANsjEAEAANsjEAEAANsjEAEAANsjEAEAANsjEAEAANuLCHYB4aK6ulrfffedoqOj5XA4gl0OAAC4CMYYlZaWyuPx6IorGp4HIhBdpO+++04JCQnBLgMAADTBiRMndNVVVzV4nUB0kaKjoyWd/QcaExMT5GoAAMDFKCkpUUJCgvX/8YYQiC5SzW2ymJgYAhEAAGHmQstdWFQNAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj7fd24zf75fX67WOU1JS5HQ6g1gRAADBRyCyGa/Xq2kvb1ZM9x4qyT+mldOlAQMGBLssAACCKqi3zD766CPdc8898ng8cjgcevfdd61rfr9fTz75pFJSUhQVFSWPx6OHHnpI3333XcDPqKio0MyZM9W1a1dFRUVp3LhxOnnyZMCYoqIiTZo0SS6XSy6XS5MmTdKpU6daoMPQFNO9h2ITeyume49glwIAQEgIaiD66aef1K9fP61YsaLOtZ9//lkHDhzQM888owMHDmjjxo06evSoxo0bFzAuNTVVmzZt0oYNG7R7926VlZVp7NixqqqqssZMnDhRubm52rp1q7Zu3arc3FxNmjTpsvcHAADCQ1BvmY0ZM0Zjxoyp95rL5VJ2dnbAuczMTN1888365ptvlJiYqOLiYq1evVpvvPGGRo4cKUnKyspSQkKCtm/frtGjR8vn82nr1q36+OOPNXjwYEnS66+/riFDhujIkSPq3bv35W0SAACEvLDaZVZcXCyHw6FOnTpJknJycuT3+zVq1ChrjMfjUXJysvbs2SNJ2rt3r1wulxWGJOmWW26Ry+WyxtSnoqJCJSUlAR8AANA6hU0gOn36tBYsWKCJEycqJiZGklRQUKC2bduqc+fOAWPj4+NVUFBgjYmLi6vz8+Li4qwx9cnIyLDWHLlcLiUkJDRjNwAAIJSExS4zv9+v++67T9XV1Vq5cuUFxxtj5HA4rONzf93QmNoWLlyoOXPmWMclJSWtLhRVV52Rz+cLOMc2fACAHYV8IPL7/ZowYYLy8vL0wQcfWLNDkuR2u1VZWamioqKAWaLCwkINHTrUGvP999/X+bk//PCD4uPjG/x9IyMjFRkZ2YydhJ6ywpN6cctpdfP5JYlt+AAA2wrpW2Y1YejLL7/U9u3b1aVLl4DrAwcOlNPpDFh8nZ+fr0OHDlmBaMiQISouLta+ffusMZ988omKi4utMXbWMS5RsYm92YYPALC1oM4QlZWV6auvvrKO8/LylJubq9jYWHk8Hv3Lv/yLDhw4oL/+9a+qqqqy1vzExsaqbdu2crlcmjJliubOnasuXbooNjZW8+bNU0pKirXrrE+fPrrzzjv12GOP6dVXX5Uk/fa3v9XYsWPZYQYAACQFORB9+umnuv32263jmjU7kydPVlpamjZv3ixJuvHGGwO+t3PnTg0fPlyStGzZMkVERGjChAkqLy/XiBEjtHbtWrVp08Ya/+abb2rWrFnWbrRx48bV++wjAABgT0ENRMOHD5cxpsHr57tWo127dsrMzFRmZmaDY2JjY5WVldWkGsNd7XeX+Xw+6SL+uQIAYCchv6gal+bcd5dJUr53rzr17BfcogAACDEEIhuoeXeZdHYnGQAACBTSu8wAAABaAjNEsPCgRgCAXRGIYOFBjQAAuyIQIUDNgxoBALAT1hABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADb4233aFB11Rn5fL6AcykpKXI6nUGqCACAy4NAhAaVFZ7Ui1tOq5vPL0kqyT+mldOlAQMGBLkyAACaF4EI59UxLlGxib0lMWMEAGi9CES4aMwYAQBaKwIRGuXcGSMAAFoLdpkBAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbiwh2AWhefr9fXq/XOvb5fJIxQawIAIDQRyBqZbxer6a9vFkx3XtIkvK9e9WpZ7/gFgUAQIgjELVCMd17KDaxtySpJP9YcIsBACAMsIYIAADYHoEIAADYHrfM0GTVVWfOLto+R0pKipxOZ5AqAgCgaQhEaLKywpN6cctpdfP5JZ1dr7RyujRgwIAgVwYAQOME9ZbZRx99pHvuuUcej0cOh0PvvvtuwHVjjNLS0uTxeNS+fXsNHz5chw8fDhhTUVGhmTNnqmvXroqKitK4ceN08uTJgDFFRUWaNGmSXC6XXC6XJk2apFOnTl3m7uyhY1yiYhN7Kzaxt7WzDQCAcBPUQPTTTz+pX79+WrFiRb3XlyxZoqVLl2rFihXav3+/3G637rjjDpWWllpjUlNTtWnTJm3YsEG7d+9WWVmZxo4dq6qqKmvMxIkTlZubq61bt2rr1q3Kzc3VpEmTLnt/AAAgPAT1ltmYMWM0ZsyYeq8ZY7R8+XItWrRI48ePlyStW7dO8fHxWr9+vaZOnari4mKtXr1ab7zxhkaOHClJysrKUkJCgrZv367Ro0fL5/Np69at+vjjjzV48GBJ0uuvv64hQ4boyJEj6t27d8s0CwAAQlbI7jLLy8tTQUGBRo0aZZ2LjIzUsGHDtGfPHklSTk6O/H5/wBiPx6Pk5GRrzN69e+VyuawwJEm33HKLXC6XNaY+FRUVKikpCfgAAIDWKWQDUUFBgSQpPj4+4Hx8fLx1raCgQG3btlXnzp3POyYuLq7Oz4+Li7PG1CcjI8Nac+RyuZSQkHBJ/QAAgNAVsoGohsPhCDg2xtQ5V1vtMfWNv9DPWbhwoYqLi63PiRMnGlk5AAAIFyEbiNxutyTVmcUpLCy0Zo3cbrcqKytVVFR03jHff/99nZ//ww8/1Jl9OldkZKRiYmICPgAAoHUK2UCUlJQkt9ut7Oxs61xlZaV27dqloUOHSpIGDhwop9MZMCY/P1+HDh2yxgwZMkTFxcXat2+fNeaTTz5RcXGxNQYAANhbUHeZlZWV6auvvrKO8/LylJubq9jYWCUmJio1NVXp6enq1auXevXqpfT0dHXo0EETJ06UJLlcLk2ZMkVz585Vly5dFBsbq3nz5iklJcXaddanTx/deeedeuyxx/Tqq69Kkn77299q7Nix7DADAACSghyIPv30U91+++3W8Zw5cyRJkydP1tq1azV//nyVl5dr2rRpKioq0uDBg7Vt2zZFR0db31m2bJkiIiI0YcIElZeXa8SIEVq7dq3atGljjXnzzTc1a9YsazfauHHjGnz2EQAAsJ+gBqLhw4fLGNPgdYfDobS0NKWlpTU4pl27dsrMzFRmZmaDY2JjY5WVlXUppQIAgFYsZNcQAQAAtBQCEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsL2gvu0erUt11Rn5fL6AcykpKXI6nUGqCACAi0MgQrMpKzypF7ecVjefX5JUkn9MK6dLAwYMCHJlAACcH4EIzapjXKJiE3sHuwwAABqFNUQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2eA5RmPP7/fJ6vdaxz+eTjAliRQAAhB8CUZjzer2a9vJmxXTvIUnK9+5Vp579glsUAABhhkDUCsR072E9Hbok/1hwiwEAIAwRiHDZ8LJXAEC4IBDhsuFlrwCAcEEgwmXFy14BAOGAbfcAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2QjoQnTlzRk8//bSSkpLUvn179ezZU4sXL1Z1dbU1xhijtLQ0eTwetW/fXsOHD9fhw4cDfk5FRYVmzpyprl27KioqSuPGjdPJkydbuh0AABCiQjoQ/elPf9Irr7yiFStWyOfzacmSJfrzn/+szMxMa8ySJUu0dOlSrVixQvv375fb7dYdd9yh0tJSa0xqaqo2bdqkDRs2aPfu3SorK9PYsWNVVVUVjLYAAECIiQh2Aeezd+9e/fM//7PuvvtuSVKPHj301ltv6dNPP5V0dnZo+fLlWrRokcaPHy9JWrduneLj47V+/XpNnTpVxcXFWr16td544w2NHDlSkpSVlaWEhARt375do0ePDk5zAAAgZIT0DNFtt92mHTt26OjRo5Kkzz//XLt379Zdd90lScrLy1NBQYFGjRplfScyMlLDhg3Tnj17JEk5OTny+/0BYzwej5KTk60x9amoqFBJSUnABwAAtE4hPUP05JNPqri4WNddd53atGmjqqoqPf/887r//vslSQUFBZKk+Pj4gO/Fx8fr+PHj1pi2bduqc+fOdcbUfL8+GRkZeu6555qzHQAAEKJCeobo7bffVlZWltavX68DBw5o3bp1evHFF7Vu3bqAcQ6HI+DYGFPnXG0XGrNw4UIVFxdbnxMnTjS9EQAAENJCeobo97//vRYsWKD77rtPkpSSkqLjx48rIyNDkydPltvtlnR2Fqh79+7W9woLC61ZI7fbrcrKShUVFQXMEhUWFmro0KEN/t6RkZGKjIy8HG0BAIAQE9IzRD///LOuuCKwxDZt2ljb7pOSkuR2u5WdnW1dr6ys1K5du6ywM3DgQDmdzoAx+fn5OnTo0HkDEQAAsI+QniG655579PzzzysxMVHXX3+9PvvsMy1dulSPPPKIpLO3ylJTU5Wenq5evXqpV69eSk9PV4cOHTRx4kRJksvl0pQpUzR37lx16dJFsbGxmjdvnlJSUqxdZ2gZ1VVn5PP5As6lpKTI6XQGqSIAAM4K6UCUmZmpZ555RtOmTVNhYaE8Ho+mTp2qP/zhD9aY+fPnq7y8XNOmTVNRUZEGDx6sbdu2KTo62hqzbNkyRUREaMKECSovL9eIESO0du1atWnTJhht2VZZ4Um9uOW0uvn8kqSS/GNaOV0aMGBAkCsDANhdSAei6OhoLV++XMuXL29wjMPhUFpamtLS0hoc065dO2VmZgY80BHB0TEuUbGJvYNdBgAAAUJ6DREAAEBLIBABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbiwh2AbCv6qoz8vl81nFKSoqcTmcQKwIA2BWBCEFTVnhSL245rW4+v0ryj2nldGnAgAHBLgsAYEMEIgRVx7hExSb2rjNbJDFjBABoOQQihIRzZ4skMWMEAGhRTVpU3bNnT/3jH/+oc/7UqVPq2bPnJRcFe6qZLYpN7K2Y7j2CXQ4AwEaaFIiOHTumqqqqOucrKir07bffXnJRAAAALalRt8w2b95s/fr999+Xy+WyjquqqrRjxw716NGj2YoDAABoCY0KRPfee68kyeFwaPLkyQHXnE6nevTooZdeeqnZigMAAGgJjQpE1dXVkqSkpCTt379fXbt2vSxFAQAAtKQm7TLLy8tr7joAAACCpsnb7nfs2KEdO3aosLDQmjmq8e///u+XXBgAAEBLaVIgeu6557R48WINGjRI3bt3l8PhaO66AAAAWkyTAtErr7yitWvXatKkSc1dDwAAQItr0nOIKisrNXTo0OauBQAAICiaFIgeffRRrV+/vrlrAQAACIom3TI7ffq0XnvtNW3fvl033HBDnRdwLl26tFmKAwAAaAlNCkQHDx7UjTfeKEk6dOhQwDUWWAMAgHDTpEC0c+fO5q4DAAAgaJq0hggAAKA1adIM0e23337eW2MffPBBkwsCAABoaU0KRDXrh2r4/X7l5ubq0KFDdV76CgAAEOqaFIiWLVtW7/m0tDSVlZVdUkEAAAAtrcnvMqvPgw8+qJtvvlkvvvhic/5YQH6/X16vN+BcSkpKnUc+AADQFM0aiPbu3at27do1548EJEler1fTXt6smO49JEkl+ce0cro0YMCA4BYGAGgVmhSIxo8fH3BsjFF+fr4+/fRTPfPMM81SGFBbTPceik3sHewyAACtUJMCkcvlCji+4oor1Lt3by1evFijRo1qlsJQv9q3jnw+n2RMECsCACD8NSkQrVmzprnrwEWqfeso37tXnXr2C25RAACEuUtaQ5STkyOfzyeHw6G+ffuqf//+zVUXzuPcW0cl+ceCWwwAAK1AkwJRYWGh7rvvPn344Yfq1KmTjDEqLi7W7bffrg0bNqhbt27NXScAAMBl06RXd8ycOVMlJSU6fPiwfvzxRxUVFenQoUMqKSnRrFmzmrtG2FB11Rn5fD4dOHBABw4cYK0UAOCyalIg2rp1q1atWqU+ffpY5/r27auXX35Z7733XrMVJ0nffvutHnzwQXXp0kUdOnTQjTfeqJycHOu6MUZpaWnyeDxq3769hg8frsOHDwf8jIqKCs2cOVNdu3ZVVFSUxo0bp5MnTzZrnWheZYUn9eIWrxZsPKgFGw/qhf/zkcp/Lg92WQCAVqpJgai6urreB+I5nU5VV1dfclE1ioqKdOutt8rpdOq9997TF198oZdeekmdOnWyxixZskRLly7VihUrtH//frndbt1xxx0qLS21xqSmpmrTpk3asGGDdu/erbKyMo0dO1ZVVVXNViuaX8e4RMUm9lZsYm9Fde0e7HIAAK1Yk9YQ/epXv9Ls2bP11ltvyePxSDo7k/PEE09oxIgRzVbcn/70JyUkJATsauvRo4f1a2OMli9frkWLFlnPRlq3bp3i4+O1fv16TZ06VcXFxVq9erXeeOMNjRw5UpKUlZWlhIQEbd++XaNHj262egEAQHhq0gzRihUrVFpaqh49euiaa67Rtddeq6SkJJWWliozM7PZitu8ebMGDRqkX//614qLi1P//v31+uuvW9fz8vJUUFAQ8OyjyMhIDRs2THv27JF0diec3+8PGOPxeJScnGyNqU9FRYVKSkoCPgAAoHVq0gxRQkKCDhw4oOzsbP3v//6vjDHq27evNQPTXL7++mutWrVKc+bM0VNPPaV9+/Zp1qxZioyM1EMPPaSCggJJUnx8fMD34uPjdfz4cUlSQUGB2rZtq86dO9cZU/P9+mRkZOi5555r1n4AAEBoatQM0QcffKC+fftasyV33HGHZs6cqVmzZummm27S9ddfr7/97W/NVlx1dbUGDBig9PR09e/fX1OnTtVjjz2mVatWBYxzOBwBx8aYOudqu9CYhQsXqri42PqcOHGi6Y0AAICQ1qhAtHz5cj322GOKiYmpc83lcmnq1KlaunRpsxXXvXt39e3bN+Bcnz599M0330iS3G63JNWZ6SksLLRmjdxutyorK1VUVNTgmPpERkYqJiYm4AMAAFqnRgWizz//XHfeeWeD10eNGhWwJf5S3XrrrTpy5EjAuaNHj+rqq6+WJCUlJcntdis7O9u6XllZqV27dmno0KGSpIEDB8rpdAaMyc/P16FDh6wxAADA3hq1huj777+vd7u99cMiIvTDDz9cclE1nnjiCQ0dOlTp6emaMGGC9u3bp9dee02vvfaapLO3ylJTU5Wenq5evXqpV69eSk9PV4cOHTRx4kRJZ2eupkyZorlz56pLly6KjY3VvHnzlJKS0uxrngAAQHhqVCC68sor5fV6de2119Z7/eDBg+revfmeF3PTTTdp06ZNWrhwoRYvXqykpCQtX75cDzzwgDVm/vz5Ki8v17Rp01RUVKTBgwdr27Ztio6OtsYsW7ZMERERmjBhgsrLyzVixAitXbtWbdq0abZaAQBA+GpUILrrrrv0hz/8QWPGjFG7du0CrpWXl+vZZ5/V2LFjm7XAsWPHnvdnOhwOpaWlKS0trcEx7dq1U2ZmZrM+EgAAALQejQpETz/9tDZu3Khf/OIXmjFjhnr37i2HwyGfz6eXX35ZVVVVWrRo0eWqFQAA4LJoVCCKj4/Xnj179Lvf/U4LFy6U+f9ftulwODR69GitXLnyvDu3AAAAQlGjH8x49dVXa8uWLSoqKtJXX30lY4x69epV58GHAAAA4aJJT6qWpM6dO+umm25qzloAAACCoknvMgMAAGhNCEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2mvykaiCYqqvOyOfzBZxLSUmR0+kMUkUAgHBGIEJYKis8qRe3nFY3n1+SVJJ/TCunSwMGDAhyZQCAcEQgQtjqGJeo2MTewS4DANAKsIYIAADYHoEIAADYHoEIAADYHoEIAADYHoEIAADYHoEIAADYHoEIAADYHoEIAADYHoEIAADYHoEIAADYHoEIAADYHoEIAADYHoEIAADYHoEIAADYHoEIAADYXkSwCwCaQ3XVGfl8voBzKSkpcjqdQaoIABBOCERoFcoKT+rFLafVzeeXJJXkH9PK6dKAAQOCXBkAIBwQiNBqdIxLVGxib0nMGAEAGodAhFaJGSMAQGMQiNBqnTtjBADA+bDLDAAA2B6BCAAA2B6BCAAA2B5riGAL7DoDAJwPgQi2wK4zAMD5EIhgG+w6AwA0hDVEAADA9ghEAADA9ghEAADA9ghEAADA9sIqEGVkZMjhcCg1NdU6Z4xRWlqaPB6P2rdvr+HDh+vw4cMB36uoqNDMmTPVtWtXRUVFady4cTp58mQLVw8AAEJV2ASi/fv367XXXtMNN9wQcH7JkiVaunSpVqxYof3798vtduuOO+5QaWmpNSY1NVWbNm3Shg0btHv3bpWVlWns2LGqqqpq6TYAAEAICotAVFZWpgceeECvv/66OnfubJ03xmj58uVatGiRxo8fr+TkZK1bt04///yz1q9fL0kqLi7W6tWr9dJLL2nkyJHq37+/srKy5PV6tX379mC1BAAAQkhYBKLp06fr7rvv1siRIwPO5+XlqaCgQKNGjbLORUZGatiwYdqzZ48kKScnR36/P2CMx+NRcnKyNQYAANhbyD+YccOGDTpw4ID2799f51pBQYEkKT4+PuB8fHy8jh8/bo1p27ZtwMxSzZia79enoqJCFRUV1nFJSUmTewAAAKEtpGeITpw4odmzZysrK0vt2rVrcJzD4Qg4NsbUOVfbhcZkZGTI5XJZn4SEhMYVDwAAwkZIB6KcnBwVFhZq4MCBioiIUEREhHbt2qW//OUvioiIsGaGas/0FBYWWtfcbrcqKytVVFTU4Jj6LFy4UMXFxdbnxIkTzdwdAAAIFSEdiEaMGCGv16vc3FzrM2jQID3wwAPKzc1Vz5495Xa7lZ2dbX2nsrJSu3bt0tChQyVJAwcOlNPpDBiTn5+vQ4cOWWPqExkZqZiYmIAPAABonUJ6DVF0dLSSk5MDzkVFRalLly7W+dTUVKWnp6tXr17q1auX0tPT1aFDB02cOFGS5HK5NGXKFM2dO1ddunRRbGys5s2bp5SUlDqLtAEAgD2FdCC6GPPnz1d5ebmmTZumoqIiDR48WNu2bVN0dLQ1ZtmyZYqIiNCECRNUXl6uESNGaO3atWrTpk0QKwcAAKEi7ALRhx9+GHDscDiUlpamtLS0Br/Trl07ZWZmKjMz8/IWBwAAwlJIryECAABoCQQiAABgewQiAABgewQiAABgewQiAABge2G3ywy4HPx+v7xer3WckpIip9MZxIoAAC2JQARbqq46I5/PZx37fD5l7jiqGE+SSvKPaeV0acCAAUGsEADQkghEsKWywpN6cctpdfP5JUn53r3q1LOfYhN7B7kyAEAwEIhgWx3jEq0AVJJ/LLjFAACCikXVAADA9ghEAADA9ghEAADA9ghEAADA9ghEAADA9thlBtRS+xlFEg9qBIDWjkAE1FL7GUU8qBEAWj8CEVCPc59RBABo/VhDBAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI/nEAEXwJOrAaD1IxABF8CTqwGg9SMQAReBJ1cDQOvGGiIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7bLsHGokHNQJA60MgAhqJBzUCQOtDIAKagAc1AkDrwhoiAABge8wQAZeINUUAEP4IRMAlYk0RAIQ/AhHQDFhTBADhjUAU4vx+v7xer3Xs8/kkY4JYEQAArQ+BKMR5vV5Ne3mzYrr3kCTle/eqU89+wS0KAIBWhkAUBmK697Bux5TkHwtuMQAAtEJsuwcAALbHDBHQzNiGDwDhh0AENDO24QNA+CEQAZfBudvwmTECgNAX0muIMjIydNNNNyk6OlpxcXG69957deTIkYAxxhilpaXJ4/Goffv2Gj58uA4fPhwwpqKiQjNnzlTXrl0VFRWlcePG6eTJky3ZCmzs7IyRVws2HtSCjQc17eXNAY9SAAAEX0gHol27dmn69On6+OOPlZ2drTNnzmjUqFH66aefrDFLlizR0qVLtWLFCu3fv19ut1t33HGHSktLrTGpqanatGmTNmzYoN27d6usrExjx45VVVVVMNqCDdXMGMUm9lbHuKvk8/l04MAB6+P3+4NdIgDYWkjfMtu6dWvA8Zo1axQXF6ecnBz98pe/lDFGy5cv16JFizR+/HhJ0rp16xQfH6/169dr6tSpKi4u1urVq/XGG29o5MiRkqSsrCwlJCRo+/btGj16dIv3BXtjjREAhJ6QniGqrbi4WJIUGxsrScrLy1NBQYFGjRpljYmMjNSwYcO0Z88eSVJOTo78fn/AGI/Ho+TkZGtMfSoqKlRSUhLwAZrLuTNGNQ/dBAAET9gEImOM5syZo9tuu03JycmSpIKCAklSfHx8wNj4+HjrWkFBgdq2bavOnTs3OKY+GRkZcrlc1ichIaE52wEAACEkbALRjBkzdPDgQb311lt1rjkcjoBjY0ydc7VdaMzChQtVXFxsfU6cONG0wgEAQMgLi0A0c+ZMbd68WTt37tRVV11lnXe73ZJUZ6ansLDQmjVyu92qrKxUUVFRg2PqExkZqZiYmIAPAABonUI6EBljNGPGDG3cuFEffPCBkpKSAq4nJSXJ7XYrOzvbOldZWaldu3Zp6NChkqSBAwfK6XQGjMnPz9ehQ4esMQAAwN5CepfZ9OnTtX79ev3Xf/2XoqOjrZkgl8ul9u3by+FwKDU1Venp6erVq5d69eql9PR0dejQQRMnTrTGTpkyRXPnzlWXLl0UGxurefPmKSUlxdp1BgAA7C2kA9GqVaskScOHDw84v2bNGj388MOSpPnz56u8vFzTpk1TUVGRBg8erG3btik6Otoav2zZMkVERGjChAkqLy/XiBEjtHbtWrVp06alWgEAACEspAORMeaCYxwOh9LS0pSWltbgmHbt2ikzM1OZmZnNWB0AAGgtQnoNEQAAQEsI6RkiwG78fn+d95zxIlgAuPwIRECQVVedkc/nkyT5fD5l7jiqGM/ZHZW81gMAWgaBCAiyc99tlu/dq049+yk2sXewywIAW2ENERACat5tFtW1e7BLAQBbIhABAADb45YZEMLOXV8knV107XA4FBHx//7qsugaAC4dgQgIYeeuL5KkfO9eRXTsrG5J10mqu+iaXWoA0DQEIiDE1awvks4GIGdMnHVcewaJXWoA0DQEIiCM1TeDxC41AGg8AhEQ5mrPIAEAGo9dZgAAwPYIRAAAwPa4ZQa0YrUXXUvsOgOA+hCIgFas9qJrdp0BQP0IREArd+6iawBA/VhDBAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9dZgAsfr9fXq/XOuaZRQDsgkAE2EjtBzX6/X45HA5FRJz9T4HP51PmjqOK8STxzCIAtkIgAmyk9oMa8717FdGxs7olXWcdd+rZj+cWAbAdAhFgM+c+qLEk/5icMXEBxzV47QcAOyEQAagXr/0AYCcEIgAN4rUfAOyCbfcAAMD2mCECcFFYUwSgNSMQAbgotdcUnfr2/2r2SJ/69OljjSEgAQhXBCIAF632DrUXt3gvetF17Yc+SgQoAKGDQASgyc4NSBe6peb1ejXt5c2K6d5DErvWAIQWAhGAZnEx2/Rjuvdg1xqAkEQgAtBszjdj5PP5JGOCVRoAnBeBCMBlUd9rQjr17NfgeNYYAQgmAhGAy6b2IuzzYY0RgGAiEAEIivpuqcW4r27wlhuzRQAuJwIRgKC40C21c6+zpR/A5UYgAhA0F7qldrHvUuN2G4BLRSACEPIu5rUhbOkHcCkIRCGm9tQ/W5WBC782pPbfkwsFqNp/z/x+vxwOhyIiIuod3xjcvgPCE4EoxNSe+r/QVmXALs732pDzrT+S6g9QmTuOKsaTZH0/omNndUu6zvr5Tb3lxu07IDwRiELQuVP/F9qqDNhVY9YfNRSgzr3ujIm7qFtuFzMDxO07IPwQiADYQmOeiVT7ltu5t9Rqzy5d6PbdhXCLDQgNBCIAqKW+RwLU3FKrb3bpfLfvLudLbwlTQPMhEAFAPWrPKNXcUruY23Pnupj1TOc+kLIxQmm9EuEM4Y5ABACXWWMWhJ/vdl3t4ws93buxu+cuJdSEUjgDmsJWgWjlypX685//rPz8fF1//fVavny5/umf/inYZQGwmcbMKNXeAVff7buL/W7tkFLfYz7OXR91ofGNCWdS48LYhcLc+cafL0ReTC2wJ9sEorffflupqalauXKlbr31Vr366qsaM2aMvvjiCyUmJga7PACwNHS7rvbxxdy+O/e79b0/rvbjB85dH3Ux4y82nNW+VVg7pFzoUQiNeXTC+ULkxdRCgLIn2wSipUuXasqUKXr00UclScuXL9f777+vVatWKSMjI8jVAcDl19D74xozW9XQ4wqa8uiD2qHlfI9CaMyjE84XIi+2lvMFKImA1BrZIhBVVlYqJydHCxYsCDg/atQo7dmzp97vVFRUqKKiwjouLi6WJJWUlDR7fbm5udavjxw5oh+Pf60zFeVnf7/844ooLZYzwtHsx5fzZ1NL+P3e1GKTWjp2sv77UuWvVPG3XzbL+Kb83g5/RcjWcu5x2fcn9Yd/OyJXfI4k6ecfv9fcXw9X7948a6o53XjjjZfl59b8f9tc4HEYtghEf//731VVVaX4+PiA8/Hx8SooKKj3OxkZGXruuefqnE9ISLgsNQIAwsdvs9cHuwQ0UmlpqVwuV4PXbRGIajgcjoBjY0ydczUWLlyoOXPmWMfV1dX68ccf1aVLlwa/czFKSkqUkJCgEydOKCYmpsk/JxTRW3iit/DVmvujt/AUir0ZY1RaWiqPx3PecbYIRF27dlWbNm3qzAYVFhbWmTWqERkZqcjIyIBznTp1araaYmJiQuZfluZGb+GJ3sJXa+6P3sJTqPV2vpmhGle0QB1B17ZtWw0cOFDZ2dkB57OzszV06NAgVQUAAEKFLWaIJGnOnDmaNGmSBg0apCFDhui1117TN998o8cffzzYpQEAgCCzTSD6zW9+o3/84x9avHix8vPzlZycrC1btujqq69u0ToiIyP17LPP1rkd1xrQW3iit/DVmvujt/AUzr05zIX2oQEAALRytlhDBAAAcD4EIgAAYHsEIgAAYHsEIgAAYHsEoha0cuVKJSUlqV27dho4cKD+9re/BbukC/roo490zz33yOPxyOFw6N133w24boxRWlqaPB6P2rdvr+HDh+vw4cMBYyoqKjRz5kx17dpVUVFRGjdunE6ePNmCXdQvIyNDN910k6KjoxUXF6d7771XR44cCRgTrv2tWrVKN9xwg/VwtCFDhui9996zrodrX/XJyMiQw+FQamqqdS5c+0tLS5PD4Qj4uN1u63q49lXj22+/1YMPPqguXbqoQ4cOuvHGG5WTk2NdD+f+evToUefPzuFwaPr06ZLCu7czZ87o6aefVlJSktq3b6+ePXtq8eLFqq6utsaEc38WgxaxYcMG43Q6zeuvv26++OILM3v2bBMVFWWOHz8e7NLOa8uWLWbRokXmnXfeMZLMpk2bAq6/8MILJjo62rzzzjvG6/Wa3/zmN6Z79+6mpKTEGvP444+bK6+80mRnZ5sDBw6Y22+/3fTr18+cOXOmhbsJNHr0aLNmzRpz6NAhk5uba+6++26TmJhoysrKrDHh2t/mzZvN//zP/5gjR46YI0eOmKeeeso4nU5z6NAhY0z49lXbvn37TI8ePcwNN9xgZs+ebZ0P1/6effZZc/3115v8/HzrU1hYaF0P176MMebHH380V199tXn44YfNJ598YvLy8sz27dvNV199ZY0J5/4KCwsD/tyys7ONJLNz505jTHj39sc//tF06dLF/PWvfzV5eXnmP//zP03Hjh3N8uXLrTHh3F8NAlELufnmm83jjz8ecO66664zCxYsCFJFjVc7EFVXVxu3221eeOEF69zp06eNy+Uyr7zyijHGmFOnThmn02k2bNhgjfn222/NFVdcYbZu3dpitV+MwsJCI8ns2rXLGNP6+uvcubP5t3/7t1bTV2lpqenVq5fJzs42w4YNswJROPf37LPPmn79+tV7LZz7MsaYJ5980tx2220NXg/3/mqbPXu2ueaaa0x1dXXY93b33XebRx55JODc+PHjzYMPPmiMaT1/dtwyawGVlZXKycnRqFGjAs6PGjVKe/bsCVJVly4vL08FBQUBfUVGRmrYsGFWXzk5OfL7/QFjPB6PkpOTQ6734uJiSVJsbKyk1tNfVVWVNmzYoJ9++klDhgxpNX1Nnz5dd999t0aOHBlwPtz7+/LLL+XxeJSUlKT77rtPX3/9taTw72vz5s0aNGiQfv3rXysuLk79+/fX66+/bl0P9/7OVVlZqaysLD3yyCNyOBxh39ttt92mHTt26OjRo5Kkzz//XLt379Zdd90lqfX82dnmSdXB9Pe//11VVVV1XiQbHx9f54Wz4aSm9vr6On78uDWmbdu26ty5c50xodS7MUZz5szRbbfdpuTkZEnh35/X69WQIUN0+vRpdezYUZs2bVLfvn2t//iEa1+StGHDBh04cED79++vcy2c/9wGDx6s//iP/9AvfvELff/99/rjH/+ooUOH6vDhw2HdlyR9/fXXWrVqlebMmaOnnnpK+/bt06xZsxQZGamHHnoo7Ps717vvvqtTp07p4YcflhTe/05K0pNPPqni4mJdd911atOmjaqqqvT888/r/vvvlxT+/dUgELUgh8MRcGyMqXMuHDWlr1DrfcaMGTp48KB2795d51q49te7d2/l5ubq1KlTeueddzR58mTt2rXLuh6ufZ04cUKzZ8/Wtm3b1K5duwbHhWN/Y8aMsX6dkpKiIUOG6JprrtG6det0yy23SArPviSpurpagwYNUnp6uiSpf//+Onz4sFatWqWHHnrIGheu/Z1r9erVGjNmjDweT8D5cO3t7bffVlZWltavX6/rr79eubm5Sk1Nlcfj0eTJk61x4dpfDW6ZtYCuXbuqTZs2dVJwYWFhnUQdTmp2v5yvL7fbrcrKShUVFTU4JthmzpypzZs3a+fOnbrqqqus8+HeX9u2bXXttddq0KBBysjIUL9+/fSv//qvYd9XTk6OCgsLNXDgQEVERCgiIkK7du3SX/7yF0VERFj1hWt/54qKilJKSoq+/PLLsP9z6969u/r27Rtwrk+fPvrmm28khf/ftxrHjx/X9u3b9eijj1rnwr233//+91qwYIHuu+8+paSkaNKkSXriiSeUkZEhKfz7q0EgagFt27bVwIEDlZ2dHXA+OztbQ4cODVJVly4pKUlutzugr8rKSu3atcvqa+DAgXI6nQFj8vPzdejQoaD3bozRjBkztHHjRn3wwQdKSkoKuB7u/dVmjFFFRUXY9zVixAh5vV7l5uZan0GDBumBBx5Qbm6uevbsGdb9nauiokI+n0/du3cP+z+3W2+9tc5jLY4ePWq9YDvc+6uxZs0axcXF6e6777bOhXtvP//8s664IjAutGnTxtp2H+79WVp2Dbd91Wy7X716tfniiy9MamqqiYqKMseOHQt2aedVWlpqPvvsM/PZZ58ZSWbp0qXms88+sx4X8MILLxiXy2U2btxovF6vuf/+++vdannVVVeZ7du3mwMHDphf/epXIbHV8ne/+51xuVzmww8/DNgu+/PPP1tjwrW/hQsXmo8++sjk5eWZgwcPmqeeespcccUVZtu2bcaY8O2rIefuMjMmfPubO3eu+fDDD83XX39tPv74YzN27FgTHR1t/XciXPsy5uwjEiIiIszzzz9vvvzyS/Pmm2+aDh06mKysLGtMOPdnjDFVVVUmMTHRPPnkk3WuhXNvkydPNldeeaW17X7jxo2ma9euZv78+daYcO6vBoGoBb388svm6quvNm3btjUDBgywtneHsp07dxpJdT6TJ082xpzdbvnss88at9ttIiMjzS9/+Uvj9XoDfkZ5ebmZMWOGiY2NNe3btzdjx44133zzTRC6CVRfX5LMmjVrrDHh2t8jjzxi/bvWrVs3M2LECCsMGRO+fTWkdiAK1/5qnt3idDqNx+Mx48ePN4cPH7auh2tfNf77v//bJCcnm8jISHPdddeZ1157LeB6uPf3/vvvG0nmyJEjda6Fc28lJSVm9uzZJjEx0bRr18707NnTLFq0yFRUVFhjwrm/Gg5jjAnK1BQAAECIYA0RAACwPQIRAACwPQIRAACwPQIRAACwPQIRAACwPQIRAACwPQIRAACwPQIRAACwPQIRAACwPQIRAACwPQIRAACwPQIRAACwvf8PI0YQ1o4pKBQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of review length\n",
    "sns.histplot(review_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71af02-9e66-44f7-8512-cb86e894334d",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c246439-ca3c-4821-980b-d4443a011148",
   "metadata": {},
   "source": [
    "- Split raw data into train-val-test sets\n",
    "- Manually Normalize\n",
    "  - Remove punctuation\n",
    "  - remove stop words\n",
    "  - lowercase\n",
    "  - part of speech tags\n",
    "  - lemmanize\n",
    "- Normalize by creating tensorflow datasets\n",
    "  - CountVectorization\n",
    "  - TfidfVecorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e8c60f7-7a59-4d10-9977-55dfea3ffd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Labels\n",
      "  Yes Reviews:\n",
      "     Count: 6245\n",
      "     Percent: 33.69%\n",
      "  No Reviews:\n",
      "     Count: 12291\n",
      "     Percent: 66.31%\n",
      "\n",
      "Validation Labels\n",
      "  Yes Reviews:\n",
      "     Count: 781\n",
      "     Percent: 33.71%\n",
      "  No Reviews:\n",
      "     Count: 1536\n",
      "     Percent: 66.29%\n",
      "\n",
      "Testing Labels\n",
      "  Yes Reviews:\n",
      "     Count: 781\n",
      "     Percent: 33.69%\n",
      "  No Reviews:\n",
      "     Count: 1537\n",
      "     Percent: 66.31%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make 80-10-10 train-validation-test split\n",
    "\n",
    "# First split with stratification\n",
    "train_reviews, temp_reviews, train_labels, temp_labels = train_test_split(reviews, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "# Second split for val and test sets with stratification\n",
    "val_reviews, test_reviews, val_labels, test_labels = train_test_split(temp_reviews, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42)\n",
    "\n",
    "def label_distribution(name,labels):\n",
    "    total = len(labels)\n",
    "    yes_counts = (labels==1).sum()\n",
    "    yes_rate = yes_counts/total\n",
    "    no_counts = (labels==0).sum()\n",
    "    no_rate = no_counts/total\n",
    "    print(f'{name} Labels')\n",
    "    print('  Yes Reviews:')\n",
    "    print(f'     Count: {yes_counts}')\n",
    "    print(f'     Percent: {round(100*yes_rate,2)}%')\n",
    "    print('  No Reviews:')\n",
    "    print(f'     Count: {no_counts}')\n",
    "    print(f'     Percent: {round(100*no_rate,2)}%')\n",
    "    print('')\n",
    "\n",
    "label_distribution('Training',train_labels)\n",
    "label_distribution('Validation',val_labels)\n",
    "label_distribution('Testing',test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7044ae49-c886-41aa-8e9b-226892ea9fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't need to do this as I will include it in my pipeline\n",
    "# Manual Normalization including stopword removal\n",
    "# STOPWORDS = stopwords.words('english')\n",
    "\n",
    "# X_train = [clean_text(review, stop_words = STOPWORDS) for review in train_reviews]\n",
    "# X_val = [clean_text(review, stop_words = STOPWORDS) for review in val_reviews]\n",
    "# X_test = [clean_text(review, stop_words = STOPWORDS) for review in test_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2792b1e-df90-4873-9622-3390a054abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensorflow datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import tensorflow as tf\n",
    "\n",
    "def vectorize_text(train_reviews, val_reviews, test_reviews, stop_words, use_tf_idf=False):\n",
    "    \"\"\"\n",
    "    Vectorizes text data. Optionally applies TF-IDF transformation.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(decode_error='replace', strip_accents='unicode', stop_words=stop_words,\n",
    "                                 ngram_range=(1, 2), max_df=0.95, min_df=2)\n",
    "    if use_tf_idf:\n",
    "        pipeline = make_pipeline(vectorizer, TfidfTransformer())\n",
    "        X_train = pipeline.fit_transform(train_reviews)\n",
    "        X_val = pipeline.transform(val_reviews)\n",
    "        X_test = pipeline.transform(test_reviews)\n",
    "    else:\n",
    "        X_train = vectorizer.fit_transform(train_reviews)\n",
    "        X_val = vectorizer.transform(val_reviews)\n",
    "        X_test = vectorizer.transform(test_reviews)\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "def select_features(X_train, train_labels, X_val, X_test, k_best=20000):\n",
    "    \"\"\"\n",
    "    Selects the k best features from the vectorized text data.\n",
    "    \"\"\"\n",
    "    selector = SelectKBest(k=min(k_best, X_train.shape[1]))\n",
    "    selector.fit(X_train, train_labels)\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    return X_train_selected, X_val_selected, X_test_selected\n",
    "\n",
    "def create_tf_datasets(X_train_selected, train_labels, X_val_selected, val_labels, X_test_selected, test_labels):\n",
    "    \"\"\"\n",
    "    Converts selected features and labels into TensorFlow datasets.\n",
    "    \"\"\"\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train_selected.toarray(), train_labels.astype('float32')))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val_selected.toarray(), val_labels.astype('float32')))\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test_selected.toarray(), test_labels.astype('float32')))\n",
    "    \n",
    "    train_ds = train_ds.shuffle(1000, reshuffle_each_iteration=True).batch(64).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(64).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.batch(64).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def prepare_datasets(train_reviews, train_labels, val_reviews, val_labels, test_reviews, test_labels, stop_words, use_tf_idf=False, k_best=20000):\n",
    "    \"\"\"\n",
    "    Prepares training and validation datasets by vectorizing text data, selecting features,\n",
    "    and converting to TensorFlow datasets.\n",
    "    \"\"\"\n",
    "    X_train, X_val, X_test = vectorize_text(train_reviews, val_reviews, test_reviews, stop_words, use_tf_idf)\n",
    "    X_train_selected, X_val_selected, X_test_selected = select_features(X_train, train_labels, X_val, X_test, k_best)\n",
    "    train_ds, val_ds, test_ds = create_tf_datasets(X_train_selected, train_labels, X_val_selected, val_labels, X_test_selected, test_labels)\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "# Counts\n",
    "# train_ds_counts, val_ds_counts, test_ds_counts = prepare_datasets(train_reviews, train_labels, val_reviews, val_labels, test_reviews, test_labels, STOPWORDS)\n",
    "\n",
    "# Tf-idf\n",
    "# train_ds_tfidf, val_ds_tfidf, test_ds_tfidf = prepare_datasets(train_reviews, train_labels, val_reviews, val_labels, test_reviews, test_labels, STOPWORDS, use_tf_idf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa53d6-426f-4328-9389-b3db511d723f",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1618b676-1d95-4eed-a8f1-c1d2b438d62e",
   "metadata": {},
   "source": [
    "- CountVectorizer vs TfidfVectorizer\n",
    "- Basic\n",
    "  - Dummy\n",
    "  - Baseline: LogisticRegression\n",
    "  - MultinomialNB\n",
    "  - SVC\n",
    "- Ensemble\n",
    "  - GradientBoostingClassifier\n",
    "  - RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c445d9-0bd1-4963-a3d3-eb0d76c004e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "class TextCleanerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stop_words=None):\n",
    "        self.stop_words = stop_words\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Nothing to do here\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        cleaned_reviews = []\n",
    "        for review in X:\n",
    "            cleaned_reviews.append(self.clean_text(review, self.stop_words))\n",
    "        return cleaned_reviews\n",
    "    \n",
    "    def clean_text(self, review, stop_words):\n",
    "        tokenizer = RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\")\n",
    "        tokens = tokenizer.tokenize(review)\n",
    "        if stop_words is None:\n",
    "            tokens = [word.lower() for word in tokens]\n",
    "        else:\n",
    "            tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        pos_tags = pos_tag(tokens)\n",
    "        wordnet_tags = [(word, self.get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word, tag) for word, tag in wordnet_tags]\n",
    "        return ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    def get_wordnet_pos(self, treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "def model_cv_scores(pipeline, model_name, X= None, y= None, scoring=None, cv=None):\n",
    "    \"\"\"\n",
    "    Performs cross-validation on the given pipeline and data, displaying the results.\n",
    "    \n",
    "    Parameters:\n",
    "    - pipeline: The scikit-learn pipeline to evaluate.\n",
    "    - model_name: A name for the model, used in the output DataFrame.\n",
    "    - X: The feature matrix. Default is None. Should be provided explicitly.\n",
    "    - y: The target vector. Default is None. Should be provided explicitly.\n",
    "    - scoring: Scoring parameter for cross-validation. Default is None. Should be provided explicitly.\n",
    "    - cv: Cross-validation splitting strategy. Default is None. Should be provided explicitly.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary containing the cross-validation scores.\n",
    "    \"\"\"\n",
    "    scores = cross_validate(pipeline, X, y, scoring=scoring, cv=cv, n_jobs=-1, return_train_score=False)\n",
    "    \n",
    "    # Calculating mean values and rounding them for readability\n",
    "    time = round((scores['fit_time'] + scores['score_time']).mean())\n",
    "    accuracy = round(scores['test_accuracy'].mean(), 4)\n",
    "    roc_auc = round(scores['test_roc_auc'].mean(), 4)\n",
    "    \n",
    "    # Creating and displaying a DataFrame with the results\n",
    "    results_df = pd.DataFrame([[accuracy, roc_auc, time]], \n",
    "                              index=[model_name], \n",
    "                              columns=['Accuracy', 'ROC AUC', 'Time (s)'])\n",
    "    display(results_df)\n",
    "\n",
    "def RandCV_scores(model_name,random_search):\n",
    "    index = random_search.best_index_\n",
    "    results = random_search.cv_results_\n",
    "\n",
    "    # Calculating mean values and rounding them for readability\n",
    "    time = round(results['mean_fit_time'][index] + results['mean_score_time'][index])\n",
    "    accuracy = round(results['mean_test_accuracy'][index], 4)\n",
    "    roc_auc = round(results['mean_test_roc_auc'][index], 4)\n",
    "    \n",
    "    # Creating and displaying a DataFrame with the results\n",
    "    results_df = pd.DataFrame([[accuracy, roc_auc, time]], \n",
    "                              index=[model_name+'_RandCV'], \n",
    "                              columns=['Accuracy', 'ROC AUC', 'Time (s)'])\n",
    "    display(results_df)\n",
    "    return random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55d92c06-ed2d-49b3-9cc7-bc4dd95b744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Instantiating processing transformers\n",
    "text_cleaner = TextCleanerTransformer(stop_words=STOPWORDS)\n",
    "vectorizer = CountVectorizer(decode_error='replace', strip_accents='unicode', stop_words=None, ngram_range=(1, 2), max_df=0.95, min_df=2)\n",
    "tf_idf = TfidfTransformer()\n",
    "k_best = SelectKBest(k=10000)\n",
    "\n",
    "# Setting CV parameters\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "SCORING = {'accuracy':'accuracy',\n",
    "           'roc_auc':'roc_auc'}\n",
    "\n",
    "# Concatenating the valildation set as I don't need it here. 90-10 split\n",
    "X_train = pd.concat([train_reviews, val_reviews])\n",
    "y_train = pd.concat([train_labels, val_labels])\n",
    "\n",
    "# Clean data before entering the pipeline for efficiency\n",
    "X_train_clean = text_cleaner.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac2eab-439f-451e-bbaf-9f4614339d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Model\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf = DummyClassifier(strategy='uniform', random_state=42)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"dummy\", clf)])\n",
    "\n",
    "model_cv_scores(pipe, 'Dummy', X= X_train_clean, y= y_train, scoring=SCORING, cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22dc7ff-d934-4cb9-b4e3-e245cadac751",
   "metadata": {},
   "source": [
    "### Bag of Words Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821affd0-113b-4f8b-a176-ff7cc4e29d77",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fe813-7c36-4842-b676-684a2d785d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"log_reg\", clf)])\n",
    "\n",
    "model_cv_scores(pipe, 'log_reg', X= X_train_clean, y= y_train, scoring=SCORING, cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d7d98-39f2-48ee-827a-280b69a076f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression RandCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"log_reg\", clf)])\n",
    "\n",
    "params = {\n",
    "    'log_reg__C': loguniform(0.001, 100),\n",
    "    'log_reg__solver': ['liblinear', 'lbfgs', 'sag', 'newton-cg'],\n",
    "    'log_reg__max_iter': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(estimator= pipe, \n",
    "                        param_distributions= params,\n",
    "                        n_iter=100, \n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        scoring=SCORING,\n",
    "                        refit= 'roc_auc',\n",
    "                        random_state=42)\n",
    "rand_search = rs.fit(X_train_clean, y_train)\n",
    "RandCV_scores('log_reg',rand_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6c42e-b603-4d16-aa36-3783d8fdad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"mnb\", clf)])\n",
    "\n",
    "model_cv_scores(pipe, 'mnb', X= X_train_clean, y= y_train, scoring=SCORING, cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee05392-607f-40aa-afed-912ba97936eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB RandCV\n",
    "clf = MultinomialNB()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"mnb\", clf)])\n",
    "\n",
    "params = {\n",
    "    'mnb__alpha': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "    'mnb__fit_prior': [True, False],\n",
    "    'mnb__class_prior': [None,[0.66,0.34]]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(estimator= pipe, \n",
    "                        param_distributions= params,\n",
    "                        n_iter=100, \n",
    "                        cv=5,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        scoring=SCORING,\n",
    "                        refit= 'roc_auc',\n",
    "                        random_state=42)\n",
    "rand_search = rs.fit(X_train_clean, y_train)\n",
    "RandCV_scores('mnb',rand_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7efc7-1c7b-49f3-9fb9-e4e5e80ae696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(C=1.0, kernel='rbf', gamma='scale')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"svc\", clf)])\n",
    "\n",
    "model_cv_scores(pipe, 'svc', X= X_train_clean, y= y_train, scoring=SCORING, cv=skf)\n",
    "# Too Slow even for one round, not doing RandomizedCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef7958-3dc0-4ac5-97a0-b9dfa37f391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "                                    max_depth=3, min_samples_split=2,\n",
    "                                    max_features=None)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"gbc\", clf)])\n",
    "\n",
    "model_cv_scores(pipe, 'gbc', X= X_train_clean, y= y_train, scoring=SCORING, cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2033f9-f9be-4bf2-894b-ae325a2df3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier RandCV\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"gbc\", clf)])\n",
    "\n",
    "params = {\n",
    "    'gbc__n_estimators': [100, 200, 300],  # Number of boosting stages\n",
    "    'gbc__learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate\n",
    "    'gbc__max_depth': [3, 4, 5, 6],  # Maximum depth of the individual estimators\n",
    "    'gbc__min_samples_split': [2, 4, 6],  # Minimum number of samples required to split an internal node\n",
    "    'gbc__max_features': [None, 'sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
    "    'gbc__subsample': [0.8, 0.9, 1.0],  # The fraction of samples to be used for fitting the individual base learners\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(estimator= pipe, \n",
    "                        param_distributions= params,\n",
    "                        n_iter=100, \n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        scoring=SCORING,\n",
    "                        refit= 'roc_auc',\n",
    "                        random_state=42)\n",
    "rand_search = rs.fit(X_train_clean, y_train)\n",
    "RandCV_scores('gbc',rand_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707c356-6d93-4eb2-9e4f-0c0edbd25ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=None,\n",
    "                                min_samples_split=2, min_samples_leaf=1,\n",
    "                                max_features='sqrt')\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"rfc\", clf)])\n",
    "\n",
    "model_cv_scores(pipe, 'rfc', X= X_train_clean, y= y_train, scoring=SCORING, cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed9ddd4-1675-4f77-aa50-f8bc3b60c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier RandCV\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"rfc\", clf)])\n",
    "\n",
    "params = {\n",
    "    'rfc__n_estimators': [100, 200, 300, 400, 500],  # Number of trees\n",
    "    'rfc__max_depth': [None, 10, 20, 30, 40, 50],  # Maximum depth of the trees\n",
    "    'rfc__min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'rfc__min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'rfc__max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider for the best split\n",
    "    'rfc__bootstrap': [True, False],  # Method of selecting samples for training each tree\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(estimator= pipe, \n",
    "                        param_distributions= params,\n",
    "                        n_iter=100, \n",
    "                        cv=5,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        scoring=SCORING,\n",
    "                        refit= 'roc_auc',\n",
    "                        random_state=42)\n",
    "rand_search = rs.fit(X_train_clean, y_train)\n",
    "RandCV_scores('rfc',rand_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf641b-7044-4f7d-8b50-2441b87995a0",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('tf_idf',tf_idf),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"log_reg\", clf)])\n",
    "\n",
    "model_cv_scores(pipe, 'log_reg', X= X_train_clean, y= y_train, scoring=SCORING, cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60724d-95e3-4146-abd6-a5808a977d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB\n",
    "clf = MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('tf_idf',tf_idf),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"mnb\", clf)])\n",
    "\n",
    "model_cv_scores(pipe, 'mnb', X= X_train_clean, y= y_train, scoring=SCORING, cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72d71f-2c07-4ffd-8a10-b7e0d603f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "clf = SVC(C=1.0, kernel='rbf', gamma='scale')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('tf_idf',tf_idf),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"svc\", clf)])\n",
    "\n",
    "model_cv_scores(pipe, 'svc', X= X_train_clean, y= y_train, scoring=SCORING, cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c392f3-c9ef-4a91-b973-30a89451aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "                                    max_depth=3, min_samples_split=2,\n",
    "                                    max_features=None)\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('tf_idf',tf_idf),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"gbc\", clf)])\n",
    "\n",
    "model_cv_scores(pipe, 'gbc', X= X_train_clean, y= y_train, scoring=SCORING, cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f0b21-73b4-42d0-b2e2-6c7d562ebb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=None,\n",
    "                                min_samples_split=2, min_samples_leaf=1,\n",
    "                                max_features='sqrt')\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('tf_idf',tf_idf),\n",
    "    ('feature_selection', k_best),\n",
    "    (\"rfc\", clf)])\n",
    "\n",
    "model_cv_scores(pipe, 'rfc', X= X_train_clean, y= y_train, scoring=SCORING, cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94764d9-ab51-440e-b630-ac5e93415f61",
   "metadata": {},
   "source": [
    "### Multi-layer perceptrons (MLPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "848a5266-18bd-451a-a233-f66688ee3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "CALLBACKS = [tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
    "                                              patience=5, \n",
    "                                              restore_best_weights=True,\n",
    "                                              verbose=1,\n",
    "                                              start_from_epoch=5)]\n",
    "# Concatenating the valildation set as I don't need it here. 90-10 split\n",
    "X_train = pd.concat([train_reviews, val_reviews])\n",
    "y_train = pd.concat([train_labels, val_labels])\n",
    "\n",
    "# Clean data before entering the pipeline for efficiency\n",
    "X_train_clean = text_cleaner.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2ed51f6-ad8b-4b63-bebe-80ab3cc5fe15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7d86dc5-6e83-41a0-90fc-0e9dd17c24ab",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 21:44:01.020589: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-27 21:44:01.021428: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-27 21:44:01.021805: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-27 21:44:01.026908: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-27 21:44:01.029234: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "522/522 [==============================] - 3s 4ms/step - loss: 0.4176 - accuracy: 0.8530 - auc: 0.9253\n",
      "506/522 [============================>.] - ETA: 0s - loss: 0.4311 - accuracy: 0.8342 - auc: 0.9186Epoch 2/100\n",
      "522/522 [==============================] - 3s 4ms/step - loss: 0.4266 - accuracy: 0.8364 - auc: 0.9198\n",
      "522/522 [==============================] - 3s 4ms/step - loss: 0.4485 - accuracy: 0.8709 - auc: 0.9256\n",
      "Epoch 2/100\n",
      "Epoch 2/100\n",
      "522/522 [==============================] - 3s 4ms/step - loss: 0.4378 - accuracy: 0.8563 - auc: 0.9257\n",
      " 33/522 [>.............................] - ETA: 1s - loss: 0.2472 - accuracy: 0.9223 - auc: 0.9680Epoch 2/100\n",
      "522/522 [==============================] - 3s 4ms/step - loss: 0.4283 - accuracy: 0.8250 - auc: 0.9171\n",
      " 29/522 [>.............................] - ETA: 1s - loss: 0.2552 - accuracy: 0.9084 - auc: 0.9697Epoch 2/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.2246 - accuracy: 0.9193 - auc: 0.9704\n",
      "Epoch 3/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 0.2284 - accuracy: 0.9178 - auc: 0.9686\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 0.2295 - accuracy: 0.9186 - auc: 0.9696\n",
      "Epoch 3/100\n",
      "Epoch 3/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 0.2296 - accuracy: 0.9196 - auc: 0.9696\n",
      "Epoch 3/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 0.2307 - accuracy: 0.9158 - auc: 0.9681\n",
      "  1/522 [..............................] - ETA: 34s - loss: 0.1324 - accuracy: 0.9688 - auc: 1.0000Epoch 3/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.1752 - accuracy: 0.9374 - auc: 0.9808\n",
      "450/522 [========================>.....] - ETA: 0s - loss: 0.1841 - accuracy: 0.9326 - auc: 0.9784Epoch 4/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.1790 - accuracy: 0.9336 - auc: 0.9798\n",
      "Epoch 4/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.1788 - accuracy: 0.9359 - auc: 0.9799\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.1792 - accuracy: 0.9350 - auc: 0.9796\n",
      " 16/522 [..............................] - ETA: 1s - loss: 0.1601 - accuracy: 0.9492 - auc: 0.9826 Epoch 4/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.1819 - accuracy: 0.9332 - auc: 0.9790\n",
      "Epoch 4/100\n",
      " 79/522 [===>..........................] - ETA: 1s - loss: 0.1562 - accuracy: 0.9403 - auc: 0.9852Epoch 4/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.1455 - accuracy: 0.9489 - auc: 0.9867\n",
      "Epoch 5/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.1489 - accuracy: 0.9479 - auc: 0.9861\n",
      "387/522 [=====================>........] - ETA: 0s - loss: 0.1505 - accuracy: 0.9480 - auc: 0.9854Epoch 5/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.1493 - accuracy: 0.9463 - auc: 0.9858\n",
      " 16/522 [..............................] - ETA: 3s - loss: 0.1078 - accuracy: 0.9688 - auc: 0.9938Epoch 5/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.1528 - accuracy: 0.9447 - auc: 0.9852\n",
      " 70/522 [===>..........................] - ETA: 2s - loss: 0.1313 - accuracy: 0.9549 - auc: 0.9891Epoch 5/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 0.1499 - accuracy: 0.9462 - auc: 0.9856\n",
      " 78/522 [===>..........................] - ETA: 2s - loss: 0.1385 - accuracy: 0.9499 - auc: 0.9880Epoch 5/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 0.1237 - accuracy: 0.9567 - auc: 0.9905\n",
      "494/522 [===========================>..] - ETA: 0s - loss: 0.1266 - accuracy: 0.9558 - auc: 0.9899Epoch 6/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 0.1268 - accuracy: 0.9559 - auc: 0.9899\n",
      "448/522 [========================>.....] - ETA: 0s - loss: 0.1303 - accuracy: 0.9535 - auc: 0.9893Epoch 6/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 0.1273 - accuracy: 0.9549 - auc: 0.9897\n",
      "484/522 [==========================>...] - ETA: 0s - loss: 0.1302 - accuracy: 0.9533 - auc: 0.9893Epoch 6/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 0.1311 - accuracy: 0.9531 - auc: 0.9891\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.1285 - accuracy: 0.9559 - auc: 0.9894\n",
      "Epoch 6/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.1057 - accuracy: 0.9654 - auc: 0.9931\n",
      "486/522 [==========================>...] - ETA: 0s - loss: 0.1096 - accuracy: 0.9636 - auc: 0.9926Epoch 7/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.1087 - accuracy: 0.9640 - auc: 0.9927\n",
      "Epoch 7/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 0.1092 - accuracy: 0.9627 - auc: 0.9925\n",
      "Epoch 7/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 0.1108 - accuracy: 0.9627 - auc: 0.9921\n",
      "Epoch 7/100\n",
      "122/522 [======>.......................] - ETA: 1s - loss: 0.0881 - accuracy: 0.9754 - auc: 0.9958Epoch 6/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0908 - accuracy: 0.9709 - auc: 0.9951\n",
      "Epoch 8/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0936 - accuracy: 0.9700 - auc: 0.9948\n",
      "388/522 [=====================>........] - ETA: 0s - loss: 0.0951 - accuracy: 0.9701 - auc: 0.9945Epoch 8/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0939 - accuracy: 0.9705 - auc: 0.9946\n",
      "Epoch 8/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0961 - accuracy: 0.9691 - auc: 0.9941\n",
      "Epoch 8/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.1134 - accuracy: 0.9603 - auc: 0.9920\n",
      "300/522 [================>.............] - ETA: 0s - loss: 0.0761 - accuracy: 0.9769 - auc: 0.9971Epoch 7/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0777 - accuracy: 0.9764 - auc: 0.9967\n",
      "470/522 [==========================>...] - ETA: 0s - loss: 0.0795 - accuracy: 0.9756 - auc: 0.9966Epoch 9/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0807 - accuracy: 0.9750 - auc: 0.9963\n",
      "502/522 [===========================>..] - ETA: 0s - loss: 0.0810 - accuracy: 0.9762 - auc: 0.9961Epoch 9/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0811 - accuracy: 0.9760 - auc: 0.9961\n",
      "406/522 [======================>.......] - ETA: 0s - loss: 0.0840 - accuracy: 0.9743 - auc: 0.9954Epoch 9/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0833 - accuracy: 0.9746 - auc: 0.9957\n",
      "104/522 [====>.........................] - ETA: 1s - loss: 0.0726 - accuracy: 0.9802 - auc: 0.9972Epoch 9/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0986 - accuracy: 0.9666 - auc: 0.9941\n",
      "Epoch 8/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0668 - accuracy: 0.9804 - auc: 0.9976\n",
      "207/522 [==========>...................] - ETA: 1s - loss: 0.0860 - accuracy: 0.9722 - auc: 0.9957Epoch 10/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0695 - accuracy: 0.9802 - auc: 0.9973\n",
      " 45/522 [=>............................] - ETA: 1s - loss: 0.0480 - accuracy: 0.9889 - auc: 0.9992Epoch 10/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0699 - accuracy: 0.9796 - auc: 0.9971\n",
      "Epoch 10/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0720 - accuracy: 0.9793 - auc: 0.9967\n",
      "Epoch 10/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0856 - accuracy: 0.9714 - auc: 0.9956\n",
      "255/522 [=============>................] - ETA: 1s - loss: 0.0572 - accuracy: 0.9864 - auc: 0.9983Epoch 9/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0572 - accuracy: 0.9851 - auc: 0.9984\n",
      "203/522 [==========>...................] - ETA: 1s - loss: 0.0696 - accuracy: 0.9786 - auc: 0.9975Epoch 11/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0598 - accuracy: 0.9844 - auc: 0.9982\n",
      "503/522 [===========================>..] - ETA: 0s - loss: 0.0602 - accuracy: 0.9844 - auc: 0.9980Epoch 11/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0602 - accuracy: 0.9842 - auc: 0.9980\n",
      " 74/522 [===>..........................] - ETA: 1s - loss: 0.0474 - accuracy: 0.9890 - auc: 0.9992Epoch 11/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0622 - accuracy: 0.9830 - auc: 0.9975\n",
      "Epoch 11/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0741 - accuracy: 0.9769 - auc: 0.9968\n",
      "Epoch 10/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0487 - accuracy: 0.9887 - auc: 0.9989\n",
      "Epoch 12/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0513 - accuracy: 0.9880 - auc: 0.9987\n",
      "Epoch 12/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0517 - accuracy: 0.9875 - auc: 0.9985\n",
      "Epoch 12/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0538 - accuracy: 0.9874 - auc: 0.9982\n",
      "Epoch 12/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0642 - accuracy: 0.9813 - auc: 0.9978\n",
      "Epoch 11/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0414 - accuracy: 0.9920 - auc: 0.9992\n",
      "Epoch 13/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0440 - accuracy: 0.9907 - auc: 0.9992\n",
      "506/522 [============================>.] - ETA: 0s - loss: 0.0447 - accuracy: 0.9897 - auc: 0.9989Epoch 13/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0444 - accuracy: 0.9897 - auc: 0.9989\n",
      "Epoch 13/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0463 - accuracy: 0.9896 - auc: 0.9986\n",
      "Epoch 13/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0556 - accuracy: 0.9849 - auc: 0.9985\n",
      "Epoch 12/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0350 - accuracy: 0.9938 - auc: 0.9995\n",
      "301/522 [================>.............] - ETA: 0s - loss: 0.0364 - accuracy: 0.9938 - auc: 0.9994Epoch 14/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0375 - accuracy: 0.9929 - auc: 0.9994\n",
      "Epoch 14/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0380 - accuracy: 0.9926 - auc: 0.9992\n",
      "  1/522 [..............................] - ETA: 31s - loss: 0.0265 - accuracy: 1.0000 - auc: 1.0000Epoch 14/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0398 - accuracy: 0.9920 - auc: 0.9989\n",
      "Epoch 14/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0478 - accuracy: 0.9881 - auc: 0.9989\n",
      "Epoch 13/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0294 - accuracy: 0.9953 - auc: 0.9997\n",
      "Epoch 15/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0318 - accuracy: 0.9944 - auc: 0.9997\n",
      "277/522 [==============>...............] - ETA: 0s - loss: 0.0409 - accuracy: 0.9909 - auc: 0.9993Epoch 15/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0325 - accuracy: 0.9940 - auc: 0.9994\n",
      "Epoch 15/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0340 - accuracy: 0.9938 - auc: 0.9992\n",
      "432/522 [=======================>......] - ETA: 0s - loss: 0.0411 - accuracy: 0.9908 - auc: 0.9993Epoch 15/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0411 - accuracy: 0.9906 - auc: 0.9993\n",
      "Epoch 14/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0246 - accuracy: 0.9968 - auc: 0.9998\n",
      "Epoch 16/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0268 - accuracy: 0.9954 - auc: 0.9998\n",
      "Epoch 16/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0275 - accuracy: 0.9958 - auc: 0.9995\n",
      "Epoch 16/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0290 - accuracy: 0.9953 - auc: 0.9994\n",
      "Epoch 16/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0351 - accuracy: 0.9934 - auc: 0.9996\n",
      "Epoch 15/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0204 - accuracy: 0.9974 - auc: 0.9999\n",
      "Epoch 17/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0225 - accuracy: 0.9966 - auc: 0.9999\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0234 - accuracy: 0.9965 - auc: 0.9997\n",
      "Epoch 17/100\n",
      "406/522 [======================>.......] - ETA: 0s - loss: 0.0286 - accuracy: 0.9955 - auc: 0.9998Epoch 17/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0246 - accuracy: 0.9965 - auc: 0.9996\n",
      "233/522 [============>.................] - ETA: 1s - loss: 0.0155 - accuracy: 0.9987 - auc: 0.9999Epoch 17/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0298 - accuracy: 0.9949 - auc: 0.9997\n",
      "Epoch 16/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0168 - accuracy: 0.9983 - auc: 0.9999\n",
      "199/522 [==========>...................] - ETA: 1s - loss: 0.0243 - accuracy: 0.9973 - auc: 0.9999Epoch 18/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0197 - accuracy: 0.9973 - auc: 0.9998\n",
      "Epoch 18/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0187 - accuracy: 0.9973 - auc: 0.9999\n",
      "396/522 [=====================>........] - ETA: 0s - loss: 0.0252 - accuracy: 0.9965 - auc: 0.9999Epoch 18/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0209 - accuracy: 0.9974 - auc: 0.9996\n",
      "Epoch 18/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0253 - accuracy: 0.9963 - auc: 0.9998\n",
      "184/522 [=========>....................] - ETA: 1s - loss: 0.0150 - accuracy: 0.9988 - auc: 0.9996Epoch 17/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0138 - accuracy: 0.9989 - auc: 1.0000\n",
      "Epoch 19/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0165 - accuracy: 0.9981 - auc: 0.9999\n",
      "Epoch 19/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0154 - accuracy: 0.9984 - auc: 1.0000\n",
      "Epoch 19/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0175 - accuracy: 0.9978 - auc: 0.9997\n",
      "Epoch 19/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0212 - accuracy: 0.9972 - auc: 0.9999\n",
      "180/522 [=========>....................] - ETA: 1s - loss: 0.0130 - accuracy: 0.9990 - auc: 0.9999Epoch 18/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0112 - accuracy: 0.9992 - auc: 1.0000\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0139 - accuracy: 0.9987 - auc: 0.9999\n",
      "Epoch 20/100\n",
      "434/522 [=======================>......] - ETA: 0s - loss: 0.0122 - accuracy: 0.9988 - auc: 1.0000Epoch 20/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0128 - accuracy: 0.9987 - auc: 1.0000\n",
      " 89/522 [====>.........................] - ETA: 1s - loss: 0.0107 - accuracy: 0.9996 - auc: 0.9999Epoch 20/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0148 - accuracy: 0.9983 - auc: 0.9998\n",
      "Epoch 20/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0177 - accuracy: 0.9980 - auc: 1.0000\n",
      "Epoch 19/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0090 - accuracy: 0.9993 - auc: 1.0000\n",
      "Epoch 21/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0116 - accuracy: 0.9991 - auc: 0.9999\n",
      "Epoch 21/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0105 - accuracy: 0.9991 - auc: 1.0000\n",
      " 98/522 [====>.........................] - ETA: 1s - loss: 0.0066 - accuracy: 0.9997 - auc: 1.0000Epoch 21/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0123 - accuracy: 0.9989 - auc: 0.9998\n",
      "Epoch 21/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0147 - accuracy: 0.9987 - auc: 1.0000\n",
      "Epoch 20/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0074 - accuracy: 0.9994 - auc: 1.0000\n",
      "Epoch 22/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0096 - accuracy: 0.9993 - auc: 1.0000\n",
      "380/522 [====================>.........] - ETA: 0s - loss: 0.0104 - accuracy: 0.9989 - auc: 0.9999Epoch 22/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0087 - accuracy: 0.9993 - auc: 1.0000.........] - ETA: 2s - loss: 0.0056 - accuracy: 1.0000 - auc: 1.00\n",
      "Epoch 22/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0103 - accuracy: 0.9990 - auc: 0.9999\n",
      "Epoch 22/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0121 - accuracy: 0.9990 - auc: 1.0000\n",
      "214/522 [===========>..................] - ETA: 1s - loss: 0.0068 - accuracy: 0.9996 - auc: 1.0000Epoch 21/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0059 - accuracy: 0.9994 - auc: 1.0000\n",
      "184/522 [=========>....................] - ETA: 1s - loss: 0.0087 - accuracy: 0.9992 - auc: 1.0000Epoch 23/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0079 - accuracy: 0.9992 - auc: 1.0000\n",
      "  1/522 [..............................] - ETA: 33s - loss: 0.0026 - accuracy: 1.0000 - auc: 1.0000Epoch 23/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0071 - accuracy: 0.9993 - auc: 1.0000\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0085 - accuracy: 0.9992 - auc: 0.9999\n",
      "Epoch 23/100\n",
      "202/522 [==========>...................] - ETA: 1s - loss: 0.0062 - accuracy: 0.9995 - auc: 1.0000Epoch 23/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0101 - accuracy: 0.9991 - auc: 1.0000\n",
      "164/522 [========>.....................] - ETA: 1s - loss: 0.0058 - accuracy: 0.9996 - auc: 1.0000Epoch 22/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0048 - accuracy: 0.9995 - auc: 1.0000\n",
      "505/522 [============================>.] - ETA: 0s - loss: 0.0064 - accuracy: 0.9994 - auc: 1.0000Epoch 24/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0065 - accuracy: 0.9994 - auc: 1.0000\n",
      "503/522 [===========================>..] - ETA: 0s - loss: 0.0069 - accuracy: 0.9994 - auc: 1.0000Epoch 24/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0071 - accuracy: 0.9993 - auc: 1.0000\n",
      "Epoch 24/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0058 - accuracy: 0.9993 - auc: 1.0000\n",
      " 77/522 [===>..........................] - ETA: 1s - loss: 0.0066 - accuracy: 0.9992 - auc: 1.0000Epoch 24/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0082 - accuracy: 0.9994 - auc: 1.0000\n",
      "211/522 [===========>..................] - ETA: 1s - loss: 0.0057 - accuracy: 0.9994 - auc: 1.0000Epoch 23/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0039 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 25/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0053 - accuracy: 0.9995 - auc: 1.0000===>........] - ETA: 0s - loss: 0.0060 - accuracy: 0.9993 - auc: 1.003 - auc: 1.00\n",
      "Epoch 25/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0058 - accuracy: 0.9993 - auc: 1.0000\n",
      "Epoch 25/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0048 - accuracy: 0.9995 - auc: 1.0000\n",
      "404/522 [======================>.......] - ETA: 0s - loss: 0.0067 - accuracy: 0.9995 - auc: 1.0000Epoch 25/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0068 - accuracy: 0.9996 - auc: 1.0000\n",
      "Epoch 24/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0032 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 26/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0044 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 26/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0048 - accuracy: 0.9995 - auc: 1.0000 0.0028 - accuracy: 1.0000 - auc: 1.000\n",
      "484/522 [==========================>...] - ETA: 0s - loss: 0.0039 - accuracy: 0.9995 - auc: 1.0000Epoch 26/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0040 - accuracy: 0.9995 - auc: 1.0000\n",
      "213/522 [===========>..................] - ETA: 1s - loss: 0.0024 - accuracy: 0.9997 - auc: 1.0000Epoch 26/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 0.0056 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 25/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 0.0026 - accuracy: 0.9995 - auc: 1.0000\n",
      "174/522 [=========>....................] - ETA: 2s - loss: 0.0044 - accuracy: 0.9996 - auc: 1.0000Epoch 27/100\n",
      "522/522 [==============================] - 3s 6ms/step - loss: 0.0036 - accuracy: 0.9995 - auc: 1.0000================>.........] - ETA: 0s - loss: 0.0031 - accuracy: 0.9996 - auc: 1.00ccuracy: 0.9996 - auc: 1.00\n",
      "Epoch 27/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 0.0033 - accuracy: 0.9995 - auc: 1.0000\n",
      "522/522 [==============================] - 3s 6ms/step - loss: 0.0039 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 27/100\n",
      "Epoch 27/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 0.0045 - accuracy: 0.9996 - auc: 1.0000==>....................] - ETA: 1s - loss: 0.0027 - accuracy: 0.9995 - auc: 1.00\n",
      "Epoch 26/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 0.0022 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 28/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 0.0030 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 28/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0032 - accuracy: 0.9995 - auc: 1.0000\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0027 - accuracy: 0.9994 - auc: 1.0000\n",
      "Epoch 28/100\n",
      "185/522 [=========>....................] - ETA: 1s - loss: 0.0017 - accuracy: 0.9995 - auc: 1.0000Epoch 28/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0038 - accuracy: 0.9996 - auc: 1.0000\n",
      "154/522 [=======>......................] - ETA: 1s - loss: 0.0033 - accuracy: 0.9992 - auc: 1.0000Epoch 27/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0018 - accuracy: 0.9995 - auc: 1.0000\n",
      "168/522 [========>.....................] - ETA: 1s - loss: 0.0033 - accuracy: 0.9996 - auc: 1.0000Epoch 29/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0024 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 29/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0023 - accuracy: 0.9995 - auc: 1.0000\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0026 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 29/100\n",
      "Epoch 29/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0031 - accuracy: 0.9996 - auc: 1.0000\n",
      "154/522 [=======>......................] - ETA: 1s - loss: 0.0017 - accuracy: 1.0000 - auc: 1.0000Epoch 28/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0015 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 30/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0020 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 30/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0020 - accuracy: 0.9998 - auc: 1.0000\n",
      "360/522 [===================>..........] - ETA: 0s - loss: 0.0024 - accuracy: 0.9998 - auc: 1.0000Epoch 30/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0021 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 30/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0026 - accuracy: 0.9997 - auc: 1.0000\n",
      "152/522 [=======>......................] - ETA: 1s - loss: 0.0012 - accuracy: 1.0000 - auc: 1.0000Epoch 29/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0012 - accuracy: 0.9997 - auc: 1.0000\n",
      "Epoch 31/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0017 - accuracy: 0.9995 - auc: 1.0000\n",
      "474/522 [==========================>...] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997 - auc: 1.0000Epoch 31/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0016 - accuracy: 0.9997 - auc: 1.0000\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0018 - accuracy: 0.9995 - auc: 1.0000\n",
      " 42/522 [=>............................] - ETA: 1s - loss: 0.0021 - accuracy: 0.9985 - auc: 1.0000Epoch 31/100\n",
      "440/522 [========================>.....] - ETA: 0s - loss: 0.0022 - accuracy: 0.9996 - auc: 1.000000Epoch 31/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0022 - accuracy: 0.9996 - auc: 1.0000\n",
      "307/522 [================>.............] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996 - auc: 1.0000Epoch 30/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0011 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 32/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0015 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 32/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0015 - accuracy: 0.9995 - auc: 1.0000\n",
      " 49/522 [=>............................] - ETA: 2s - loss: 0.0011 - accuracy: 0.9994 - auc: 1.0000Epoch 32/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0014 - accuracy: 0.9999 - auc: 1.0000\n",
      "353/522 [===================>..........] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998 - auc: 1.0000Epoch 32/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0018 - accuracy: 0.9996 - auc: 1.0000\n",
      "Epoch 31/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 8.9773e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 33/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 0.0012 - accuracy: 0.9996 - auc: 1.0000\n",
      "Epoch 33/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0012 - accuracy: 0.9995 - auc: 1.0000\n",
      "275/522 [==============>...............] - ETA: 1s - loss: 0.0014 - accuracy: 0.9998 - auc: 1.0000Epoch 33/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0013 - accuracy: 0.9998 - auc: 1.0000\n",
      " 79/522 [===>..........................] - ETA: 1s - loss: 8.5649e-04 - accuracy: 0.9996 - auc: 1.0000Epoch 33/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0016 - accuracy: 0.9996 - auc: 1.0000\n",
      "Epoch 32/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 7.8073e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 34/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 0.0011 - accuracy: 0.9996 - auc: 1.0000\n",
      "Epoch 34/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 0.0010 - accuracy: 0.9995 - auc: 1.0000\n",
      " 64/522 [==>...........................] - ETA: 1s - loss: 4.5678e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 34/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 0.0011 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 34/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 0.0013 - accuracy: 0.9996 - auc: 1.0000\n",
      "Epoch 33/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 6.8072e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "314/522 [=================>............] - ETA: 0s - loss: 9.8843e-04 - accuracy: 0.9993 - auc: 1.0000Epoch 35/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 0.0010 - accuracy: 0.9995 - auc: 1.0000\n",
      "362/522 [===================>..........] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997 - auc: 1.0000Epoch 35/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 8.7241e-04 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 35/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 9.8693e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "363/522 [===================>..........] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997 - auc: 1.0000Epoch 35/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 0.0011 - accuracy: 0.9997 - auc: 1.0000\n",
      "Epoch 34/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 5.9901e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 36/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 8.5960e-04 - accuracy: 0.9996 - auc: 1.0000\n",
      "118/522 [=====>........................] - ETA: 1s - loss: 5.0908e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 36/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 7.5523e-04 - accuracy: 0.9995 - auc: 1.0000\n",
      "Epoch 36/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 8.7424e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "376/522 [====================>.........] - ETA: 0s - loss: 8.8934e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 36/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 0.0010 - accuracy: 0.9999 - auc: 1.0000\n",
      "307/522 [================>.............] - ETA: 0s - loss: 6.8112e-04 - accuracy: 0.9996 - auc: 1.0000Epoch 35/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 5.3403e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "320/522 [=================>............] - ETA: 0s - loss: 6.1471e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 37/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 7.7878e-04 - accuracy: 0.9996 - auc: 1.0000\n",
      "Epoch 37/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 6.5726e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "459/522 [=========================>....] - ETA: 0s - loss: 8.9278e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 37/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 8.4412e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 37/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 8.9294e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 36/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.8062e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "397/522 [=====================>........] - ETA: 0s - loss: 7.8307e-04 - accuracy: 0.9995 - auc: 1.0000Epoch 38/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 7.1883e-04 - accuracy: 0.9996 - auc: 1.0000\n",
      "Epoch 38/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.8259e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "297/522 [================>.............] - ETA: 0s - loss: 5.4692e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 38/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 7.4351e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "165/522 [========>.....................] - ETA: 1s - loss: 6.7426e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 38/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 8.5092e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "213/522 [===========>..................] - ETA: 1s - loss: 4.5826e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 37/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.3984e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      " 71/522 [===>..........................] - ETA: 1s - loss: 3.4313e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 39/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 6.6299e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 39/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.2036e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 39/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 6.7625e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      " 66/522 [==>...........................] - ETA: 1s - loss: 7.1136e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 39/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 7.2719e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "206/522 [==========>...................] - ETA: 1s - loss: 5.1641e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 38/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.0573e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 40/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.9741e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "419/522 [=======================>......] - ETA: 0s - loss: 5.4837e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 40/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.6843e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "451/522 [========================>.....] - ETA: 0s - loss: 6.6705e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 40/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 6.4256e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "178/522 [=========>....................] - ETA: 1s - loss: 4.4513e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 40/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 6.5713e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 39/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.7585e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      " 67/522 [==>...........................] - ETA: 1s - loss: 2.5869e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 41/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.8323e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 41/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.2509e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 41/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.9845e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 41/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 6.6253e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 40/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.5751e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "273/522 [==============>...............] - ETA: 1s - loss: 4.7039e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 42/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 6.3704e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 42/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.8577e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 42/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.7538e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 42/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 6.3512e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 41/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.3466e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "263/522 [==============>...............] - ETA: 1s - loss: 5.7340e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 43/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.7484e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 43/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.5382e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "466/522 [=========================>....] - ETA: 0s - loss: 5.4543e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 43/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.0264e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "380/522 [====================>.........] - ETA: 0s - loss: 5.0659e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 43/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.4281e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 42/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.1979e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "381/522 [====================>.........] - ETA: 0s - loss: 4.8077e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 44/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.7299e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "390/522 [=====================>........] - ETA: 0s - loss: 2.9709e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 44/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.2513e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 44/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.3788e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 44/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.0811e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "332/522 [==================>...........] - ETA: 0s - loss: 2.1034e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 43/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.0447e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "383/522 [=====================>........] - ETA: 0s - loss: 2.4412e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 45/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.5633e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "397/522 [=====================>........] - ETA: 0s - loss: 3.0710e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 45/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.0096e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "310/522 [================>.............] - ETA: 0s - loss: 6.4584e-04 - accuracy: 0.9997 - auc: 1.0000Epoch 45/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.6695e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      " 38/522 [=>............................] - ETA: 2s - loss: 3.2812e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 45/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 5.5911e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "483/522 [==========================>...] - ETA: 0s - loss: 3.1322e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 44/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 2.9219e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "380/522 [====================>.........] - ETA: 0s - loss: 4.8829e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 46/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 4.5981e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 46/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 2.8116e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "111/522 [=====>........................] - ETA: 1s - loss: 9.3520e-04 - accuracy: 0.9997 - auc: 1.0000Epoch 46/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 4.9327e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      " 43/522 [=>............................] - ETA: 1s - loss: 2.1826e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 46/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 5.2774e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "224/522 [===========>..................] - ETA: 1s - loss: 8.7251e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 45/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 2.8386e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 47/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.7395e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "405/522 [======================>.......] - ETA: 0s - loss: 2.6114e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 47/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 2.6032e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "124/522 [======>.......................] - ETA: 1s - loss: 1.9946e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 47/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 4.6389e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 47/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 4.5038e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "496/522 [===========================>..] - ETA: 0s - loss: 2.1775e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 46/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.7616e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 48/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.6581e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 48/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.4604e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.5713e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 48/100\n",
      " 13/522 [..............................] - ETA: 2s - loss: 4.1142e-05 - accuracy: 1.0000 - auc: 1.0000 Epoch 48/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.5067e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 47/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.6810e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 49/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.0327e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 49/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.5807e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 49/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.2803e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 49/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.1533e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "500/522 [===========================>..] - ETA: 0s - loss: 2.4701e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 48/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.5949e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 50/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.5112e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 50/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.1471e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 50/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.1397e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "  1/522 [..............................] - ETA: 36s - loss: 5.9900e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 50/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.7005e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "508/522 [============================>.] - ETA: 0s - loss: 2.6109e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 49/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.5457e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 51/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.9538e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "330/522 [=================>............] - ETA: 0s - loss: 2.2505e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 51/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.2104e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 51/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.0131e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 51/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.5982e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.5262e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "171/522 [========>.....................] - ETA: 1s - loss: 5.2540e-04 - accuracy: 0.9996 - auc: 1.0000Epoch 52/100\n",
      "447/522 [========================>.....] - ETA: 0s - loss: 1.9736e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 50/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.7949e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      " 79/522 [===>..........................] - ETA: 1s - loss: 8.2363e-04 - accuracy: 0.9996 - auc: 1.0000Epoch 52/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.7562e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 52/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.9010e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 52/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.5283e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "170/522 [========>.....................] - ETA: 1s - loss: 4.9581e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 53/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.7581e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 51/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.8456e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 53/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.1545e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 53/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.7874e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 53/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.4190e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 54/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.1818e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "231/522 [============>.................] - ETA: 1s - loss: 3.2401e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 52/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.6856e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 54/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.1793e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "349/522 [===================>..........] - ETA: 0s - loss: 3.2269e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 54/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.6880e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 54/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.3655e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "360/522 [===================>..........] - ETA: 0s - loss: 2.0665e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 55/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.8403e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 53/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.6631e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "332/522 [==================>...........] - ETA: 0s - loss: 4.9021e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 55/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.9917e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 55/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.5905e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 55/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.3427e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "346/522 [==================>...........] - ETA: 0s - loss: 4.4945e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 56/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.7652e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 54/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.6162e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 56/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.1985e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "283/522 [===============>..............] - ETA: 0s - loss: 4.6684e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 56/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.4994e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 56/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.3047e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "155/522 [=======>......................] - ETA: 1s - loss: 1.0245e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 57/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.6053e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "203/522 [==========>...................] - ETA: 1s - loss: 1.2388e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 55/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.1978e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      " 96/522 [====>.........................] - ETA: 1s - loss: 4.2616e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 57/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.9158e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.4239e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 57/100\n",
      "446/522 [========================>.....] - ETA: 0s - loss: 2.6567e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 57/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.2761e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 58/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.6971e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 56/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.0884e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 58/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.3390e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "213/522 [===========>..................] - ETA: 1s - loss: 4.5600e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 58/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.0163e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 58/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.2503e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "344/522 [==================>...........] - ETA: 0s - loss: 3.2758e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 59/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.6458e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "156/522 [=======>......................] - ETA: 1s - loss: 8.9647e-04 - accuracy: 0.9996 - auc: 1.0000Epoch 57/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.1721e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 59/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.2638e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 59/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.7046e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "362/522 [===================>..........] - ETA: 0s - loss: 5.8480e-04 - accuracy: 0.9997 - auc: 1.0000Epoch 59/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.2322e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 60/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.4582e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 58/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.8932e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 60/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.1944e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "310/522 [================>.............] - ETA: 0s - loss: 2.3392e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 60/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.8023e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 60/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.2210e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "437/522 [========================>.....] - ETA: 0s - loss: 4.2104e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 61/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.5729e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "160/522 [========>.....................] - ETA: 1s - loss: 3.3724e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 59/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.2281e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 61/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.1293e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "394/522 [=====================>........] - ETA: 0s - loss: 2.2387e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 61/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.9408e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 61/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.1858e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 62/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.6049e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 60/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.9541e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 62/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.0724e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.6864e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 62/100\n",
      "278/522 [==============>...............] - ETA: 1s - loss: 3.9249e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 62/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.1762e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 63/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.3442e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 61/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.0472e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 63/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 4.5630e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "446/522 [========================>.....] - ETA: 0s - loss: 2.5155e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 63/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 1.0101e-04 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 63/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 2.1531e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 64/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 3.4955e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "434/522 [=======================>......] - ETA: 0s - loss: 3.3996e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 62/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 3.0808e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 64/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 3.7948e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 64/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 9.5504e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "283/522 [===============>..............] - ETA: 1s - loss: 1.3033e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 64/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 2.1323e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 65/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 4.4020e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 63/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 4.0713e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "260/522 [=============>................] - ETA: 1s - loss: 2.0306e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 65/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.7228e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "345/522 [==================>...........] - ETA: 0s - loss: 1.5920e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 65/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 9.0241e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "281/522 [===============>..............] - ETA: 0s - loss: 4.9305e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 65/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.1178e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "423/522 [=======================>......] - ETA: 0s - loss: 2.3997e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 66/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.4904e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "172/522 [========>.....................] - ETA: 1s - loss: 5.1272e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 64/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.9870e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 66/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 4.6000e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "499/522 [===========================>..] - ETA: 0s - loss: 7.9560e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 66/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 8.5367e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 66/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 2.1019e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "416/522 [======================>.......] - ETA: 0s - loss: 3.2110e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 67/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 3.9696e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "179/522 [=========>....................] - ETA: 1s - loss: 2.8224e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 65/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 3.7059e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 67/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 4.0180e-04 - accuracy: 0.9998 - auc: 1.0000 2.0471e-04 - accuracy: 0.9999 - auc: 1.\n",
      "449/522 [========================>.....] - ETA: 0s - loss: 2.2584e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 67/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 8.1534e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "  1/522 [..............................] - ETA: 36s - loss: 4.0611e-06 - accuracy: 1.0000 - auc: 1.0000Epoch 67/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 2.0894e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      " 39/522 [=>............................] - ETA: 1s - loss: 3.7421e-07 - accuracy: 1.0000 - auc: 1.0000Epoch 68/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 3.4872e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "446/522 [========================>.....] - ETA: 0s - loss: 2.2337e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 66/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 3.5754e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "238/522 [============>.................] - ETA: 1s - loss: 7.5553e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 68/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.3820e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "453/522 [=========================>....] - ETA: 0s - loss: 2.2290e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 68/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 7.6379e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "479/522 [==========================>...] - ETA: 0s - loss: 2.2614e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 68/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.0781e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "410/522 [======================>.......] - ETA: 0s - loss: 3.6422e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 69/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.4350e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "107/522 [=====>........................] - ETA: 1s - loss: 1.3658e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 67/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.8868e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 69/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.5598e-04 - accuracy: 0.9999 - auc: 1.0000 1.0699e-04 - accuracy: 1.0000 - auc: 1.\n",
      "Epoch 69/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 7.2452e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 69/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.0652e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 70/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.4460e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "154/522 [=======>......................] - ETA: 1s - loss: 7.8533e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 68/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.8176e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      " 68/522 [==>...........................] - ETA: 2s - loss: 4.7491e-04 - accuracy: 0.9995 - auc: 1.0000Epoch 70/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.6193e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 70/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 6.8367e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "365/522 [===================>..........] - ETA: 0s - loss: 5.2995e-04 - accuracy: 0.9997 - auc: 1.0000Epoch 70/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.0544e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 71/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.0070e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 69/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.5756e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 71/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.6182e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 71/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 6.4532e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "276/522 [==============>...............] - ETA: 1s - loss: 7.4805e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 71/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.0430e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 72/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.3146e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 70/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.8447e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "202/522 [==========>...................] - ETA: 1s - loss: 3.5393e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 72/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.5127e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 72/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 6.0871e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 72/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.0376e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 73/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.0927e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "168/522 [========>.....................] - ETA: 1s - loss: 4.0862e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 71/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.8063e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "235/522 [============>.................] - ETA: 1s - loss: 7.1743e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 73/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.5780e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "350/522 [===================>..........] - ETA: 0s - loss: 3.7671e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 73/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.7522e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.0261e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 74/100\n",
      "393/522 [=====================>........] - ETA: 0s - loss: 3.5147e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 73/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.3383e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 72/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.8370e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 74/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.2592e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "406/522 [======================>.......] - ETA: 0s - loss: 3.9309e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 74/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.0261e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 75/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.4697e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 74/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.2755e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 73/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.6094e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "126/522 [======>.......................] - ETA: 1s - loss: 4.8161e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 75/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.6238e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "273/522 [==============>...............] - ETA: 0s - loss: 4.2043e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 75/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.0085e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "404/522 [======================>.......] - ETA: 0s - loss: 4.5668e-04 - accuracy: 0.9998 - auc: 1.0000Epoch 76/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 5.1251e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 75/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.5547e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "154/522 [=======>......................] - ETA: 1s - loss: 8.0083e-04 - accuracy: 0.9996 - auc: 1.0000Epoch 74/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.7155e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      " 80/522 [===>..........................] - ETA: 1s - loss: 9.3829e-04 - accuracy: 0.9996 - auc: 1.0000Epoch 76/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.3759e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 76/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.0048e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 77/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.8384e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      " 96/522 [====>.........................] - ETA: 1s - loss: 8.1606e-04 - accuracy: 0.9997 - auc: 1.0000Epoch 76/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.5035e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 75/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.5948e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 77/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.3394e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 77/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.9941e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "300/522 [================>.............] - ETA: 0s - loss: 1.3720e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 78/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.5654e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 77/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.3057e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 76/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 2.5919e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 78/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 4.2810e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "496/522 [===========================>..] - ETA: 0s - loss: 1.9651e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 78/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 1.9889e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 79/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 4.3107e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      " 77/522 [===>..........................] - ETA: 2s - loss: 9.2346e-08 - accuracy: 1.0000 - auc: 1.0000Epoch 78/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 2.8869e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 77/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 3.7053e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 79/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 3.5541e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 79/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 1.9821e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "431/522 [=======================>......] - ETA: 0s - loss: 4.3940e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 80/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 4.0735e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 79/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 4.2739e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 78/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 3.1788e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "219/522 [===========>..................] - ETA: 1s - loss: 8.3232e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 80/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 4.3416e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "499/522 [===========================>..] - ETA: 0s - loss: 1.9456e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 80/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 1.9780e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "293/522 [===============>..............] - ETA: 1s - loss: 1.0689e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 81/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 3.8370e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 80/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 3.3847e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 79/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.1276e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 81/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 3.5715e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "271/522 [==============>...............] - ETA: 1s - loss: 3.2042e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 81/100\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 1.9710e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 82/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 3.6185e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      " 96/522 [====>.........................] - ETA: 1s - loss: 6.9726e-04 - accuracy: 0.9997 - auc: 1.0000Epoch 81/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 3.3658e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 80/100\n",
      "522/522 [==============================] - 3s 5ms/step - loss: 3.0364e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "221/522 [===========>..................] - ETA: 1s - loss: 3.5688e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 82/100\n",
      "386/522 [=====================>........] - ETA: 0s - loss: 4.1191e-05 - accuracy: 1.0000 - auc: 1.0000Restoring model weights from the end of the best epoch: 71.\n",
      "522/522 [==============================] - 2s 5ms/step - loss: 3.5267e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 81: early stopping\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.9661e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 83/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.4209e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 82/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.1740e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "150/522 [=======>......................] - ETA: 1s - loss: 7.7900e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 81/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.4697e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      " 57/522 [==>...........................] - ETA: 1s - loss: 3.7882e-06 - accuracy: 1.0000 - auc: 1.0000Epoch 83/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.9608e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 84/100\n",
      "131/131 [==============================] - 1s 3ms/steposs: 2.4248e-04 - accuracy: 1.0000 - auc: 1.00\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.2367e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 83/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.2643e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 82/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.4803e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "105/522 [=====>........................] - ETA: 1s - loss: 1.7297e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 84/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.9581e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 85/100\n",
      "131/131 [==============================] - 0s 3ms/steposs: 2.0216e-04 - accuracy: 1.0000 - auc: 1.00\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.0511e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "479/522 [==========================>...] - ETA: 0s - loss: 3.5561e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 84/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.2715e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 83/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 3.2548e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 85/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.9523e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 86/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.8778e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 85/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 4.2610e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "449/522 [========================>.....] - ETA: 0s - loss: 2.8552e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 84/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.4640e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "245/522 [=============>................] - ETA: 0s - loss: 4.7212e-04 - accuracy: 0.9997 - auc: 1.0000Epoch 86/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.9483e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "102/522 [====>.........................] - ETA: 1s - loss: 6.9060e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 87/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 2.7202e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "228/522 [============>.................] - ETA: 0s - loss: 6.2096e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 86/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 3.8385e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      " 33/522 [>.............................] - ETA: 1s - loss: 4.6166e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 85/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 3.2739e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 87/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.9439e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 88/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 2.5673e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 3.8186e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 86/100\n",
      "182/522 [=========>....................] - ETA: 0s - loss: 6.2315e-04 - accuracy: 0.9997 - auc: 1.0000Epoch 87/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 3.0608e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "412/522 [======================>.......] - ETA: 0s - loss: 2.1917e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 88/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.9476e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 89/100\n",
      "350/522 [===================>..........] - ETA: 0s - loss: 2.0332e-05 - accuracy: 1.0000 - auc: 1.0000Restoring model weights from the end of the best epoch: 76.\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 3.8001e-04 - accuracy: 0.9998 - auc: 1.0000\n",
      "Epoch 86: early stopping\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 2.4240e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 88/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 2.2798e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 89/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.9399e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "159/522 [========>.....................] - ETA: 1s - loss: 8.5757e-06 - accuracy: 1.0000 - auc: 1.0000Epoch 90/100\n",
      "131/131 [==============================] - 0s 2ms/steposs: 4.5800e-04 - accuracy: 0.9998 - auc: 1.00\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 2.2824e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 89/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 3.1149e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 90/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.9353e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "104/522 [====>.........................] - ETA: 1s - loss: 1.8397e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 91/100\n",
      "131/131 [==============================] - 0s 2ms/steposs: 7.8114e-06 - accuracy: 1.0000 - auc: 1.00\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 2.1464e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "349/522 [===================>..........] - ETA: 0s - loss: 1.0549e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 90/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 2.3699e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 91/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.9342e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "110/522 [=====>........................] - ETA: 1s - loss: 5.9505e-04 - accuracy: 0.9997 - auc: 1.0000Epoch 92/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 2.0235e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 91/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 3.1749e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 92/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.9307e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 93/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.9113e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 92/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 2.3956e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "395/522 [=====================>........] - ETA: 0s - loss: 7.8387e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 93/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.9292e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 94/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.8042e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 3.2915e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "395/522 [=====================>........] - ETA: 0s - loss: 2.2804e-04 - accuracy: 0.9999 - auc: 1.0000Epoch 94/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.9246e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 95/100\n",
      "187/522 [=========>....................] - ETA: 0s - loss: 1.0861e-04 - accuracy: 1.0000 - auc: 1.0000Epoch 93/100\n",
      "522/522 [==============================] - 1s 3ms/step - loss: 2.8890e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 95/100\n",
      "522/522 [==============================] - 1s 3ms/step - loss: 1.9245e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "112/522 [=====>........................] - ETA: 1s - loss: 5.2709e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 96/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.7004e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 94/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 2.2920e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 96/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.9197e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "310/522 [================>.............] - ETA: 0s - loss: 2.1062e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 97/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.6009e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 95/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 2.3270e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 97/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.9201e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 98/100\n",
      "522/522 [==============================] - 2s 4ms/step - loss: 1.5174e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "200/522 [==========>...................] - ETA: 1s - loss: 9.8803e-05 - accuracy: 1.0000 - auc: 1.0000Epoch 96/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 2.3457e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 98/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.9168e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 99/100\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.4253e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 97/100\n",
      "193/522 [==========>...................] - ETA: 0s - loss: 8.2569e-06 - accuracy: 1.0000 - auc: 1.0000Restoring model weights from the end of the best epoch: 88.\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 3.1303e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 98: early stopping\n",
      "522/522 [==============================] - 2s 3ms/step - loss: 1.9153e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "Epoch 100/100\n",
      "522/522 [==============================] - 1s 3ms/step - loss: 1.3414e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 98/100\n",
      "131/131 [==============================] - 0s 2ms/steposs: 2.2155e-04 - accuracy: 0.9999 - auc: 1.00\n",
      "522/522 [==============================] - 1s 3ms/step - loss: 1.9120e-04 - accuracy: 0.9999 - auc: 1.0000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 1.2639e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 99/100\n",
      "131/131 [==============================] - 0s 1ms/steposs: 1.5050e-05 - accuracy: 1.0000 - auc: 1.000\n",
      "131/131 [==============================] - 0s 2ms/steposs: 1.9947e-05 - accuracy: 1.0000 - auc: 1.00\n",
      "522/522 [==============================] - 1s 3ms/step - loss: 1.1942e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 100/100\n",
      "131/131 [==============================] - 0s 1ms/steposs: 3.2742e-08 - accuracy: 1.0000 - auc: 1.00\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 1.1252e-05 - accuracy: 1.0000 - auc: 1.0000\n",
      "131/131 [==============================] - 0s 1ms/step\n",
      "131/131 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>simple_mlp</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.9288</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Accuracy  ROC AUC  Time (s)\n",
       "simple_mlp      0.87   0.9288       216"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_model(input_shape=10000):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(input_shape,)),\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, verbose=1, \n",
    "                        callbacks=CALLBACKS,\n",
    "                        input_shape=10000)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('tf_idf',tf_idf),\n",
    "    ('feature_selection', k_best),\n",
    "    ('simple_mlp', model)\n",
    "])\n",
    "\n",
    "model_cv_scores(pipe, 'simple_mlp', X= X_train_clean, y= y_train, scoring=SCORING, cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6e85d-a063-481a-a337-6b628c9383ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692a4ed-ff4e-434e-81af-ab69a2bc715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(20000,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "\n",
    "history = model.fit(train_ds,\n",
    "                    epochs=1000,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=val_ds,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6845a22-d78a-4f54-b78d-dc360ac278c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(20000,)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "\n",
    "history = model.fit(train_ds,\n",
    "                    epochs=1000,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=val_ds,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7f1d94-7ab1-4c48-be22-58c6c7a35e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(20000,)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "\n",
    "history = model.fit(train_ds,\n",
    "                    epochs=1000,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=val_ds,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6704eb-7350-4fe6-a21e-60469de3d254",
   "metadata": {},
   "source": [
    "### Sequence Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ed3dd-f25c-4b17-a8f0-b50cc4773101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32354cc3-46a4-4092-bcd6-1ae693e7733a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f517055-ff28-43ad-a75e-da245ecb8365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cda2ba-b9d1-466b-9a5a-133e8a51661a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
