{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e200e85f-4b9e-46de-965c-55a0e4ffe3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 20:57:45.796678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/Airline_review.csv')[['Review_Title','Review','Recommended']]\n",
    "reviews = df['Review_Title'] + ' ' + df['Review']\n",
    "labels = df['Recommended'].map({'yes':1,'no':0})\n",
    "train_reviews, temp_reviews, train_labels, temp_labels = train_test_split(reviews, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "val_reviews, test_reviews, val_labels, test_labels = train_test_split(temp_reviews, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42)\n",
    "\n",
    "# Concatenating the valildation set as I don't need it here. 90-10 split\n",
    "X_train = pd.concat([train_reviews, val_reviews])\n",
    "y_train = pd.concat([train_labels, val_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff2e76d-58e2-406c-a07f-50e13f0edcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleanerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stop_words=None, lemmatize=True):\n",
    "        self.stop_words = stop_words\n",
    "        self.lemmatize = lemmatize\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        cleaned_reviews = []\n",
    "        for review in X:\n",
    "            cleaned_reviews.append(self.clean_text(review, self.stop_words, self.lemmatize))\n",
    "        return cleaned_reviews\n",
    "    \n",
    "    def clean_text(self, review, stop_words, lemmatize):\n",
    "        tokenizer = RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\")\n",
    "        tokens = tokenizer.tokenize(review)\n",
    "        if stop_words is None:\n",
    "            tokens = [word.lower() for word in tokens]\n",
    "        else:\n",
    "            tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "        if lemmatize:\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            wordnet_tags = [(word, self.get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(word, tag) for word, tag in wordnet_tags]\n",
    "            return ' '.join(lemmatized_tokens)\n",
    "        else:\n",
    "            return ' '.join(tokens)\n",
    "    \n",
    "    def get_wordnet_pos(self, treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1942daa1-85d3-4b0c-9364-e46bb5a75d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Sequence models later\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "max_features = 20000 # 28593 Unlemmatized, 23171 lemmatized\n",
    "sequence_length = 500 # more than 98% are less than this anyway\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=None, # already done by transformation.\n",
    "    split='whitespace',\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Learning the vocabulary\n",
    "vectorize_layer.adapt(X_train_clean) \n",
    "\n",
    "# Transforming to sequence vectors\n",
    "X_train_sequence_vec = vectorize_layer(X_train_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e5dbd-3928-44b8-9e16-b27e2c20d68f",
   "metadata": {},
   "source": [
    "I still question whether I shoud force everything into an sklearn pipeline or not. While ellegant, it doesn't feel as compatable with tensorflow as I want it to be, especially considering validation scores. At least during trial and error, I don't think i will convert to sklearn, maybe I will once I decide on a final model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46c4aa69-0061-45e9-8df1-cb02b1393214",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret metric identifier: loss",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\scikeras\\wrappers.py\", line 1491, in fit\n    super().fit(X=X, y=y, sample_weight=sample_weight, **kwargs)\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\scikeras\\wrappers.py\", line 760, in fit\n    self._fit(\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\scikeras\\wrappers.py\", line 928, in _fit\n    self._fit_keras_model(\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\scikeras\\wrappers.py\", line 536, in _fit_keras_model\n    raise e\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\scikeras\\wrappers.py\", line 531, in _fit_keras_model\n    key = metric_name(key)\n          ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\scikeras\\utils\\__init__.py\", line 111, in metric_name\n    fn_or_cls = keras_metric_get(metric)\n                ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\keras\\src\\metrics\\__init__.py\", line 204, in get\n    raise ValueError(f\"Could not interpret metric identifier: {identifier}\")\nValueError: Could not interpret metric identifier: loss\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 76\u001b[0m\n\u001b[0;32m     66\u001b[0m gs \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mpipe, \n\u001b[0;32m     67\u001b[0m                   param_grid\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m     68\u001b[0m                   scoring\u001b[38;5;241m=\u001b[39mSCORING, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m                   verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, \n\u001b[0;32m     73\u001b[0m                   error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Assuming X_train_clean and y_train are defined\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m gs\u001b[38;5;241m.\u001b[39mfit(X_train_clean, y_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret metric identifier: loss"
     ]
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Instantiating processing transformers\n",
    "text_cleaner = TextCleanerTransformer(stop_words=STOPWORDS, lemmatize=True)\n",
    "vectorizer = CountVectorizer(decode_error='replace', strip_accents='unicode', stop_words=None, ngram_range=(1, 2), max_df=0.95, min_df=2)\n",
    "tf_idf = TfidfTransformer()\n",
    "k_best = SelectKBest(k=20000)\n",
    "\n",
    "# Setting parameters globally\n",
    "SCORING = {'accuracy': 'accuracy', 'roc_auc': 'roc_auc'}\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "# CALLBACKS = [tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
    "#                                               patience=2, \n",
    "#                                               restore_best_weights=True,\n",
    "#                                               verbose=1,\n",
    "                                              # start_from_epoch=5)]\n",
    "\n",
    "# Clean data before entering the pipeline for efficiency\n",
    "X_train_clean = text_cleaner.transform(X_train)\n",
    "\n",
    "def build_mlp_model(input_shape, num_layers, units, initializer=None):\n",
    "    model = Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(input_shape,)))\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(layers.Dense(units, activation=\"relu\", kernel_initializer=initializer))\n",
    "        units = units // 2\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "    return model\n",
    "\n",
    "model_wrapper = KerasClassifier(\n",
    "    build_fn=build_mlp_model,\n",
    "    input_shape=20000, \n",
    "    epochs=20,\n",
    "    random_state=42,\n",
    "    num_layers=1,\n",
    "    units=64,\n",
    "    initializer=None,\n",
    "    verbose=4,\n",
    "    callbacks=None\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('tf_idf', tf_idf),\n",
    "    ('feature_selection', k_best),\n",
    "    ('mlp', model_wrapper)\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'mlp__num_layers': [1, 2, 3],  \n",
    "    'mlp__units': [8, 16, 32, 64], \n",
    "    'mlp__initializer': [None, 'he_normal']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe, \n",
    "                  param_grid=params,\n",
    "                  scoring=SCORING, \n",
    "                  n_jobs=-1, \n",
    "                  refit='accuracy',\n",
    "                  cv=2, \n",
    "                  verbose=4, \n",
    "                  error_score='raise')\n",
    "\n",
    "# Assuming X_train_clean and y_train are defined\n",
    "grid_search = gs.fit(X_train_clean, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b471293-3273-468a-bf0e-1fd3686c1283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.0\n"
     ]
    }
   ],
   "source": [
    "import scikeras\n",
    "print(scikeras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c6b4a11-58dc-43b2-b176-06f31f57372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaner = TextCleanerTransformer(stop_words=STOPWORDS, lemmatize=True)\n",
    "\n",
    "# Clean data before entering the pipeline for efficiency\n",
    "X_train_clean = text_cleaner.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5934623f-7b82-4ad3-be9f-9e53086b8f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 21:00:56.586164: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 21:00:56.586391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 21:00:56.586813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 21:00:56.588275: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 21:00:56.589945: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 21:00:56.591750: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 21:00:56.595176: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 21:00:56.656023: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6201 - accuracy: 0.6672\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6201 - accuracy: 0.6672\n",
      "Epoch 2/20\n",
      "309/326 [===========================>..] - ETA: 0s - loss: 0.6237 - accuracy: 0.6649Epoch 2/20\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6201 - accuracy: 0.6672\n",
      "  1/326 [..............................] - ETA: 12s - loss: 0.6016 - accuracy: 0.5938Epoch 2/20\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6201 - accuracy: 0.6672\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6213 - accuracy: 0.6687\n",
      "Epoch 2/20\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6213 - accuracy: 0.6687\n",
      " 20/326 [>.............................] - ETA: 0s - loss: 0.5669 - accuracy: 0.6516 Epoch 2/20\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6213 - accuracy: 0.6687\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6213 - accuracy: 0.6687\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5248 - accuracy: 0.7195\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5248 - accuracy: 0.7195\n",
      "324/326 [============================>.] - ETA: 0s - loss: 0.5250 - accuracy: 0.7193Epoch 3/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5248 - accuracy: 0.7195\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7225\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5248 - accuracy: 0.7195\n",
      "Epoch 3/20\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7225\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7225\n",
      "Epoch 3/20\n",
      "106/326 [========>.....................] - ETA: 0s - loss: 0.4785 - accuracy: 0.7904Epoch 2/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4653 - accuracy: 0.8063- loss: 0.4803 - accuracy: 0.79\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4653 - accuracy: 0.8063\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4653 - accuracy: 0.8063\n",
      "Epoch 4/20\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4653 - accuracy: 0.8063\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4668 - accuracy: 0.8064\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4668 - accuracy: 0.8064\n",
      "314/326 [===========================>..] - ETA: 0s - loss: 0.4684 - accuracy: 0.8042Epoch 4/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4668 - accuracy: 0.8064\n",
      "  1/326 [..............................] - ETA: 12s - loss: 0.4339 - accuracy: 0.8438Epoch 4/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7225\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4209 - accuracy: 0.8604247 - accuracy: 0.85=================>.......] - ETA: 0s - loss: 0.4245 - accuracy: 0.8\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4209 - accuracy: 0.8604\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4209 - accuracy: 0.8604\n",
      "Epoch 5/20\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4225 - accuracy: 0.8570\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4209 - accuracy: 0.8604\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4225 - accuracy: 0.8570\n",
      "Epoch 5/20\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4225 - accuracy: 0.8570 ETA: 0s - loss: 0.3907 - accuracy: 0.8661\n",
      " 37/326 [==>...........................] - ETA: 0s - loss: 0.3929 - accuracy: 0.8742Epoch 5/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4668 - accuracy: 0.8064\n",
      " 78/326 [======>.......................] - ETA: 0s - loss: 0.3937 - accuracy: 0.8750Epoch 4/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.3860 - accuracy: 0.88438736/326 [===========>..................] - ETA: 0s - loss: 0.4354 - accuracy: 0.84 0.3871 - accuracy: 0.88\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.3860 - accuracy: 0.8843\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.3860 - accuracy: 0.8843\n",
      "286/326 [=========================>....] - ETA: 0s - loss: 0.3904 - accuracy: 0.8793Epoch 6/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.3878 - accuracy: 0.8810\n",
      "323/326 [============================>.] - ETA: 0s - loss: 0.3881 - accuracy: 0.8808Epoch 6/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.3878 - accuracy: 0.8810\n",
      " 14/326 [>.............................] - ETA: 1s - loss: 0.3711 - accuracy: 0.8884 Epoch 6/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.3860 - accuracy: 0.8843\n",
      "  1/326 [..............................] - ETA: 13s - loss: 0.4625 - accuracy: 0.8438Epoch 6/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3878 - accuracy: 0.8810...] - ETA: 0s - loss: 0.3624 - accuracy: 0.90\n",
      " 48/326 [===>..........................] - ETA: 0s - loss: 0.3627 - accuracy: 0.9036Epoch 6/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.4225 - accuracy: 0.8570\n",
      " 96/326 [=======>......................] - ETA: 1s - loss: 0.3673 - accuracy: 0.8916Epoch 5/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3580 - accuracy: 0.8963[======================>.......] - ETA: 0s - loss: 0.3639 - accuracy: 0.89\n",
      "299/326 [==========================>...] - ETA: 0s - loss: 0.3582 - accuracy: 0.8975Epoch 7/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3580 - accuracy: 0.8963\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3580 - accuracy: 0.8963\n",
      "302/326 [==========================>...] - ETA: 0s - loss: 0.3603 - accuracy: 0.89- loss: 0.3625 - accuracy: 0.8919Epoch 7/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3580 - accuracy: 0.8963\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3600 - accuracy: 0.8933\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3600 - accuracy: 0.8933\n",
      "  8/326 [..............................] - ETA: 2s - loss: 0.3496 - accuracy: 0.8867 Epoch 7/20\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3600 - accuracy: 0.8933\n",
      " 52/326 [===>..........................] - ETA: 1s - loss: 0.3488 - accuracy: 0.8894Epoch 7/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3878 - accuracy: 0.8810\n",
      " 91/326 [=======>......................] - ETA: 0s - loss: 0.3443 - accuracy: 0.9004Epoch 6/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3350 - accuracy: 0.9036\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3350 - accuracy: 0.9036\n",
      "Epoch 8/20\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3350 - accuracy: 0.9036\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3372 - accuracy: 0.9020\n",
      "  1/326 [..............................] - ETA: 12s - loss: 0.2410 - accuracy: 0.9688Epoch 8/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3372 - accuracy: 0.9020\n",
      "300/326 [==========================>...] - ETA: 0s - loss: 0.3379 - accuracy: 0.9025Epoch 8/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3350 - accuracy: 0.9036\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3372 - accuracy: 0.9020\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3600 - accuracy: 0.8933\n",
      " 76/326 [=====>........................] - ETA: 1s - loss: 0.3267 - accuracy: 0.9025Epoch 7/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3158 - accuracy: 0.9095\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3158 - accuracy: 0.9095\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3180 - accuracy: 0.9070\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3158 - accuracy: 0.9095\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3158 - accuracy: 0.9095\n",
      "298/326 [==========================>...] - ETA: 0s - loss: 0.3183 - accuracy: 0.9073Epoch 9/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3180 - accuracy: 0.9070\n",
      "255/326 [======================>.......] - ETA: 0s - loss: 0.3399 - accuracy: 0.9010Epoch 9/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3180 - accuracy: 0.9070\n",
      " 16/326 [>.............................] - ETA: 1s - loss: 0.3044 - accuracy: 0.9082 Epoch 9/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3372 - accuracy: 0.9020\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2993 - accuracy: 0.9129\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2993 - accuracy: 0.9129\n",
      "Epoch 10/20\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.3017 - accuracy: 0.9103\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2993 - accuracy: 0.9129\n",
      "303/326 [==========================>...] - ETA: 0s - loss: 0.3010 - accuracy: 0.9114Epoch 10/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2993 - accuracy: 0.9129\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3017 - accuracy: 0.9103\n",
      " 29/326 [=>............................] - ETA: 1s - loss: 0.2861 - accuracy: 0.9106Epoch 10/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3017 - accuracy: 0.9103\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3180 - accuracy: 0.9070......] - ETA: 1s - loss: 0.2949 - accuracy: 0.912==>.......................] - ETA: 1s - loss: 0.2843 - accuracy: 0.91\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2851 - accuracy: 0.9161\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2851 - accuracy: 0.9161\n",
      "195/326 [================>.............] - ETA: 0s - loss: 0.3047 - accuracy: 0.9069Epoch 11/20\n",
      "301/326 [==========================>...] - ETA: 0s - loss: 0.2857 - accuracy: 0.9159Epoch 11/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2876 - accuracy: 0.9153\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2851 - accuracy: 0.9161\n",
      "212/326 [==================>...........] - ETA: 0s - loss: 0.3050 - accuracy: 0.9071Epoch 11/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2851 - accuracy: 0.9161\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2876 - accuracy: 0.9153\n",
      "Epoch 11/20\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2876 - accuracy: 0.9153\n",
      " 35/326 [==>...........................] - ETA: 0s - loss: 0.2906 - accuracy: 0.9152Epoch 11/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.3017 - accuracy: 0.9103\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2726 - accuracy: 0.9191\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2752 - accuracy: 0.9181\n",
      "274/326 [========================>.....] - ETA: 0s - loss: 0.2772 - accuracy: 0.9177Epoch 12/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2726 - accuracy: 0.9191\n",
      "313/326 [===========================>..] - ETA: 0s - loss: 0.2729 - accuracy: 0.9190Epoch 12/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2726 - accuracy: 0.9191\n",
      "  1/326 [..............................] - ETA: 12s - loss: 0.2566 - accuracy: 0.9688Epoch 12/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2726 - accuracy: 0.9191\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2752 - accuracy: 0.9181\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2752 - accuracy: 0.9181\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2876 - accuracy: 0.9153\n",
      "126/326 [==========>...................] - ETA: 0s - loss: 0.2652 - accuracy: 0.9209Epoch 11/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2642 - accuracy: 0.9203\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2615 - accuracy: 0.9230\n",
      "292/326 [=========================>....] - ETA: 0s - loss: 0.2648 - accuracy: 0.9196Epoch 13/20\n",
      "303/326 [==========================>...] - ETA: 0s - loss: 0.2630 - accuracy: 0.9223Epoch 13/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2615 - accuracy: 0.9230\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2615 - accuracy: 0.9230\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2615 - accuracy: 0.9230\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2642 - accuracy: 0.9203\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2642 - accuracy: 0.9203\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2752 - accuracy: 0.9181\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2516 - accuracy: 0.9258\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2516 - accuracy: 0.9258\n",
      "297/326 [==========================>...] - ETA: 0s - loss: 0.2508 - accuracy: 0.9275Epoch 14/20\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2542 - accuracy: 0.9229\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2516 - accuracy: 0.9258\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2516 - accuracy: 0.9258\n",
      " 17/326 [>.............................] - ETA: 1s - loss: 0.2642 - accuracy: 0.9081 Epoch 14/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2542 - accuracy: 0.9229\n",
      " 34/326 [==>...........................] - ETA: 0s - loss: 0.2559 - accuracy: 0.9173Epoch 14/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2542 - accuracy: 0.9229\n",
      " 17/326 [>.............................] - ETA: 0s - loss: 0.2680 - accuracy: 0.9210 Epoch 14/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2642 - accuracy: 0.9203\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2452 - accuracy: 0.9260 0.2472 - accuracy: 0.92\n",
      "276/326 [========================>.....] - ETA: 0s - loss: 0.2477 - accuracy: 0.9244Epoch 15/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2425 - accuracy: 0.9291\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2425 - accuracy: 0.9291\n",
      "Epoch 15/20\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2425 - accuracy: 0.9291\n",
      " 17/326 [>.............................] - ETA: 1s - loss: 0.2557 - accuracy: 0.9265 Epoch 15/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2425 - accuracy: 0.9291\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2452 - accuracy: 0.9260\n",
      " 46/326 [===>..........................] - ETA: 0s - loss: 0.2445 - accuracy: 0.9280Epoch 15/20\n",
      "  1/326 [..............................] - ETA: 13s - loss: 0.2213 - accuracy: 0.9688Epoch 15/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2452 - accuracy: 0.9260\n",
      " 87/326 [=======>......................] - ETA: 0s - loss: 0.2380 - accuracy: 0.9307Epoch 15/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2542 - accuracy: 0.9229\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2370 - accuracy: 0.9287\n",
      "275/326 [========================>.....] - ETA: 0s - loss: 0.2353 - accuracy: 0.9319Epoch 16/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2342 - accuracy: 0.9322\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2342 - accuracy: 0.9322\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2342 - accuracy: 0.9322\n",
      "221/326 [===================>..........] - ETA: 0s - loss: 0.2480 - accuracy: 0.9256Epoch 16/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2370 - accuracy: 0.9287\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2342 - accuracy: 0.9322\n",
      "Epoch 16/20\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2370 - accuracy: 0.9287\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2452 - accuracy: 0.9260\n",
      "141/326 [===========>..................] - ETA: 0s - loss: 0.2308 - accuracy: 0.9300Epoch 15/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2294 - accuracy: 0.9308\n",
      "325/326 [============================>.] - ETA: 0s - loss: 0.2267 - accuracy: 0.9337Epoch 17/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2266 - accuracy: 0.9336\n",
      "242/326 [=====================>........] - ETA: 0s - loss: 0.2299 - accuracy: 0.9303Epoch 17/20\n",
      "314/326 [===========================>..] - ETA: 0s - loss: 0.2275 - accuracy: 0.9328Epoch 16/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2266 - accuracy: 0.9336\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2294 - accuracy: 0.9308\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2266 - accuracy: 0.9336\n",
      "280/326 [========================>.....] - ETA: 0s - loss: 0.2290 - accuracy: 0.9305Epoch 17/20\n",
      "Epoch 17/20\n",
      " 58/326 [====>.........................] - ETA: 0s - loss: 0.2203 - accuracy: 0.9386Epoch 17/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2294 - accuracy: 0.9308\n",
      " 78/326 [======>.......................] - ETA: 1s - loss: 0.2190 - accuracy: 0.9391Epoch 17/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2370 - accuracy: 0.9287\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2266 - accuracy: 0.9336\n",
      "161/326 [=============>................] - ETA: 0s - loss: 0.2285 - accuracy: 0.9305Epoch 17/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2223 - accuracy: 0.9326\n",
      "323/326 [============================>.] - ETA: 0s - loss: 0.2198 - accuracy: 0.9357Epoch 18/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2195 - accuracy: 0.9357\n",
      "238/326 [====================>.........] - ETA: 0s - loss: 0.2252 - accuracy: 0.9316Epoch 18/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2195 - accuracy: 0.9357\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2195 - accuracy: 0.9357\n",
      " 60/326 [====>.........................] - ETA: 0s - loss: 0.2180 - accuracy: 0.9406Epoch 18/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2223 - accuracy: 0.9326\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2223 - accuracy: 0.9326\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2294 - accuracy: 0.9308\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2195 - accuracy: 0.9357\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2158 - accuracy: 0.9351\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2129 - accuracy: 0.9377\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2129 - accuracy: 0.9377\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2129 - accuracy: 0.9377\n",
      " 34/326 [==>...........................] - ETA: 0s - loss: 0.2172 - accuracy: 0.9311Epoch 19/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2158 - accuracy: 0.9351\n",
      "Epoch 19/20\n",
      "  1/326 [..............................] - ETA: 12s - loss: 0.1185 - accuracy: 1.0000Epoch 18/20\n",
      " 93/326 [=======>......................] - ETA: 0s - loss: 0.2144 - accuracy: 0.9318Epoch 17/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2129 - accuracy: 0.9377\n",
      "187/326 [================>.............] - ETA: 0s - loss: 0.2216 - accuracy: 0.9350Epoch 19/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2068 - accuracy: 0.9390\n",
      "268/326 [=======================>......] - ETA: 0s - loss: 0.2085 - accuracy: 0.9374Epoch 20/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2096 - accuracy: 0.9368\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2158 - accuracy: 0.9351\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2068 - accuracy: 0.9390\n",
      " 35/326 [==>...........................] - ETA: 0s - loss: 0.2022 - accuracy: 0.9375Epoch 20/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2068 - accuracy: 0.9390\n",
      " 51/326 [===>..........................] - ETA: 0s - loss: 0.2077 - accuracy: 0.9344Epoch 19/20\n",
      "326/326 [==============================] - 1s 4ms/step - loss: 0.2096 - accuracy: 0.9368\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2223 - accuracy: 0.9326\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2068 - accuracy: 0.9390\n",
      "303/326 [==========================>...] - ETA: 0s - loss: 0.2044 - accuracy: 0.9382Epoch 20/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2010 - accuracy: 0.9414\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2038 - accuracy: 0.9382\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2010 - accuracy: 0.9414\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2096 - accuracy: 0.9368\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2038 - accuracy: 0.9382\n",
      "322/326 [============================>.] - ETA: 0s - loss: 0.2158 - accuracy: 0.9352Epoch 20/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2158 - accuracy: 0.9351\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2010 - accuracy: 0.9414\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2038 - accuracy: 0.9382\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2010 - accuracy: 0.9414\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2096 - accuracy: 0.9368\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.2038 - accuracy: 0.9382\n",
      "326/326 [==============================] - 1s 2ms/step\n",
      "326/326 [==============================] - 1s 2ms/step\n",
      "326/326 [==============================] - 1s 2ms/step\n",
      "326/326 [==============================] - 1s 2ms/step\n",
      "326/326 [==============================] - 1s 2ms/step\n",
      "326/326 [==============================] - 1s 2ms/step\n",
      "326/326 [==============================] - 1s 2ms/step\n",
      "326/326 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END mlp__initializer=None, mlp__num_layers=1, mlp__units=64;, score=0.897 total time=  33.9s\n",
      "Epoch 1/20\n",
      "[CV 2/2] END mlp__initializer=None, mlp__num_layers=1, mlp__units=64;, score=0.899 total time=  34.0s\n",
      "Epoch 1/20\n",
      "[CV 2/2] END mlp__initializer=None, mlp__num_layers=1, mlp__units=32;, score=0.899 total time=  34.0s\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END mlp__initializer=None, mlp__num_layers=1, mlp__units=32;, score=0.897 total time=  34.2s\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END mlp__initializer=None, mlp__num_layers=1, mlp__units=8;, score=0.899 total time=  34.8s\n",
      "Epoch 1/20\n",
      "[CV 1/2] END mlp__initializer=None, mlp__num_layers=1, mlp__units=16;, score=0.897 total time=  34.8s\n",
      "Epoch 1/20\n",
      "[CV 1/2] END mlp__initializer=None, mlp__num_layers=1, mlp__units=8;, score=0.897 total time=  34.9s\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 3s 6ms/step - loss: 0.5372 - accuracy: 0.8465\n",
      "281/326 [========================>.....] - ETA: 0s - loss: 0.4845 - accuracy: 0.8124Epoch 2/20\n",
      "326/326 [==============================] - 3s 6ms/step - loss: 0.5365 - accuracy: 0.8488\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 3s 7ms/step - loss: 0.4593 - accuracy: 0.8251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/326 [..............................] - ETA: 14:40 - loss: 0.6936 - accuracy: 0.3438Epoch 2/20\n",
      "326/326 [==============================] - 4s 7ms/step - loss: 0.4589 - accuracy: 0.8236\n",
      " 54/326 [===>..........................] - ETA: 2s - loss: 0.2482 - accuracy: 0.9271Epoch 2/20\n",
      "132/326 [===========>..................] - ETA: 2s - loss: 0.2271 - accuracy: 0.9342[CV 2/2] END mlp__initializer=None, mlp__num_layers=1, mlp__units=16;, score=0.899 total time=  36.3s\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2814 - accuracy: 0.9215\n",
      "317/326 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9189Epoch 3/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2838 - accuracy: 0.9195\n",
      "283/326 [=========================>....] - ETA: 0s - loss: 0.2233 - accuracy: 0.9279Epoch 3/20\n",
      "326/326 [==============================] - 7s 13ms/step - loss: 0.4164 - accuracy: 0.8492: 0.1825 - accuracy: 0.95=====================>.......] - ETA: 0s - loss: 0.2348 - accuracy: 0.92\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2218 - accuracy: 0.9277\n",
      "326/326 [==============================] - 7s 13ms/step - loss: 0.4167 - accuracy: 0.8497\n",
      " 83/326 [======>.......................] - ETA: 2s - loss: 0.2119 - accuracy: 0.9352Epoch 2/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2249 - accuracy: 0.9242=====>..] - ETA: 0s - loss: 0.2272 - accuracy: 0.9232\n",
      "121/326 [==========>...................] - ETA: 1s - loss: 0.2048 - accuracy: 0.9398Epoch 3/20\n",
      "187/326 [================>.............] - ETA: 3s - loss: 0.4459 - accuracy: 0.8240Epoch 2/20\n",
      " 75/326 [=====>........................] - ETA: 2s - loss: 0.2136 - accuracy: 0.9296Epoch 3/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1896 - accuracy: 0.9428\n",
      "141/326 [===========>..................] - ETA: 2s - loss: 0.1605 - accuracy: 0.9488Epoch 4/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1927 - accuracy: 0.9394\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 11s 23ms/step - loss: 0.3599 - accuracy: 0.8621\n",
      "296/326 [==========================>...] - ETA: 0s - loss: 0.1893 - accuracy: 0.9349Epoch 2/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1547 - accuracy: 0.9505\n",
      "269/326 [=======================>......] - ETA: 0s - loss: 0.1527 - accuracy: 0.9512Epoch 4/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1860 - accuracy: 0.9363\n",
      " 16/326 [>.............................] - ETA: 6s - loss: 0.1580 - accuracy: 0.9609Epoch 3/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1824 - accuracy: 0.9387\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1512 - accuracy: 0.9512\n",
      "Epoch 3/20\n",
      " 53/326 [===>..........................] - ETA: 3s - loss: 0.1216 - accuracy: 0.9617Epoch 4/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1432 - accuracy: 0.95570.1108 - accuracy: 0.96\n",
      "141/326 [===========>..................] - ETA: 2s - loss: 0.1215 - accuracy: 0.9610Epoch 5/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1458 - accuracy: 0.9561\n",
      "  4/326 [..............................] - ETA: 6s - loss: 0.1119 - accuracy: 0.9844 Epoch 5/20\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1123 - accuracy: 0.9671ccuracy: 0.96\n",
      "266/326 [=======================>......] - ETA: 1s - loss: 0.1157 - accuracy: 0.9638Epoch 5/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1185 - accuracy: 0.9637\n",
      "164/326 [==============>...............] - ETA: 2s - loss: 0.1181 - accuracy: 0.9644Epoch 4/20\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1095 - accuracy: 0.9669\n",
      "221/326 [===================>..........] - ETA: 2s - loss: 0.1468 - accuracy: 0.9516Epoch 5/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1150 - accuracy: 0.9644\n",
      "187/326 [================>.............] - ETA: 2s - loss: 0.1107 - accuracy: 0.9674Epoch 4/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1115 - accuracy: 0.9674\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1139 - accuracy: 0.9676\n",
      "151/326 [============>.................] - ETA: 3s - loss: 0.0786 - accuracy: 0.9801Epoch 6/20\n",
      "326/326 [==============================] - 9s 29ms/step - loss: 0.1475 - accuracy: 0.9502\n",
      "150/326 [============>.................] - ETA: 3s - loss: 0.0756 - accuracy: 0.9829Epoch 3/20\n",
      "326/326 [==============================] - 18s 29ms/step - loss: 0.3613 - accuracy: 0.8613\n",
      "247/326 [=====================>........] - ETA: 1s - loss: 0.0738 - accuracy: 0.9813Epoch 2/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0829 - accuracy: 0.9780\n",
      "120/326 [==========>...................] - ETA: 3s - loss: 0.0943 - accuracy: 0.9758Epoch 6/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.0774 - accuracy: 0.9805\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.0798 - accuracy: 0.9788\n",
      "Epoch 5/20\n",
      " 93/326 [=======>......................] - ETA: 6s - loss: 0.0901 - accuracy: 0.9755Epoch 6/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.0747 - accuracy: 0.9803\n",
      "202/326 [=================>............] - ETA: 2s - loss: 0.0894 - accuracy: 0.9779Epoch 5/20\n",
      " 69/326 [=====>........................] - ETA: 4s - loss: 0.0530 - accuracy: 0.9891Epoch 6/20.] - ETA: 7s - loss: 0.1732 - accuracy: 0.94\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0873 - accuracy: 0.9779y: 0.98\n",
      "144/326 [============>.................] - ETA: 3s - loss: 0.0526 - accuracy: 0.9907Epoch 7/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.0609 - accuracy: 0.9867: 0.1074 - accuracy: 1.0 - accuracy: 0.97- loss: 0.0629 - accuracy: 0.\n",
      "278/326 [========================>.....] - ETA: 0s - loss: 0.0585 - accuracy: 0.9880Epoch 7/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.0578 - accuracy: 0.9881\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.0511 - accuracy: 0.9901\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.0480 - accuracy: 0.9907s: 0.1578 - accuracy: 0.94=====>............] - ETA: 2s - loss: 0.0695 - accuracy: 0.\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.0896 - accuracy: 0.9764\n",
      "275/326 [========================>.....] - ETA: 1s - loss: 0.1554 - accuracy: 0.9463Epoch 7/20\n",
      "326/326 [==============================] - 11s 32ms/step - loss: 0.0822 - accuracy: 0.9773\n",
      " 64/326 [====>.........................] - ETA: 6s - loss: 0.0436 - accuracy: 0.9927Epoch 4/20\n",
      "326/326 [==============================] - 10s 31ms/step - loss: 0.1511 - accuracy: 0.9482\n",
      " 43/326 [==>...........................] - ETA: 7s - loss: 0.0428 - accuracy: 0.9949Epoch 3/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.0684 - accuracy: 0.9852\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.0449 - accuracy: 0.9922\n",
      "278/326 [========================>.....] - ETA: 0s - loss: 0.0351 - accuracy: 0.9940Epoch 8/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.0418 - accuracy: 0.9934\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.0338 - accuracy: 0.9947\n",
      "128/326 [==========>...................] - ETA: 5s - loss: 0.0914 - accuracy: 0.9729Epoch 7/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.0306 - accuracy: 0.9957\n",
      "186/326 [================>.............] - ETA: 4s - loss: 0.0456 - accuracy: 0.9924Epoch 7/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.0708 - accuracy: 0.9834\n",
      "  1/326 [..............................] - ETA: 18s - loss: 0.0334 - accuracy: 1.0000Epoch 8/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.0535 - accuracy: 0.9901\n",
      " 94/326 [=======>......................] - ETA: 3s - loss: 0.0608 - accuracy: 0.9867Epoch 9/20\n",
      "265/326 [=======================>......] - ETA: 1s - loss: 0.0878 - accuracy: 0.9750Epoch 8/20\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.0454 - accuracy: 0.9910\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0331 - accuracy: 0.9947\n",
      " 64/326 [====>.........................] - ETA: 4s - loss: 0.0312 - accuracy: 0.9956Epoch 9/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0861 - accuracy: 0.9755\n",
      " 29/326 [=>............................] - ETA: 8s - loss: 0.0242 - accuracy: 0.9989Epoch 4/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0228 - accuracy: 0.9974\n",
      " 57/326 [====>.........................] - ETA: 4s - loss: 0.0259 - accuracy: 0.9984Epoch 8/20\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0560 - accuracy: 0.9895\n",
      "148/326 [============>.................] - ETA: 2s - loss: 0.0318 - accuracy: 0.9954Epoch 9/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0200 - accuracy: 0.9987\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0420 - accuracy: 0.9934\n",
      " 77/326 [======>.......................] - ETA: 4s - loss: 0.0148 - accuracy: 0.9992Epoch 10/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0301 - accuracy: 0.9955\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0247 - accuracy: 0.9970\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0158 - accuracy: 0.9987\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0445 - accuracy: 0.9929\n",
      "226/326 [===================>..........] - ETA: 1s - loss: 0.0335 - accuracy: 0.9949Epoch 9/20\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0134 - accuracy: 0.9993\n",
      "246/326 [=====================>........] - ETA: 2s - loss: 0.0483 - accuracy: 0.9909Epoch 9/20\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.0328 - accuracy: 0.9954\n",
      "314/326 [===========================>..] - ETA: 0s - loss: 0.0248 - accuracy: 0.9971Epoch 11/20\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.0249 - accuracy: 0.9969\n",
      "Epoch 6/20\n",
      " 44/326 [===>..........................] - ETA: 4s - loss: 0.0332 - accuracy: 0.9964Epoch 9/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0486 - accuracy: 0.9898\n",
      "157/326 [=============>................] - ETA: 2s - loss: 0.0120 - accuracy: 0.9992Epoch 5/20\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0185 - accuracy: 0.9986\n",
      "158/326 [=============>................] - ETA: 2s - loss: 0.0267 - accuracy: 0.9974Epoch 11/20\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0355 - accuracy: 0.9949\n",
      "120/326 [==========>...................] - ETA: 5s - loss: 0.0160 - accuracy: 0.9992Epoch 11/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0113 - accuracy: 0.9992\n",
      " 77/326 [======>.......................] - ETA: 4s - loss: 0.0155 - accuracy: 0.9988Epoch 10/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0094 - accuracy: 0.9997\n",
      "226/326 [===================>..........] - ETA: 1s - loss: 0.0225 - accuracy: 0.9988Epoch 10/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0257 - accuracy: 0.9977\n",
      "180/326 [===============>..............] - ETA: 4s - loss: 0.0151 - accuracy: 0.9993Epoch 12/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0219 - accuracy: 0.9986\n",
      " 43/326 [==>...........................] - ETA: 4s - loss: 0.0170 - accuracy: 1.0000Epoch 10/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0141 - accuracy: 0.9989\n",
      "116/326 [=========>....................] - ETA: 3s - loss: 0.0169 - accuracy: 0.9987Epoch 12/20\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.0284 - accuracy: 0.9964\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 9s 29ms/step - loss: 0.0141 - accuracy: 0.9993\n",
      " 21/326 [>.............................] - ETA: 4s - loss: 0.0232 - accuracy: 1.0000Epoch 7/20\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.0280 - accuracy: 0.9958\n",
      " 39/326 [==>...........................] - ETA: 4s - loss: 0.0209 - accuracy: 1.0000Epoch 6/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.0082 - accuracy: 0.9994\n",
      "261/326 [=======================>......] - ETA: 1s - loss: 0.0194 - accuracy: 0.9993Epoch 11/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.0067 - accuracy: 0.9997\n",
      " 16/326 [>.............................] - ETA: 7s - loss: 0.0174 - accuracy: 0.9980Epoch 11/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0203 - accuracy: 0.9989\n",
      "107/326 [========>.....................] - ETA: 3s - loss: 0.0229 - accuracy: 0.9982Epoch 13/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.0161 - accuracy: 0.9990\n",
      " 47/326 [===>..........................] - ETA: 6s - loss: 0.0151 - accuracy: 1.0000Epoch 11/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.0107 - accuracy: 0.9993\n",
      "204/326 [=================>............] - ETA: 2s - loss: 0.0051 - accuracy: 0.9998Epoch 13/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.0228 - accuracy: 0.9980\n",
      " 41/326 [==>...........................] - ETA: 5s - loss: 0.0095 - accuracy: 0.9992Epoch 13/20\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 0.0062 - accuracy: 0.9996\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 0.0049 - accuracy: 0.9998\n",
      "Epoch 12/20\n",
      "246/326 [=====================>........] - ETA: 2s - loss: 0.0173 - accuracy: 0.9986"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 59\u001b[0m\n\u001b[1;32m     50\u001b[0m gs \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mpipe, \n\u001b[1;32m     51\u001b[0m                   param_grid\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m     52\u001b[0m                   scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m                   verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, \n\u001b[1;32m     56\u001b[0m                   error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# The fitting process would be initiated with actual data\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "def build_mlp_model(input_shape, num_layers, units, initializer=None):\n",
    "    model = Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(input_shape,)))\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(layers.Dense(units, activation=\"relu\", kernel_initializer=initializer))\n",
    "        units = units // 2  # Reduce the units by half for each subsequent layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CALLBACKS list for model training\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, restore_best_weights=True, verbose=1)]\n",
    "\n",
    "model_wrapper = KerasClassifier(\n",
    "    build_fn=build_mlp_model,\n",
    "    input_shape=20000, \n",
    "    num_layers=1,\n",
    "    units=64,\n",
    "    initializer=None,\n",
    "    epochs=20,\n",
    "    random_state=42,\n",
    "    callbacks=callbacks)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(decode_error='replace', strip_accents='unicode', stop_words=None, ngram_range=(1, 2), max_df=0.95, min_df=2)),\n",
    "    ('tf_idf', TfidfTransformer()),\n",
    "    ('feature_selection', SelectKBest(k=20000)),\n",
    "    ('mlp', model_wrapper)\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'mlp__num_layers': [1, 2, 3],  \n",
    "    'mlp__units': [8, 16, 32, 64], \n",
    "    'mlp__initializer': [None, 'he_normal']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe, \n",
    "                  param_grid=params,\n",
    "                  scoring='accuracy',\n",
    "                  n_jobs=-1,\n",
    "                  cv=StratifiedKFold(n_splits=2, shuffle=True, random_state=42),\n",
    "                  verbose=4, \n",
    "                  error_score='raise')\n",
    "\n",
    "# The fitting process would be initiated with actual data\n",
    "grid_search = gs.fit(X_train_clean, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075a196-d4c0-441c-a83a-29c5c3ef35fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
