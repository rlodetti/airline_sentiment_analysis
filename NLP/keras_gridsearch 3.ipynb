{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e200e85f-4b9e-46de-965c-55a0e4ffe3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 20:57:45.796678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/Airline_review.csv')[['Review_Title','Review','Recommended']]\n",
    "reviews = df['Review_Title'] + ' ' + df['Review']\n",
    "labels = df['Recommended'].map({'yes':1,'no':0})\n",
    "train_reviews, temp_reviews, train_labels, temp_labels = train_test_split(reviews, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "val_reviews, test_reviews, val_labels, test_labels = train_test_split(temp_reviews, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42)\n",
    "\n",
    "# Concatenating the valildation set as I don't need it here. 90-10 split\n",
    "X_train = pd.concat([train_reviews, val_reviews])\n",
    "y_train = pd.concat([train_labels, val_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff2e76d-58e2-406c-a07f-50e13f0edcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleanerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stop_words=None, lemmatize=True):\n",
    "        self.stop_words = stop_words\n",
    "        self.lemmatize = lemmatize\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        cleaned_reviews = []\n",
    "        for review in X:\n",
    "            cleaned_reviews.append(self.clean_text(review, self.stop_words, self.lemmatize))\n",
    "        return cleaned_reviews\n",
    "    \n",
    "    def clean_text(self, review, stop_words, lemmatize):\n",
    "        tokenizer = RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\")\n",
    "        tokens = tokenizer.tokenize(review)\n",
    "        if stop_words is None:\n",
    "            tokens = [word.lower() for word in tokens]\n",
    "        else:\n",
    "            tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "        if lemmatize:\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            wordnet_tags = [(word, self.get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(word, tag) for word, tag in wordnet_tags]\n",
    "            return ' '.join(lemmatized_tokens)\n",
    "        else:\n",
    "            return ' '.join(tokens)\n",
    "    \n",
    "    def get_wordnet_pos(self, treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1942daa1-85d3-4b0c-9364-e46bb5a75d45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#For Sequence models later\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "max_features = 20000 # 28593 Unlemmatized, 23171 lemmatized\n",
    "sequence_length = 500 # more than 98% are less than this anyway\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=None, # already done by transformation.\n",
    "    split='whitespace',\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Learning the vocabulary\n",
    "vectorize_layer.adapt(X_train_clean) \n",
    "\n",
    "# Transforming to sequence vectors\n",
    "X_train_sequence_vec = vectorize_layer(X_train_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e5dbd-3928-44b8-9e16-b27e2c20d68f",
   "metadata": {},
   "source": [
    "I still question whether I shoud force everything into an sklearn pipeline or not. While ellegant, it doesn't feel as compatable with tensorflow as I want it to be, especially considering validation scores. At least during trial and error, I don't think i will convert to sklearn, maybe I will once I decide on a final model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46c4aa69-0061-45e9-8df1-cb02b1393214",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret metric identifier: loss",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\scikeras\\wrappers.py\", line 1491, in fit\n    super().fit(X=X, y=y, sample_weight=sample_weight, **kwargs)\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\scikeras\\wrappers.py\", line 760, in fit\n    self._fit(\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\scikeras\\wrappers.py\", line 928, in _fit\n    self._fit_keras_model(\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\scikeras\\wrappers.py\", line 536, in _fit_keras_model\n    raise e\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\scikeras\\wrappers.py\", line 531, in _fit_keras_model\n    key = metric_name(key)\n          ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\scikeras\\utils\\__init__.py\", line 111, in metric_name\n    fn_or_cls = keras_metric_get(metric)\n                ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\keras\\src\\metrics\\__init__.py\", line 204, in get\n    raise ValueError(f\"Could not interpret metric identifier: {identifier}\")\nValueError: Could not interpret metric identifier: loss\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 76\u001b[0m\n\u001b[0;32m     66\u001b[0m gs \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mpipe, \n\u001b[0;32m     67\u001b[0m                   param_grid\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m     68\u001b[0m                   scoring\u001b[38;5;241m=\u001b[39mSCORING, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m                   verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, \n\u001b[0;32m     73\u001b[0m                   error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Assuming X_train_clean and y_train are defined\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m gs\u001b[38;5;241m.\u001b[39mfit(X_train_clean, y_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone2-env\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret metric identifier: loss"
     ]
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Instantiating processing transformers\n",
    "text_cleaner = TextCleanerTransformer(stop_words=STOPWORDS, lemmatize=True)\n",
    "vectorizer = CountVectorizer(decode_error='replace', strip_accents='unicode', stop_words=None, ngram_range=(1, 2), max_df=0.95, min_df=2)\n",
    "tf_idf = TfidfTransformer()\n",
    "k_best = SelectKBest(k=20000)\n",
    "\n",
    "# Setting parameters globally\n",
    "SCORING = {'accuracy': 'accuracy', 'roc_auc': 'roc_auc'}\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "# CALLBACKS = [tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
    "#                                               patience=2, \n",
    "#                                               restore_best_weights=True,\n",
    "#                                               verbose=1,\n",
    "                                              # start_from_epoch=5)]\n",
    "\n",
    "# Clean data before entering the pipeline for efficiency\n",
    "X_train_clean = text_cleaner.transform(X_train)\n",
    "\n",
    "def build_mlp_model(input_shape, num_layers, units, initializer=None):\n",
    "    model = Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(input_shape,)))\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(layers.Dense(units, activation=\"relu\", kernel_initializer=initializer))\n",
    "        units = units // 2\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "    return model\n",
    "\n",
    "model_wrapper = KerasClassifier(\n",
    "    build_fn=build_mlp_model,\n",
    "    input_shape=20000, \n",
    "    epochs=20,\n",
    "    random_state=42,\n",
    "    num_layers=1,\n",
    "    units=64,\n",
    "    initializer=None,\n",
    "    verbose=4,\n",
    "    callbacks=None\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count\", vectorizer),\n",
    "    ('tf_idf', tf_idf),\n",
    "    ('feature_selection', k_best),\n",
    "    ('mlp', model_wrapper)\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'mlp__num_layers': [1, 2, 3],  \n",
    "    'mlp__units': [8, 16, 32, 64], \n",
    "    'mlp__initializer': [None, 'he_normal']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe, \n",
    "                  param_grid=params,\n",
    "                  scoring=SCORING, \n",
    "                  n_jobs=-1, \n",
    "                  refit='accuracy',\n",
    "                  cv=2, \n",
    "                  verbose=4, \n",
    "                  error_score='raise')\n",
    "\n",
    "# Assuming X_train_clean and y_train are defined\n",
    "grid_search = gs.fit(X_train_clean, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c6b4a11-58dc-43b2-b176-06f31f57372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaner = TextCleanerTransformer(stop_words=STOPWORDS, lemmatize=True)\n",
    "\n",
    "# Clean data before entering the pipeline for efficiency\n",
    "X_train_clean = text_cleaner.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934623f-7b82-4ad3-be9f-9e53086b8f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 21:00:56.586164: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 21:00:56.586391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 21:00:56.586813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 21:00:56.588275: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 21:00:56.589945: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 21:00:56.591750: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 21:00:56.595176: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 21:00:56.656023: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/ronlodetti/anaconda3/envs/capstone2-env/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6201 - accuracy: 0.6672\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6201 - accuracy: 0.6672\n",
      "Epoch 2/20\n",
      "309/326 [===========================>..] - ETA: 0s - loss: 0.6237 - accuracy: 0.6649Epoch 2/20\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6201 - accuracy: 0.6672\n",
      "  1/326 [..............................] - ETA: 12s - loss: 0.6016 - accuracy: 0.5938Epoch 2/20\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6201 - accuracy: 0.6672\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6213 - accuracy: 0.6687\n",
      "Epoch 2/20\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6213 - accuracy: 0.6687\n",
      " 20/326 [>.............................] - ETA: 0s - loss: 0.5669 - accuracy: 0.6516 Epoch 2/20\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6213 - accuracy: 0.6687\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 2s 4ms/step - loss: 0.6213 - accuracy: 0.6687\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5248 - accuracy: 0.7195\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5248 - accuracy: 0.7195\n",
      "324/326 [============================>.] - ETA: 0s - loss: 0.5250 - accuracy: 0.7193Epoch 3/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5248 - accuracy: 0.7195\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7225\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5248 - accuracy: 0.7195\n",
      "Epoch 3/20\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7225\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7225\n",
      "Epoch 3/20\n",
      "106/326 [========>.....................] - ETA: 0s - loss: 0.4785 - accuracy: 0.7904Epoch 2/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4653 - accuracy: 0.8063- loss: 0.4803 - accuracy: 0.79\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4653 - accuracy: 0.8063\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4653 - accuracy: 0.8063\n",
      "Epoch 4/20\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4653 - accuracy: 0.8063\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4668 - accuracy: 0.8064\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4668 - accuracy: 0.8064\n",
      "314/326 [===========================>..] - ETA: 0s - loss: 0.4684 - accuracy: 0.8042Epoch 4/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.4668 - accuracy: 0.8064\n",
      "  1/326 [..............................] - ETA: 12s - loss: 0.4339 - accuracy: 0.8438Epoch 4/20\n",
      "326/326 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7225\n",
      "Epoch 3/20\n",
      "260/326 [======================>.......] - ETA: 0s - loss: 0.4240 - accuracy: 0.85s: 0.4247 - accuracy: 0.85"
     ]
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "def build_mlp_model(input_shape, num_layers, units, initializer=None):\n",
    "    model = Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(input_shape,)))\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(layers.Dense(units, activation=\"relu\", kernel_initializer=initializer))\n",
    "        units = units // 2  # Reduce the units by half for each subsequent layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CALLBACKS list for model training\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, restore_best_weights=True, verbose=1)]\n",
    "\n",
    "model_wrapper = KerasClassifier(\n",
    "    build_fn=build_mlp_model,\n",
    "    input_shape=20000, \n",
    "    num_layers=1,\n",
    "    units=64,\n",
    "    initializer=None,\n",
    "    epochs=20,\n",
    "    random_state=42,\n",
    "    callbacks=callbacks)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(decode_error='replace', strip_accents='unicode', stop_words=None, ngram_range=(1, 2), max_df=0.95, min_df=2)),\n",
    "    ('tf_idf', TfidfTransformer()),\n",
    "    ('feature_selection', SelectKBest(k=20000)),\n",
    "    ('mlp', model_wrapper)\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'mlp__num_layers': [1, 2, 3],  \n",
    "    'mlp__units': [8, 16, 32, 64], \n",
    "    'mlp__initializer': [None, 'he_normal']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe, \n",
    "                  param_grid=params,\n",
    "                  scoring='accuracy',\n",
    "                  n_jobs=-1,\n",
    "                  cv=StratifiedKFold(n_splits=2, shuffle=True, random_state=42),\n",
    "                  verbose=4, \n",
    "                  error_score='raise')\n",
    "\n",
    "# The fitting process would be initiated with actual data\n",
    "grid_search = gs.fit(X_train_clean, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075a196-d4c0-441c-a83a-29c5c3ef35fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
