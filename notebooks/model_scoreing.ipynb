{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde919d2-e75b-4ce8-ad42-619eefee8e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "# Related third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from joblib import dump\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from tensorflow.data import AUTOTUNE as tf_AUTOTUNE, Dataset as tf_Dataset\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Bidirectional, GRU, TextVectorization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Local application/library specific imports would go here\n",
    "\n",
    "# Additional setup\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "STOPWORDS = stopwords.words('english')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow logging (1: INFO, 2: WARNING, 3: ERROR)\n",
    "warnings.filterwarnings('ignore')  # Suppress Python warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89375048-3c89-405d-b805-1f90470607bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos_optimized(treebank_tag):\n",
    "    \"\"\"Map POS tag to the first character that lemmatize() accepts.\"\"\"\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    # Default to NOUN if not found\n",
    "    return tag_dict.get(treebank_tag[0], wordnet.NOUN)\n",
    "\n",
    "def clean_text(review, tokenizer, stop_words=None, lemmatize=False, tokenize=False):\n",
    "    \"\"\"Clean and preprocess a single review text.\"\"\"\n",
    "    tokens = tokenizer.tokenize(review.lower())\n",
    "    \n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        tokens = [lemmatizer.lemmatize(word, get_wordnet_pos_optimized(tag)) for word, tag in pos_tags]\n",
    "    \n",
    "    if stop_words:\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return tokens if tokenize else ' '.join(tokens)\n",
    "\n",
    "def preprocess_texts(reviews, tokenizer, stop_words=None, lemmatize=False, tokenize=False):\n",
    "    \"\"\"Apply text cleaning and preprocessing to a list of texts.\"\"\"\n",
    "    return [clean_text(review, tokenizer, stop_words, lemmatize, tokenize) for review in reviews]\n",
    "\n",
    "def prepare_tf_dataset(X, y, batch_size, is_training=False):\n",
    "    \"\"\"Prepare a TensorFlow dataset for training or evaluation.\"\"\"\n",
    "    dataset = tf_Dataset.from_tensor_slices((X, y))\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(10000)\n",
    "    return dataset.batch(batch_size).cache().prefetch(tf_AUTOTUNE)\n",
    "\n",
    "def extract_performance_metrics(history, callbacks):\n",
    "    \"\"\"Extract best epoch performance metrics.\"\"\"\n",
    "    early_stopping = next((cb for cb in callbacks if isinstance(cb, EarlyStopping)), None)\n",
    "    if early_stopping and early_stopping.stopped_epoch > 0:\n",
    "        best_epoch = max(0, early_stopping.stopped_epoch - early_stopping.patience)\n",
    "    else:\n",
    "        best_epoch = len(history.history['loss']) - 1\n",
    "\n",
    "    metrics = {\n",
    "            'loss': history.history['loss'][best_epoch],\n",
    "            'val_loss': history.history['val_loss'][best_epoch],\n",
    "            'val_accuracy': history.history.get('val_accuracy', [None])[best_epoch],\n",
    "            'val_auc': history.history.get('val_auc', [None])[best_epoch]\n",
    "        }\n",
    "    return metrics   \n",
    "    \n",
    "def bag_of_words_cv(model_name, model, pipe, X_train, y_train, cv, model_score_list):\n",
    "    \"\"\"Cross-validate and save model and performance metrics.\"\"\"\n",
    "    model_pipeline = Pipeline(pipe.steps + [(model_name, model)])\n",
    "    cv_results = cross_validate(model_pipeline, X_train, y_train, cv=cv, scoring=['accuracy', 'roc_auc'], return_train_score=False)\n",
    "    \n",
    "    accuracy, auc = cv_results['test_accuracy'].mean(), cv_results['test_roc_auc'].mean()\n",
    "    score_list = [model_name, accuracy, auc]\n",
    "    model_score_list.append(score_list)\n",
    "    with open(f'{model_name}_scores.pkl', 'wb') as file:\n",
    "        pickle.dump(score_list, file)\n",
    "    \n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    dump(model_pipeline, f'{model_name}_pipeline.joblib')\n",
    "    df = pd.DataFrame([[accuracy, auc]], columns=['Accuracy', 'AUC'], index=[model_name])\n",
    "    return model_score_list, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd9b5cf2-f0ad-461c-9d2b-aab2a7a321ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\")\n",
    "\n",
    "# Load dataset and select relevant columns\n",
    "df = pd.read_csv('data/Airline_review.csv')[['Review_Title', 'Review', 'Recommended']]\n",
    "\n",
    "# Combine 'Review_Title' and 'Review' for feature X, map 'Recommended' to binary for target y\n",
    "X = df['Review_Title'] + ' ' + df['Review']\n",
    "y = df['Recommended'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbb1bf51-8ab5-44f0-b22b-d6f8001a251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TF-IDF Vectorizer configuration\n",
    "tf_idf = TfidfVectorizer(\n",
    "    decode_error='replace',\n",
    "    strip_accents='unicode',\n",
    "    ngram_range=(1, 2),\n",
    "    max_df=0.95,\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "# Initialize SelectKBest for feature selection\n",
    "k_best = SelectKBest(k=20000)\n",
    "\n",
    "# Setup the pipeline for bag-of-words model preparation\n",
    "bow_pipe = Pipeline([\n",
    "    (\"tf_idf\", tf_idf),\n",
    "    ('feature_selection', k_best)\n",
    "])\n",
    "\n",
    "# Preprocess texts for bag-of-words model, including lemmatization\n",
    "X_train_bow = preprocess_texts(X_train, tokenizer, stop_words=None, lemmatize=True)\n",
    "\n",
    "# Preprocess texts for sequence models without lemmatization\n",
    "X_train_seq = preprocess_texts(X_train, tokenizer, stop_words=None, lemmatize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "904ae83f-c61d-46c1-91ae-bdddcbb9c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stratified K-Fold for cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define callbacks for model training\n",
    "CALLBACKS = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0.001,\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize a list to store model performance metrics\n",
    "model_score_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61cec3ab-8229-47fd-99fa-194e6940349f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>0.499832</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy  AUC\n",
       "Dummy  0.499832  0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy Model\n",
    "dummy_model = DummyClassifier(strategy='uniform', random_state=42)\n",
    "\n",
    "# Perform cross-validation and evaluate the Dummy model using the bag-of-words pipeline\n",
    "model_score_list, df= bag_of_words_cv('Dummy', dummy_model, bow_pipe, \n",
    "                                      X_train_bow, y_train, skf, model_score_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f8dea08-c5d3-446a-8f10-24a708917201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.905481</td>\n",
       "      <td>0.96144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Accuracy      AUC\n",
       "Baseline  0.905481  0.96144"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline Model\n",
    "baseline_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "model_score_list, df= bag_of_words_cv('Baseline', baseline_model, bow_pipe, \n",
    "                                      X_train_bow, y_train, skf, model_score_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23da62f9-1a4b-4401-b41f-9620782a25f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic_Regression</th>\n",
       "      <td>0.916031</td>\n",
       "      <td>0.967635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy       AUC\n",
       "Logistic_Regression  0.916031  0.967635"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(\n",
    "    C=9.42012179027564, \n",
    "    max_iter=100, \n",
    "    solver='newton-cg'\n",
    ")\n",
    "model_score_list, df= bag_of_words_cv('Logistic_Regression', log_reg, bow_pipe, \n",
    "                                      X_train_bow, y_train, skf, model_score_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c61d7d6-4168-4c83-a482-87be299dcc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.887738</td>\n",
       "      <td>0.948371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy       AUC\n",
       "MultinomialNB  0.887738  0.948371"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Naive Bayes\n",
    "mnb_model = MultinomialNB(\n",
    "    fit_prior=False,\n",
    "    class_prior=None,\n",
    "    alpha=0.01\n",
    ")\n",
    "model_score_list, df= bag_of_words_cv('MultinomialNB', mnb_model, bow_pipe, \n",
    "                                      X_train_bow, y_train, skf, model_score_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9911819-e7d6-435b-a841-8c7a14d02aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Gradient_Boosting_Classifier</th>\n",
       "      <td>0.90222</td>\n",
       "      <td>0.958531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Accuracy       AUC\n",
       "Gradient_Boosting_Classifier   0.90222  0.958531"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "gbc_model = GradientBoostingClassifier(\n",
    "    subsample=0.8,\n",
    "    n_estimators=300,\n",
    "    min_samples_split=2,\n",
    "    max_features='sqrt',\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "model_score_list, df= bag_of_words_cv('Gradient_Boosting_Classifier', gbc_model, bow_pipe, \n",
    "                                      X_train_bow, y_train, skf, model_score_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a266864-d5a7-4af2-b2ef-dd005853b3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random_Forest_Classifier</th>\n",
       "      <td>0.886971</td>\n",
       "      <td>0.949088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Accuracy       AUC\n",
       "Random_Forest_Classifier  0.886971  0.949088"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rfc_model = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    max_depth=None,\n",
    "    bootstrap=True\n",
    ")\n",
    "model_score_list, df= bag_of_words_cv('Random_Forest_Classifier', rfc_model, bow_pipe, \n",
    "                                      X_train_bow, y_train, skf, model_score_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4132bdbc-a9cd-421f-9555-6f22ed97915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ronlo\\anaconda3\\envs\\capstone2-env\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>0.917915</td>\n",
       "      <td>0.969525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy       AUC\n",
       "MLP  0.917915  0.969525"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi-Layer Perceptions - Cross Validation\n",
    "tf_idf = TfidfVectorizer(\n",
    "    decode_error='replace', strip_accents='unicode', stop_words=None,\n",
    "    ngram_range=(1, 2), max_df=0.95, min_df=2\n",
    ")\n",
    "k_best = SelectKBest(k=20000)\n",
    "\n",
    "metrics_aggregate = {'loss': 0, 'val_loss': 0, 'val_accuracy': 0, 'val_auc': 0}\n",
    "runs = 0\n",
    "\n",
    "for train_index, validation_index in skf.split(X, y):\n",
    "    runs += 1\n",
    "    X_train_split, X_val_split = X[train_index], X[validation_index]\n",
    "    y_train_split, y_val_split = y[train_index], y[validation_index]\n",
    "\n",
    "    # Apply transformations\n",
    "    X_train_split = k_best.fit_transform(tf_idf.fit_transform(X_train_split), y_train_split)\n",
    "    X_val_split = k_best.transform(tf_idf.transform(X_val_split))\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_ds = prepare_tf_dataset(X_train_split.toarray(), y_train_split, batch_size=256, is_training=True)\n",
    "    val_ds = prepare_tf_dataset(X_val_split.toarray(), y_val_split, batch_size=256)\n",
    "\n",
    "    # Model definition\n",
    "    mlp_model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(20000,)),\n",
    "        Dropout(0.8),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "    # Model training\n",
    "    history = mlp_model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=100,\n",
    "        verbose=0,\n",
    "        callbacks=CALLBACKS\n",
    "    )\n",
    "    \n",
    "    # Extract and aggregate performance metrics\n",
    "    metrics = extract_performance_metrics(history, CALLBACKS)\n",
    "    for key, value in metrics.items():\n",
    "        metrics_aggregate[key] += value\n",
    "\n",
    "# Calculate average metrics over all runs\n",
    "results_dict = {key: value / runs for key, value in metrics_aggregate.items()}\n",
    "accuracy = results_dict['val_accuracy']\n",
    "auc = results_dict['val_auc']\n",
    "\n",
    "# Append results to the model score list and save to file\n",
    "score_list = ['MLP', accuracy, auc]\n",
    "model_score_list.append(score_list)\n",
    "with open('MLP_scores.pkl', 'wb') as file:\n",
    "    pickle.dump(score_list, file)\n",
    "\n",
    "# Create and display a DataFrame with the results\n",
    "df = pd.DataFrame([[accuracy, auc]], columns=['Accuracy', 'AUC'], index=['MLP'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "548e304f-7afa-4e12-8751-47688e85ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP - Fitting and Saving\n",
    "tf_idf = TfidfVectorizer(decode_error='replace', strip_accents='unicode', stop_words=None, ngram_range=(1, 2), max_df=0.95, min_df=2)\n",
    "k_best = SelectKBest(k=20000)\n",
    "\n",
    "# Assuming X_train_bow and y_train are your features and labels\n",
    "X = np.array(X_train_bow)\n",
    "y = np.array(y_train)\n",
    "\n",
    "# Splitting dataset\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Feature extraction and selection\n",
    "X_train_split = k_best.fit_transform(tf_idf.fit_transform(X_train_split), y_train_split)\n",
    "X_val_split = k_best.transform(tf_idf.transform(X_val_split))\n",
    "\n",
    "# Prepare datasets\n",
    "train_ds = prepare_tf_dataset(X_train_split.toarray(), y_train_split, batch_size=256, is_training=True)\n",
    "val_ds = prepare_tf_dataset(X_val_split.toarray(), y_val_split, batch_size=256)\n",
    "\n",
    "# Model definition\n",
    "mlp_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(20000,)),\n",
    "    Dropout(0.8),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Model compilation\n",
    "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "# Model fitting\n",
    "mlp_model.fit(train_ds, validation_data=val_ds, epochs=100, verbose=0, callbacks=CALLBACKS)\n",
    "\n",
    "# Save the model\n",
    "mlp_model.save('MLP_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e47ee7d3-2d07-4cda-9243-32c4106feb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RNN</th>\n",
       "      <td>0.890759</td>\n",
       "      <td>0.951667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy       AUC\n",
       "RNN  0.890759  0.951667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN Model - Cross Validation\n",
    "metrics_aggregate = {'loss': 0, 'val_loss': 0, 'val_accuracy': 0, 'val_auc': 0}\n",
    "X = np.array(X_train_seq)\n",
    "y = np.array(y_train)\n",
    "\n",
    "# Configure text vectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=20000,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=200\n",
    ")\n",
    "\n",
    "# Run cross-validation\n",
    "runs = 0\n",
    "for train_index, validation_index in skf.split(X, y):\n",
    "    runs += 1\n",
    "    X_train_split, X_val_split = X[train_index], X[validation_index]\n",
    "    y_train_split, y_val_split = y[train_index], y[validation_index]\n",
    "\n",
    "    # Adapt the text vectorization layer and transform inputs\n",
    "    text_vectorization.adapt(X_train_split)\n",
    "    X_train_split = text_vectorization(X_train_split)\n",
    "    X_val_split = text_vectorization(X_val_split)\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_ds = prepare_tf_dataset(X_train_split, y_train_split, batch_size=256, is_training=True)\n",
    "    val_ds = prepare_tf_dataset(X_val_split, y_val_split, batch_size=256)\n",
    "\n",
    "    # Define the RNN model\n",
    "    rnn_model = Sequential([\n",
    "        Embedding(input_dim=20000, output_dim=32, input_length=200),\n",
    "        Bidirectional(GRU(16)),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "    # Train the model\n",
    "    history = rnn_model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=100,\n",
    "        verbose=0,\n",
    "        callbacks=CALLBACKS\n",
    "    )\n",
    "    \n",
    "    # Extract and aggregate performance metrics\n",
    "    metrics = extract_performance_metrics(history, CALLBACKS)\n",
    "    for key in metrics_aggregate:\n",
    "        metrics_aggregate[key] += metrics[key]\n",
    "\n",
    "# Calculate average metrics over all runs\n",
    "results_dict = {key: val / runs for key, val in metrics_aggregate.items()}\n",
    "accuracy = results_dict['val_accuracy']\n",
    "auc = results_dict['val_auc']\n",
    "\n",
    "# Append results and save to a file\n",
    "score_list = ['RNN', accuracy, auc]\n",
    "model_score_list.append(score_list)\n",
    "with open('RNN_scores.pkl', 'wb') as file:\n",
    "    pickle.dump(score_list, file)\n",
    "\n",
    "# Create and display a DataFrame with the results\n",
    "df = pd.DataFrame([[accuracy, auc]], columns=['Accuracy', 'AUC'], index=['RNN'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94d221f6-bd7a-4a24-a3dc-41042cee6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN - Fitting and Saving\n",
    "text_vectorization = TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=20000,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=200\n",
    ")\n",
    "\n",
    "# Convert text data and labels to numpy arrays for TensorFlow processing\n",
    "X = np.array(X_train_seq)\n",
    "y = np.array(y_train)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Adapt the text vectorization layer to the training data\n",
    "text_vectorization.adapt(X_train_split)\n",
    "\n",
    "# Vectorize the training and validation text data\n",
    "X_train_split = text_vectorization(X_train_split)\n",
    "X_val_split = text_vectorization(X_val_split)\n",
    "\n",
    "# Prepare TensorFlow datasets for training and validation\n",
    "train_ds = prepare_tf_dataset(X_train_split, y_train_split, batch_size=256, is_training=True)\n",
    "val_ds = prepare_tf_dataset(X_val_split, y_val_split, batch_size=256)\n",
    "\n",
    "# Define the RNN model architecture\n",
    "rnn_model = Sequential([\n",
    "    Embedding(input_dim=20000, output_dim=32, input_length=200),\n",
    "    Bidirectional(GRU(16)),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with optimizer, loss function, and metrics\n",
    "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "# Fit the model on the training data and validate using the validation data\n",
    "rnn_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=100,\n",
    "    verbose=0,\n",
    "    callbacks=CALLBACKS\n",
    ")\n",
    "\n",
    "# Save the trained model to an H5 file\n",
    "rnn_model.save('RNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a62ed76f-b985-4f28-9939-288ba4e0363f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GloVe</th>\n",
       "      <td>0.909557</td>\n",
       "      <td>0.96396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy      AUC\n",
       "GloVe  0.909557  0.96396"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GloVe Model - Cross Validation\n",
    "text_vectorization = TextVectorization(standardize=None, max_tokens=20000, output_mode='int', output_sequence_length=200)\n",
    "X = np.array(X_train_seq)\n",
    "y = np.array(y_train)\n",
    "text_vectorization.adapt(X)\n",
    "\n",
    "# Load GloVe embeddings for words in the vocabulary\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "vocab_size = len(vocabulary)\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "glove_path = 'data/glove.6B.300d.txt'\n",
    "\n",
    "glove_embeddings = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word, vector = values[0], np.asarray(values[1:], dtype='float32')\n",
    "        if word in vocabulary:\n",
    "            glove_embeddings[word] = vector\n",
    "\n",
    "for i, word in enumerate(vocabulary):\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Define metrics aggregation\n",
    "metrics_aggregate = {'loss': 0, 'val_loss': 0, 'val_accuracy': 0, 'val_auc': 0}\n",
    "runs = 0\n",
    "\n",
    "for train_index, validation_index in skf.split(X, y):\n",
    "    runs += 1\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = X[train_index], X[validation_index], y[train_index], y[validation_index]\n",
    "\n",
    "    # Vectorize text data\n",
    "    X_train_split = text_vectorization(X_train_split)\n",
    "    X_val_split = text_vectorization(X_val_split)\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_ds = prepare_tf_dataset(X_train_split, y_train_split, 256, is_training=True)\n",
    "    val_ds = prepare_tf_dataset(X_val_split, y_val_split, 256)\n",
    "\n",
    "    # Initialize and compile the model\n",
    "    glv_model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=300, input_length=200, weights=[embedding_matrix], trainable=False),\n",
    "        Bidirectional(GRU(32)),\n",
    "        Dropout(0.4),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    glv_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "    # Train the model\n",
    "    history = glv_model.fit(train_ds, validation_data=val_ds, epochs=100, verbose=0, callbacks=CALLBACKS)\n",
    "    \n",
    "    # Extract and aggregate performance metrics\n",
    "    metrics = extract_performance_metrics(history, CALLBACKS)\n",
    "    for key, value in metrics.items():\n",
    "        metrics_aggregate[key] += value\n",
    "\n",
    "# Calculate and display average metrics over all runs\n",
    "results_dict = {key: value / runs for key, value in metrics_aggregate.items()}\n",
    "accuracy, auc = results_dict['val_accuracy'], results_dict['val_auc']\n",
    "\n",
    "# Save results\n",
    "score_list = ['GloVe', accuracy, auc]\n",
    "model_score_list.append(score_list)\n",
    "with open('GloVe_scores.pkl', 'wb') as file:\n",
    "    pickle.dump(score_list, file)\n",
    "\n",
    "df = pd.DataFrame([[accuracy, auc]], columns=['Accuracy', 'AUC'], index=['GloVe'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78fa243d-48b1-4d0f-aca3-f275363bc58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GloVe model - Fitting and Saving\n",
    "text_vectorization = TextVectorization(standardize=None,\n",
    "                                       max_tokens=20000,\n",
    "                                       output_mode='int',\n",
    "                                       output_sequence_length=200)\n",
    "\n",
    "X = np.array(X_train_seq)\n",
    "y = np.array(y_train)\n",
    "\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "text_vectorization.adapt(X_train_split)\n",
    "X_train_split = text_vectorization(X_train_split)\n",
    "X_val_split = text_vectorization(X_val_split)\n",
    "\n",
    "train_ds = prepare_tf_dataset(X_train_split, y_train_split, 256, is_training=True)\n",
    "val_ds = prepare_tf_dataset(X_val_split, y_val_split, 256)\n",
    "\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Load GloVe embeddings from file.\n",
    "glove_embeddings = {}\n",
    "with open('data/glove.6B.300d.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "# Initialize the embedding matrix with zeros.\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "\n",
    "# Populate the embedding matrix with GloVe vectors.\n",
    "for i, word in enumerate(vocabulary):\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "glv_model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=300, input_length=200, weights=[embedding_matrix], trainable=False),\n",
    "    Bidirectional(GRU(32)),\n",
    "    Dropout(0.4),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "glv_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "\n",
    "results = glv_model.fit(train_ds,\n",
    "                    validation_data= val_ds,\n",
    "                    epochs=100,\n",
    "                    verbose=0,\n",
    "                    callbacks=CALLBACKS)\n",
    "\n",
    "glv_model.save('GloVe_model.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6beaecf2-0eda-465e-9198-ebd85d827c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dummy</td>\n",
       "      <td>0.499832</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.905481</td>\n",
       "      <td>0.961440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>0.916031</td>\n",
       "      <td>0.967635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.887738</td>\n",
       "      <td>0.948371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient_Boosting_Classifier</td>\n",
       "      <td>0.902220</td>\n",
       "      <td>0.958531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random_Forest_Classifier</td>\n",
       "      <td>0.886971</td>\n",
       "      <td>0.949088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.917915</td>\n",
       "      <td>0.969525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RNN</td>\n",
       "      <td>0.890759</td>\n",
       "      <td>0.951667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GloVe</td>\n",
       "      <td>0.909557</td>\n",
       "      <td>0.963960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy       AUC\n",
       "0                         Dummy  0.499832  0.500000\n",
       "1                      Baseline  0.905481  0.961440\n",
       "2           Logistic_Regression  0.916031  0.967635\n",
       "3                 MultinomialNB  0.887738  0.948371\n",
       "4  Gradient_Boosting_Classifier  0.902220  0.958531\n",
       "5      Random_Forest_Classifier  0.886971  0.949088\n",
       "6                           MLP  0.917915  0.969525\n",
       "7                           RNN  0.890759  0.951667\n",
       "8                         GloVe  0.909557  0.963960"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from the scores list\n",
    "scores_df = pd.DataFrame(model_score_list, columns=['Model', 'Accuracy', 'AUC'])\n",
    "\n",
    "# Save the DataFrame to a pickle file for later use\n",
    "scores_df.to_pickle('model_scores_df.pkl')\n",
    "\n",
    "# Display the DataFrame\n",
    "scores_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
