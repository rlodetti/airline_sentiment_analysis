{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b78f010-bacc-46f5-87af-88fc2cfc7940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ronlodetti/Documents/Flatiron/capstone/airline_sentiment_analysis/hidden\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcaec2-dcc8-4672-80b3-8609c032892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import src.code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b815993-65a1-4c85-9cda-ac98102d5da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Airline_review.csv')[['Review_Title','Review','Recommended']]\n",
    "\n",
    "# Counting number of words and characters in original review\n",
    "df['word_count'] = df['Review'].apply(lambda x : len(x.split()))\n",
    "df['char_count'] = df['Review'].apply(lambda x : len(x))\n",
    "\n",
    "# Process and add the review titles after removing quotes\n",
    "df['review_title']= df['Review_Title'].apply(lambda x : x.replace('\"', ''))\n",
    "\n",
    "# Combine review titles and content, then clean and tokenize\n",
    "combined_text = df['review_title'] + ' ' + df['Review']\n",
    "df['tokens'] = combined_text.apply(clean_tokenize)\n",
    "\n",
    "# Join the tokens back into a cleaned review\n",
    "df['clean_review'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "df.rename(columns={'Recommended': 'recommended','Review':'review'}, inplace=True)\n",
    "\n",
    "df = df[['review','clean_review','tokens','word_count','char_count','recommended']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719da254-1b4a-4bd0-8d48-18a5c3ac0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdba40-e8be-4b68-858c-a48bebd34b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d25723-5f31-473a-b19b-bbd65d37e3a6",
   "metadata": {},
   "source": [
    "# Descriptive Analysis\n",
    "- Ratio of \"yes\" to \"no\"\n",
    "- Word and character count\n",
    "- Most common words & phrases\n",
    "- vectorization (e.g., TF-IDF, word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b1dec-a1aa-4cc4-9710-bab90133f644",
   "metadata": {},
   "source": [
    "### Ratio of \"Yes\" to \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc7d2c0-022a-48c9-9715-c06ec45c5b60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['recommended'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191ef02-6cc6-4926-aee8-402fe368f2cf",
   "metadata": {},
   "source": [
    "### Word and Character Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3436a83-ee23-4d67-89ab-18307f272946",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='word_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34170e3d-9f51-4954-bc97-f887c596f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='char_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430df4f-92cf-4db2-9717-7974b34c4146",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_description = df.groupby('recommended')['word_count'].describe().T\n",
    "word_count_description.columns = [f'{col}_word_count' for col in word_count_description.columns]\n",
    "word_count_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba1a96-38de-404e-8ea6-c5f2a960c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_count_description = df.groupby('recommended')['char_count'].describe().T\n",
    "char_count_description.columns = [f'{col}_char_count' for col in char_count_description.columns]\n",
    "char_count_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a14872-4c1a-4a07-ac9a-1a44995d60d9",
   "metadata": {},
   "source": [
    "### Most common words & phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b21a8d-8c21-4a37-bace-a14879a6c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_tokens = list(df.loc[df['recommended'] == 'yes', 'tokens'].explode())\n",
    "no_tokens = list(df.loc[df['recommended'] == 'no', 'tokens'].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7697502-b407-4a17-a110-2a89a396e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk import FreqDist\n",
    "freqdist = FreqDist(yes_tokens)\n",
    "yes_common = freqdist.most_common(25)\n",
    "yes_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a24e97-27cf-48fa-acd6-cfd6bd334ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freqdist = FreqDist(no_tokens)\n",
    "no_common = freqdist.most_common(25)\n",
    "no_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5c0af-8ba7-488b-8184-4a1775b95a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_yes_words = [i[0] for i in yes_common]\n",
    "common_no_words = [i[0] for i in no_common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f57a75-4324-4dd3-bda1-f18cef03b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in common_yes_words:\n",
    "    if i not in common_no_words:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae77cab-5c06-4b2b-a57a-2a0ab1b6ea86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in common_no_words:\n",
    "    if i not in common_yes_words:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20168695-24ae-49e4-ae7a-93f4652dae60",
   "metadata": {},
   "source": [
    "Do we remove stop words or not?\n",
    "Do we stem or lemmatize our text data, or leave the words as is?\n",
    "Is basic tokenization enough, or do we need to support special edge cases through the use of regex?\n",
    "Do we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?\n",
    "Do we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?\n",
    "What sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4170c7-2311-49f3-a237-f753bb91a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for count vectorizatioin\n",
    "def count_vectorize(tokenized_song):\n",
    "    unique_words = set(tokenized_song)\n",
    "\n",
    "    song_dict = {word:0 for word in unique_words}\n",
    "\n",
    "    for word in tokenized_song:\n",
    "        song_dict[word] += 1\n",
    "\n",
    "    return song_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef2859d-ae2a-42de-a5be-b6c55676a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf\n",
    "def inverse_document_frequency(list_of_token_songs):\n",
    "    num_docs = len(list_of_token_songs)\n",
    "    \n",
    "    unique_words = set([item for sublist in list_of_token_songs for item in sublist])\n",
    "    # Same as:\n",
    "    # unique_words = set()\n",
    "    # for song in list_of_dicts:\n",
    "    #     for word in song.keys():\n",
    "    #         unique_words.add(word)\n",
    "            \n",
    "    inv_doc_freq = {word:0 for word in unique_words}\n",
    "\n",
    "    for word in unique_words:\n",
    "        num_docs_with_word = 0\n",
    "        for song_tokens in list_of_token_songs:\n",
    "            if word in song_tokens:\n",
    "                num_docs_with_word += 1\n",
    "        inv_doc_freq[word] = np.log(num_docs / num_docs_with_word)\n",
    "        \n",
    "    return inv_doc_freq\n",
    "\n",
    "def tf_idf(list_of_token_songs):\n",
    "    \n",
    "    unique_words = set([item for sublist in list_of_token_songs for item in sublist])\n",
    "    \n",
    "    idf = inverse_document_frequency(list_of_token_songs)\n",
    "    \n",
    "    tf_idf_list_of_dicts = []\n",
    "    for song_tokens in list_of_token_songs:\n",
    "        song_tf = count_vectorize(song_tokens)\n",
    "        doc_tf_idf = {word:0 for word in unique_words}\n",
    "        for word in unique_words:\n",
    "            if word in song_tokens:\n",
    "                doc_tf_idf[word] = song_tf[word] * idf[word]\n",
    "            else:\n",
    "                doc_tf_idf[word] = 0\n",
    "        tf_idf_list_of_dicts.append(doc_tf_idf)\n",
    "        \n",
    "    return tf_idf_list_of_dicts\n",
    "\n",
    "def main(filenames):\n",
    "\n",
    "    all_songs = []\n",
    "    for song in filenames:\n",
    "        with open(f'data/{song}') as f:\n",
    "            song_lyrics = f.readlines()\n",
    "            all_songs.append(song_lyrics)\n",
    "    \n",
    "    all_song_tokens = []\n",
    "\n",
    "    for song in all_songs:\n",
    "        song_tokens = word_tokenize(clean_song(song))\n",
    "        all_song_tokens.append(song_tokens)\n",
    "\n",
    "    tf_idf_all_docs = tf_idf(all_song_tokens)\n",
    "    return tf_idf_all_docs\n",
    "\n",
    "tf_idf_all_docs = main(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45260879-15c8-41dc-b3d2-64f55bad90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized Word Frequency\n",
    "total_word_count = sum(macbeth_stopped_freqdist.values())\n",
    "macbeth_top_50 = macbeth_stopped_freqdist.most_common(50)\n",
    "print(f'{\"Word\":10} Normalized Frequency')\n",
    "for word in macbeth_top_50:\n",
    "    normalized_frequency = word[1] / total_word_count\n",
    "    print(f'{word[0]:10} {normalized_frequency:^20.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17afb1d-e552-415c-95c7-31ee81ba01c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigrams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "macbeth_finder = BigramCollocationFinder.from_words(tokens)\n",
    "macbeth_scored = macbeth_finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d211ea36-a1b6-4526-a68c-5cf2dbbda0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pointwise Information\n",
    "macbeth_pmi_finder = BigramCollocationFinder.from_words(macbeth_words_stopped)\n",
    "macbeth_pmi_finder.apply_freq_filter(5)\n",
    "macbeth_pmi_scored = macbeth_pmi_finder.score_ngrams(bigram_measures.pmi)\n",
    "macbeth_pmi_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dda30b-1680-4ccc-ad7f-b44f42a36d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc preparer\n",
    "def doc_preparer(doc, stop_words=sw):\n",
    "    '''\n",
    "    \n",
    "    :param doc: a document from the satire corpus \n",
    "    :return: a document string with words which have been \n",
    "            lemmatized, \n",
    "            parsed for stopwords, \n",
    "            made lowercase,\n",
    "            and stripped of punctuation and numbers.\n",
    "    '''\n",
    "    \n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\")\n",
    "    doc = regex_token.tokenize(doc)\n",
    "    doc = [word.lower() for word in doc]\n",
    "    doc = [word for word in doc if word not in sw]\n",
    "    print(doc)\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(word[0], get_wordnet_pos(word[1])) for word in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    return ' '.join(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9fedc-a60e-435c-b2d3-98a3449d77cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondary train-test split to build our best model\n",
    "X_t, X_val, y_t, y_val = train_test_split(token_docs, y_train,\n",
    "                                          test_size=0.25, random_state=42)\n",
    "cv = CountVectorizer(max_features=5)\n",
    "\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de075d87-e26f-478c-8cc2-16aed287997a",
   "metadata": {},
   "source": [
    "Load the Data\n",
    "Use pandas and sklearn.datasets to load the train and test data into appropriate data structures. Then get a sense of what is in this dataset by visually inspecting some samples.\n",
    "\n",
    "2. Perform Data Cleaning and Exploratory Data Analysis with nltk\n",
    "Standardize the case of the data and use a tokenizer to convert the full posts into lists of individual words. Then compare the raw word frequency distributions of each category.\n",
    "\n",
    "3. Build and Evaluate a Baseline Model with TfidfVectorizer and MultinomialNB\n",
    "Ultimately all data must be in numeric form in order to be able to fit a scikit-learn model. So we'll use a tool from sklearn.feature_extraction.text to convert all data into a vectorized format.\n",
    "\n",
    "Initially we'll keep all of the default parameters for both the vectorizer and the model, in order to develop a baseline score.\n",
    "\n",
    "4. Iteratively Perform and Evaluate Preprocessing and Feature Engineering Techniques\n",
    "Here you will investigate three techniques, to determine whether they should be part of our final modeling process:\n",
    "\n",
    "Removing stopwords\n",
    "Using custom tokens\n",
    "Domain-specific feature engineering\n",
    "Increasing max_features\n",
    "5. Evaluate a Final Model on the Test Set\n",
    "Once you have chosen a final modeling process, fit it on the full training data and evaluate it on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64b540-48c7-4100-b8a1-b57d8051d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "tokenizer.tokenize(politics_sample)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac3b43a-925b-4fab-9457-2395587a4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def visualize_top_10(freq_dist, title):\n",
    "\n",
    "    # Extract data for plotting\n",
    "    top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "    tokens = top_10[0]\n",
    "    counts = top_10[1]\n",
    "\n",
    "    # Set up plot and plot data\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(tokens, counts)\n",
    "\n",
    "    # Customize plot appearance\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    \n",
    "visualize_top_10(example_freq_dist, \"Top 10 Word Frequency for Example Tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0e38c-36e6-49ca-9f45-7290d3401255",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_freq_dist = FreqDist(train_sample[\"text_tokenized\"].explode())\n",
    "visualize_top_10(sample_freq_dist, \"Top 10 Word Frequency for 5 Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b1ea0-ece7-45eb-aa72-44e9174c0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the relevant vectorizer class\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate a vectorizer with max_features=10\n",
    "# (we are using the default token pattern)\n",
    "tfidf = TfidfVectorizer(max_features=10)\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n",
    "\n",
    "# Visually inspect the vectorized data\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a51ebc-eb82-42ca-b74c-f2c78cf0b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import relevant class and function\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate a MultinomialNB classifier\n",
    "baseline_model = MultinomialNB()\n",
    "\n",
    "# Evaluate the classifier on X_train_vectorized and y_train\n",
    "baseline_cv = cross_val_score(baseline_model, X_train_vectorized, y_train)\n",
    "baseline_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a2812e-4d6c-46a9-9c35-2e26edbf0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(token_list):\n",
    "    \"\"\"\n",
    "    Given a list of tokens, return a list where the tokens\n",
    "    that are also present in stopwords_list have been\n",
    "    removed\n",
    "    \"\"\"\n",
    "    stopwords_removed = [token for token in token_list if token not in stopwords_list]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55845a51-2cf2-4c93-9c72-df2c2d4b7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10,\n",
    "    stop_words=stopwords_list\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n",
    "\n",
    "# Visually inspect the vectorized data\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecaf72f-672c-4719-802e-2be4345f13d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline:         \", baseline_cv.mean())\n",
    "print(\"Stopwords removed:\", stopwords_removed_cv.mean())\n",
    "Baseline:          0.4013364135429863\n",
    "Stopwords removed: 0.41756464714211183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68459ba-f17c-4749-a7c2-ecba39a908a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num sentence\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(X_train.iloc[100][\"text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
