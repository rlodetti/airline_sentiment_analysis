{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b46d6eb-6061-4d35-baad-35d072d27044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "import math\n",
    "import re\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "def clean_tokenize(review_string):\n",
    "    \"\"\"\n",
    "    Clean and tokenize a review string by removing stopwords, punctuation,\n",
    "    numbers, and empty strings, and then applying lemmatization.\n",
    "    \"\"\"    \n",
    "    lower_string = review_string.lower()\n",
    "    \n",
    "    # Combine stopwords and punctuation into one list\n",
    "    extended_stopwords = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "    # Compile a regular expression to match numbers\n",
    "    number_pattern = re.compile(r'\\d+')\n",
    "\n",
    "    # Tokenize the review string\n",
    "    tokens = word_tokenize(lower_string)\n",
    "\n",
    "    # Filter out invalid tokens and convert to lowercase\n",
    "    valid_tokens = [w for w in tokens if w not in extended_stopwords and not number_pattern.search(w)]\n",
    "\n",
    "    # Initialize the WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Lemmatize the valid tokens\n",
    "    clean_tokens = [lemmatizer.lemmatize(token) for token in valid_tokens]\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "def count_vectorize(tokenized_doc):\n",
    "    unique_words = set(tokenized_doc)\n",
    "    word_dict = {word:0 for word in unique_words}\n",
    "    for word in tokenized_doc:\n",
    "        word_dict[word] += 1\n",
    "    return word_dict\n",
    "\n",
    "def remove_quotes(text):\n",
    "    text = text.replace('\"', '')\n",
    "    return text\n",
    "\n",
    "def remove_ordinal_suffix(day):\n",
    "    return re.sub(r'\\D', '', day)\n",
    "\n",
    "def date_converter(date_str):\n",
    "    date_list = date_str.split()        \n",
    "    day = remove_ordinal_suffix(date_list[0])\n",
    "    month = date_list[1]\n",
    "    year = date_list[2]\n",
    "\n",
    "    new_date_str = f\"{day} {month} {year}\"\n",
    "    \n",
    "    return new_date_str\n",
    "\n",
    "def split_route(route):\n",
    "    route_clean = str(route).lower().strip()\n",
    "    if ' to ' in route_clean:\n",
    "        if ' via ' in route_clean:\n",
    "            direct_route = route_clean.split(' via ')\n",
    "            locations = direct_route[0].split(' to ')\n",
    "        else:\n",
    "            locations = route_clean.split(' to ')\n",
    "        origin = locations[0].strip()\n",
    "        destination = locations[-1].strip()\n",
    "    else:\n",
    "        origin = None\n",
    "        destination = None\n",
    "\n",
    "    # Extracting the first (departing) and last (arriving) locations\n",
    "    return origin, destination\n",
    "\n",
    "# Function to clean and get unique values from a DataFrame column\n",
    "def get_unique_col_values(df, col_names):\n",
    "    unique_vals = pd.concat([df[col].str.lower().str.strip() for col in col_names]).unique()\n",
    "    return unique_vals\n",
    "    \n",
    "def closest_match(query, choices):\n",
    "    try:\n",
    "        closest_match = process.extractOne(query, choices, scorer=fuzz.WRatio)\n",
    "        match = closest_match[0]\n",
    "        score = closest_match[1]\n",
    "        return match, score\n",
    "    except Exception:\n",
    "        return (None, 0)\n",
    "\n",
    "def try_map(word,mapper):\n",
    "    try:\n",
    "        return mapper[word]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_distance(origin, destination):\n",
    "    try:\n",
    "        origin_lat, origin_lon = [float(x) for x in origin.split(', ')]\n",
    "        destination_lat, destination_lon = [float(x) for x in destination.split(', ')]\n",
    "        # Calculating haversine distance\n",
    "        origin_lon, origin_lat, destination_lon, destination_lat = map(math.radians, [origin_lon, origin_lat, destination_lon, destination_lat])\n",
    "        dlon = destination_lon - origin_lon\n",
    "        dlat = destination_lat - origin_lat\n",
    "        a = math.sin(dlat/2)**2 + math.cos(origin_lat) * math.cos(destination_lat) * math.sin(dlon/2)**2\n",
    "        c = 2 * math.asin(math.sqrt(a))\n",
    "        r = 6371\n",
    "        return c * r\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82160fb6-2fb6-49f7-96d0-3714d24e27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean & Prep new df\n",
    "\n",
    "# Load the original dataset\n",
    "old_df = pd.read_csv('data/Airline_review.csv').iloc[:, 1:]\n",
    "\n",
    "# Initialize a new DataFrame\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Process and add the review titles after removing quotes\n",
    "df['review_title']= old_df['Review_Title'].map(remove_quotes)\n",
    "\n",
    "# Combine review titles and content, then clean and tokenize\n",
    "combined_text = old_df['Review_Title'].apply(remove_quotes) + ' ' + old_df['Review']\n",
    "df['tokens'] = combined_text.apply(clean_tokenize)\n",
    "\n",
    "# Join the tokens back into a cleaned review\n",
    "df['clean_review'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Convert review and flight dates to datetime format\n",
    "df['date_review'] = pd.to_datetime(old_df['Review Date'].apply(date_converter), format='%d %B %Y')\n",
    "df['date_flown'] = pd.to_datetime(old_df['Date Flown'], format='%B %Y')\n",
    "\n",
    "# Splitting Route into origin and destination\n",
    "df['origin'], df['destination'] = zip(*old_df['Route'].apply(split_route))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5a52cfe-b604-4d42-8e74-c220dca150ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and cleaning the airport codes data\n",
    "codes_df = pd.read_csv('data/airport_codes.csv', sep=';')\n",
    "codes_df = codes_df.rename(columns={\n",
    "    'Airport Code': 'code',\n",
    "    'Airport Name': 'airport',\n",
    "    'City Name': 'city'\n",
    "}).apply(lambda x: x.str.lower().str.strip() if x.name in ['code', 'airport', 'city'] else x)\n",
    "codes_df = codes_df[['code', 'airport', 'city', 'coordinates']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36e0ccf7-55a8-4385-892d-690b16a78e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all unique airport codes, airport names and city names\n",
    "unique_codes = get_unique_col_values(codes_df, ['code', 'airport', 'city'])\n",
    "\n",
    "# Getting all unique origins and destinations\n",
    "unique_originals = get_unique_col_values(df, ['origin', 'destination'])\n",
    "\n",
    "# Finding the closest match to help merge dataframes\n",
    "matches = []\n",
    "scores = []\n",
    "for word in unique_originals:\n",
    "    match, score = closest_match(word,unique_codes)\n",
    "    matches.append(match)\n",
    "    scores.append(score)\n",
    "score_df = pd.DataFrame({'word': unique_originals, 'match': matches, 'score': scores})\n",
    "\n",
    "# Filtering out rows with match scores of less than 80\n",
    "score_df = score_df.loc[score_df['score'] >= 80]\n",
    "\n",
    "# Creating a dictionary to map coordinates onto matched values\n",
    "melted_df = codes_df.melt(id_vars='coordinates', value_vars=['code', 'airport', 'city'], value_name='Key').drop('variable', axis=1)\n",
    "result_dict = dict(zip(melted_df['Key'], melted_df['coordinates']))\n",
    "\n",
    "# Adding coordinates to score_Df\n",
    "score_df['coordinates'] = score_df['match'].apply(lambda x: try_map(x,result_dict))\n",
    "\n",
    "# Enrich the original DataFrame with origin and destination coordinates\n",
    "df['origin_coordinates'] = pd.merge(df, score_df, left_on= 'origin',right_on='word', how='left')['coordinates']\n",
    "df['destination_coordinates'] = pd.merge(df, score_df, left_on= 'destination',right_on='word', how='left')['coordinates']\n",
    "\n",
    "# Mapping city names to origin and destination based on coordinates\n",
    "df['origin_city'] = pd.merge(df, codes_df, left_on= 'origin_coordinates',right_on='coordinates', how='left')['city']\n",
    "df['destination_city'] = pd.merge(df, codes_df, left_on= 'destination_coordinates',right_on='coordinates', how='left')['city']\n",
    "\n",
    "# Calculate distances\n",
    "df['distance'] = df.apply(lambda row: calculate_distance(row['origin_coordinates'], row['destination_coordinates']), axis=1)\n",
    "\n",
    "df.drop(['origin_coordinates', 'destination_coordinates','origin','destination','review_title'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e801f11a-cfba-45e0-83ad-65c1fb888b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23171 entries, 0 to 23170\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   review_title      23171 non-null  object        \n",
      " 1   tokens            23171 non-null  object        \n",
      " 2   clean_review      23171 non-null  object        \n",
      " 3   date_review       23171 non-null  datetime64[ns]\n",
      " 4   date_flown        19417 non-null  datetime64[ns]\n",
      " 5   origin_city       19592 non-null  object        \n",
      " 6   destination_city  19517 non-null  object        \n",
      " 7   distance          19295 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(1), object(5)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf1934-ba0e-4c10-828d-851dcd26aa63",
   "metadata": {},
   "source": [
    "['Airline Name', lower\n",
    " 'Overall_Rating', Change to int, impute n to median\n",
    " 'Verified',\n",
    " 'Type Of Traveller', drop. there doesn't appear to be a correlation and i don't want to delete 4,000 rows\n",
    " 'Seat Type',cat ?? Not sure\n",
    " \n",
    " 'Seat Comfort', convert to int, impute median\n",
    " 'Cabin Staff Service', convert to int, impute median\n",
    " 'Food & Beverages', convert to int, impute median\n",
    " 'Ground Service', convert to int, impute median\n",
    " 'Inflight Entertainment', drop\n",
    " 'Wifi & Connectivity', drop\n",
    " 'Value For Money', convert to int, impute median\n",
    " 'Recommended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9ca2ec1-7060-4940-a586-1ea591c98c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23171 entries, 0 to 23170\n",
      "Data columns (total 19 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Airline Name            23171 non-null  object \n",
      " 1   Overall_Rating          23171 non-null  object \n",
      " 2   Review_Title            23171 non-null  object \n",
      " 3   Review Date             23171 non-null  object \n",
      " 4   Verified                23171 non-null  bool   \n",
      " 5   Review                  23171 non-null  object \n",
      " 6   Aircraft                7129 non-null   object \n",
      " 7   Type Of Traveller       19433 non-null  object \n",
      " 8   Seat Type               22075 non-null  object \n",
      " 9   Route                   19343 non-null  object \n",
      " 10  Date Flown              19417 non-null  object \n",
      " 11  Seat Comfort            19016 non-null  float64\n",
      " 12  Cabin Staff Service     18911 non-null  float64\n",
      " 13  Food & Beverages        14500 non-null  float64\n",
      " 14  Ground Service          18378 non-null  float64\n",
      " 15  Inflight Entertainment  10829 non-null  float64\n",
      " 16  Wifi & Connectivity     5920 non-null   float64\n",
      " 17  Value For Money         22105 non-null  float64\n",
      " 18  Recommended             23171 non-null  object \n",
      "dtypes: bool(1), float64(7), object(11)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "old_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be2324b5-aacd-42e4-bfad-763f4cb4cda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Airline Name', 'Overall_Rating', 'Review_Title', 'Review Date',\n",
       "       'Verified', 'Review', 'Aircraft', 'Type Of Traveller', 'Seat Type',\n",
       "       'Route', 'Date Flown', 'Seat Comfort', 'Cabin Staff Service',\n",
       "       'Food & Beverages', 'Ground Service', 'Inflight Entertainment',\n",
       "       'Wifi & Connectivity', 'Value For Money', 'Recommended'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e6962-9ab2-408f-85e5-b89a7635898b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
